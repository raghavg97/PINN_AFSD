{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7e8113-fc6b-42be-9a3c-685830458daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 1:  cuda:0\n",
      "Device 2:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "# from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import loadmat,savemat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device1 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device2 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Device 1: \",device1)\n",
    "print(\"Device 2: \",device2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12c536f-3169-48e3-82a1-915d437b21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "R0 = 5.36 #mm 5.36\n",
    "Rs = 19 #mm\n",
    "mu_vis = 0.3 \n",
    "mu = 0.3 #Friction Coefficient (not viscosity)\n",
    "delta = 0.5\n",
    "A = 6.41 #For slip factor\n",
    "pi = np.pi\n",
    "Omega = 300 #rpm\n",
    "V = 1 #mm/s\n",
    "F = 0.67 #mm\n",
    "rho = 2700 * 1e-6 #g/mm3\n",
    "k_B = 1.380649*1e-23 #J/K\n",
    "R = 8.314 #J/(K.mol)\n",
    "E_a = 205000 #J/mol #Q\n",
    "alpha_sig = 52 #mm^2/(kN)\n",
    "# A = np.exp(27.78)\n",
    "log_A = 27.78\n",
    "n = 3.49\n",
    "\n",
    "k = 0.167 #Thermal Conductivity #W/(mmK)\n",
    "c_p = 0.897 #J/gK \n",
    "alpha_m = k/(rho*c_p)\n",
    "T_a = 298.0\n",
    "\n",
    "\n",
    "k_t = 0.0176 #W/(mmK)\n",
    "c_p_t = 0.46 #J/gk\n",
    "rho_t = 7750 * 1e-6 #g/mm3\n",
    "alpha_t = k_t/(rho_t*c_p_t)\n",
    "\n",
    "h_sides = 5*1e-6 #W/mm^2K\n",
    "C_bot = 0.15*1e-6 #W/mm^2K^3\n",
    "\n",
    "eeta = alpha_m/(alpha_m+alpha_t)\n",
    "\n",
    "lb_xyz_uvw = np.array([-50.0,-20.0,-3.0])\n",
    "ub_xyz_uvw = np.array([50.0,20.0,0.0])\n",
    "\n",
    "\n",
    "lb_xyz = np.array([-50.0,-20.0,-3.0])\n",
    "ub_xyz = np.array([50.0,20.0,0.0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb877af0-8a4e-4628-b94d-de92a0d130bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyz_coll,xyz_1, xyz_2, xyz_3, xyz_4,xyz_top,xyz_bot,f_hat,N_hat,uvw_true_top,r_fr_ph_out):\n",
    "    # def closure():\n",
    "    optimizer.zero_grad()\n",
    "    #model_PINN.zero_grad()\n",
    "    loss_uvw,loss_T = model_PINN.loss(xyz_coll,xyz_1, xyz_2, xyz_3, xyz_4,xyz_top,xyz_bot,f_hat,N_hat,uvw_true_top,r_fr_ph_out)\n",
    "    loss_uvw.backward()\n",
    "    loss_T.backward()\n",
    "\n",
    "        # return loss\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5478b554-59b5-4825-95bb-acf6c27211d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep,n_batches):\n",
    "    print(rep)\n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    xyz_coll,xyz_1, xyz_2, xyz_3, xyz_4,xyz_top,xyz_bot, uvw_true_top,r_fr_ph_out = trainingdata_uvw(N_B,N_f,lb_xyz,ub_xyz,rep*123)\n",
    "    xyz_coll = torch.from_numpy(xyz_coll).float()#.to(device)\n",
    "    xyz_1 = torch.from_numpy(xyz_1).float()#.to(device)\n",
    "    xyz_2 = torch.from_numpy(xyz_2).float()#.to(device)\n",
    "    xyz_3 = torch.from_numpy(xyz_3).float()#.to(device)\n",
    "    xyz_4 = torch.from_numpy(xyz_4).float()#.to(device)\n",
    "    r_fr_ph_out = torch.from_numpy(r_fr_ph_out).float()\n",
    "    \n",
    "    xyz_top = torch.from_numpy(xyz_top).float()#.to(device)\n",
    "    xyz_bot = torch.from_numpy(xyz_bot).float()#.to(device)\n",
    "   \n",
    "    uvw_true_top = torch.from_numpy(uvw_true_top).float().to(device1)\n",
    "    f_hat = torch.zeros(xyz_coll.shape[0],1)#.to(device1)\n",
    "    N_hat = torch.zeros(xyz_top.shape[0],1).to(device2)\n",
    "    \n",
    "\n",
    "    #pretrain\n",
    "    # for i in range(50):\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss_pretrain = model_PINN.pretrain_T_loss(xyz_coll)\n",
    "    #     loss_pretrain.backward()\n",
    "    #     optimizer.step()\n",
    "        \n",
    "    # print(\"Pretrained with T = 300.0\")\n",
    "    \n",
    "    #Batching only Collocation and f_hat for now\n",
    "    \n",
    "    # batch_size_coll = int(xyz_coll.shape[0]/n_batches)\n",
    "    # batch_size_top = int(xyz_top.shape[0]/n_batches)\n",
    "\n",
    "    # batch_size_sides = int(xyz_1.shape[0]/n_batches)\n",
    "    # batch_size_bot = int(xyz_bot.shape[0]/n_batches)\n",
    "\n",
    "    # xyz_coll_batches = torch.split(xyz_coll,batch_size_coll)\n",
    "    # f_hat_batches = torch.split(f_hat,batch_size_coll)\n",
    "    \n",
    "    # xyz_top_batches = torch.split(xyz_top,batch_size_top)\n",
    "    # N_hat_batches = torch.split(N_hat,batch_size_top)\n",
    "\n",
    "    # xyz_1_batches = torch.split(xyz_1,batch_size_sides)\n",
    "    # xyz_2_batches = torch.split(xyz_2,batch_size_sides)\n",
    "    # xyz_3_batches = torch.split(xyz_3,batch_size_sides)\n",
    "    # xyz_4_batches = torch.split(xyz_4,batch_size_sides)\n",
    "    \n",
    "    # xyz_bot_batches = torch.split(xyz_4,batch_size_bot)\n",
    "\n",
    "    # uvw_true_top_batches = torch.split(uvw_true_top,batch_size_top)\n",
    "    # r_fr_ph_out_batches = torch.split(r_fr_ph_out,batch_size_top)\n",
    "    loss_np1,loss_np2 = model_PINN.loss(xyz_coll,xyz_1, xyz_2, xyz_3, xyz_4,xyz_top,xyz_bot,f_hat,N_hat,uvw_true_top,r_fr_ph_out)\n",
    "    loss_np = loss_np1.cpu().detach().numpy()+ loss_np2.cpu().detach().numpy()\n",
    "    print(loss_np1,loss_np2)\n",
    "    print(\"Initial Train Loss\",loss_np)\n",
    "\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "#         if(i>0 and i%50==0):\n",
    "#             _,_,_,_,_,xyz_top,xyz_bot = trainingdata(N_B,N_f,i*123)\n",
    "#             # xyz_coll = torch.from_numpy(xyz_coll).float().to(device)\n",
    "#             # xyz_1 = torch.from_numpy(xyz_1).float().to(device)\n",
    "#             # xyz_2 = torch.from_numpy(xyz_2).float().to(device)\n",
    "#             # xyz_3 = torch.from_numpy(xyz_3).float().to(device)\n",
    "#             # xyz_4 = torch.from_numpy(xyz_4).float().to(device)\n",
    "\n",
    "#             xyz_top = torch.from_numpy(xyz_top).float().to(device)\n",
    "#             xyz_bot = torch.from_numpy(xyz_bot).float().to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # for b in range(n_batches):\n",
    "        #     loss_uvw,loss_T = model_PINN.loss(xyz_coll,xyz_1, xyz_2, xyz_3, xyz_4,xyz_top,xyz_bot,f_hat,N_hat,uvw_true_top,r_fr_ph_out)\n",
    "        #     loss_uvw.backward()\n",
    "        #     loss_T.backward()\n",
    "\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # train_step(xyz_coll_batches[b],xyz_1_batches[b], xyz_2_batches[b], xyz_3_batches[b], xyz_4_batches[b],xyz_top_batches[b],xyz_bot_batches[b],f_hat_batches[b],N_hat_batches[b],uvw_true_top_batches[b],r_fr_ph_out_batches[b])\n",
    "            \n",
    "\n",
    "        train_step(xyz_coll,xyz_1, xyz_2, xyz_3, xyz_4,xyz_top,xyz_bot,f_hat,N_hat,uvw_true_top,r_fr_ph_out)\n",
    "        # loss_np = PINN.loss(xyz_coll,xyz_1, xyz_2, xyz_3, xyz_4,xyz_top,xyz_bot,f_hat,N_hat).cpu().detach().numpy()\n",
    "        # print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "        loss_np1,loss_np2 = model_PINN.loss(xyz_coll,xyz_1, xyz_2, xyz_3, xyz_4,xyz_top,xyz_bot,f_hat,N_hat,uvw_true_top,r_fr_ph_out)\n",
    "        loss_np = loss_np1.cpu().detach().numpy() + loss_np2.cpu().detach().numpy()\n",
    "        print(i,\"Train Loss\",loss_np)\n",
    "    \n",
    "        # if(i>0 and i%25 ==0):\n",
    "        #   pretrain(xyt_DBC,p_iters)\n",
    "        if(loss_np<6):\n",
    "            print(\"Loss Less than 2.5...\")\n",
    "            elapsed_time[rep] = time.time() - start_time\n",
    "            break\n",
    "\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5a6e4a-7ebc-437a-a707-0b7256025237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "Loss  270.06482 C_bot  0.15 k_c 0.0\n",
      "tensor(2.0058, device='cuda:0', grad_fn=<AddBackward0>) tensor(270.5572, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Initial Train Loss 272.563\n",
      "Loss  270.06482 C_bot  0.15 k_c 0.0\n",
      "Loss  52889.66 C_bot  0.15 k_c 0.0\n",
      "0 Train Loss 52920.688\n",
      "Loss  52889.66 C_bot  0.15 k_c 0.0\n",
      "Loss  2873.2808 C_bot  0.15 k_c 0.0\n",
      "1 Train Loss 2880.9597\n",
      "Loss  2873.2808 C_bot  0.15 k_c 0.0\n",
      "Loss  6327.0337 C_bot  0.15 k_c 0.0\n",
      "2 Train Loss 6332.466\n",
      "Loss  6327.0337 C_bot  0.15 k_c 0.0\n",
      "Loss  8717.008 C_bot  0.15 k_c 0.0\n",
      "3 Train Loss 8728.762\n",
      "Loss  8717.008 C_bot  0.15 k_c 0.0\n",
      "Loss  3475.4634 C_bot  0.15 k_c 0.0\n",
      "4 Train Loss 3502.601\n",
      "Loss  3475.4634 C_bot  0.15 k_c 0.0\n",
      "Loss  842.0917 C_bot  0.15 k_c 0.0\n",
      "5 Train Loss 887.79407\n",
      "Loss  842.0917 C_bot  0.15 k_c 0.0\n",
      "Loss  2808.4128 C_bot  0.15 k_c 0.0\n",
      "6 Train Loss 2861.5852\n",
      "Loss  2808.4128 C_bot  0.15 k_c 0.0\n",
      "Loss  4879.856 C_bot  0.15 k_c 0.0\n",
      "7 Train Loss 4937.554\n",
      "Loss  4879.856 C_bot  0.15 k_c 0.0\n",
      "Loss  4039.4558 C_bot  0.15 k_c 0.0\n",
      "8 Train Loss 4097.316\n",
      "Loss  4039.4558 C_bot  0.15 k_c 0.0\n",
      "Loss  1729.4917 C_bot  0.15 k_c 0.0\n",
      "9 Train Loss 1784.1766\n",
      "Loss  1729.4917 C_bot  0.15 k_c 0.0\n",
      "Loss  714.04736 C_bot  0.15 k_c 0.0\n",
      "10 Train Loss 765.52264\n",
      "Loss  714.04736 C_bot  0.15 k_c 0.0\n",
      "Loss  1687.7456 C_bot  0.15 k_c 0.0\n",
      "11 Train Loss 1737.3203\n",
      "Loss  1687.7456 C_bot  0.15 k_c 0.0\n",
      "Loss  2591.8271 C_bot  0.15 k_c 0.0\n",
      "12 Train Loss 2640.1667\n",
      "Loss  2591.8271 C_bot  0.15 k_c 0.0\n",
      "Loss  1874.7179 C_bot  0.15 k_c 0.0\n",
      "13 Train Loss 1922.2025\n",
      "Loss  1874.7179 C_bot  0.15 k_c 0.0\n",
      "Loss  635.43097 C_bot  0.15 k_c 0.0\n",
      "14 Train Loss 683.6371\n",
      "Loss  635.43097 C_bot  0.15 k_c 0.0\n",
      "Loss  526.7647 C_bot  0.15 k_c 0.0\n",
      "15 Train Loss 577.90845\n",
      "Loss  526.7647 C_bot  0.15 k_c 0.0\n",
      "Loss  1347.5713 C_bot  0.15 k_c 0.0\n",
      "16 Train Loss 1402.1841\n",
      "Loss  1347.5713 C_bot  0.15 k_c 0.0\n",
      "Loss  1644.653 C_bot  0.15 k_c 0.0\n",
      "17 Train Loss 1700.8099\n",
      "Loss  1644.653 C_bot  0.15 k_c 0.0\n",
      "Loss  955.86505 C_bot  0.15 k_c 0.0\n",
      "18 Train Loss 1010.5851\n",
      "Loss  955.86505 C_bot  0.15 k_c 0.0\n",
      "Loss  165.42503 C_bot  0.15 k_c 0.0\n",
      "19 Train Loss 216.86075\n",
      "Loss  165.42503 C_bot  0.15 k_c 0.0\n",
      "Loss  141.83192 C_bot  0.15 k_c 0.0\n",
      "20 Train Loss 189.97037\n",
      "Loss  141.83192 C_bot  0.15 k_c 0.0\n",
      "Loss  654.5262 C_bot  0.15 k_c 0.0\n",
      "21 Train Loss 700.5718\n",
      "Loss  654.5262 C_bot  0.15 k_c 0.0\n",
      "Loss  843.42737 C_bot  0.15 k_c 0.0\n",
      "22 Train Loss 888.90283\n",
      "Loss  843.42737 C_bot  0.15 k_c 0.0\n",
      "Loss  500.62488 C_bot  0.15 k_c 0.0\n",
      "23 Train Loss 547.0021\n",
      "Loss  500.62488 C_bot  0.15 k_c 0.0\n",
      "Loss  196.42708 C_bot  0.15 k_c 0.0\n",
      "24 Train Loss 244.94815\n",
      "Loss  196.42708 C_bot  0.15 k_c 0.0\n",
      "Loss  300.61905 C_bot  0.15 k_c 0.0\n",
      "25 Train Loss 351.6717\n",
      "Loss  300.61905 C_bot  0.15 k_c 0.0\n",
      "Loss  514.4692 C_bot  0.15 k_c 0.0\n",
      "26 Train Loss 566.9839\n",
      "Loss  514.4692 C_bot  0.15 k_c 0.0\n",
      "Loss  432.57996 C_bot  0.15 k_c 0.0\n",
      "27 Train Loss 484.4513\n",
      "Loss  432.57996 C_bot  0.15 k_c 0.0\n",
      "Loss  139.41745 C_bot  0.15 k_c 0.0\n",
      "28 Train Loss 188.81468\n",
      "Loss  139.41745 C_bot  0.15 k_c 0.0\n",
      "Loss  29.782684 C_bot  0.15 k_c 0.0\n",
      "29 Train Loss 76.118614\n",
      "Loss  29.782684 C_bot  0.15 k_c 0.0\n",
      "Loss  216.92374 C_bot  0.15 k_c 0.0\n",
      "30 Train Loss 260.81674\n",
      "Loss  216.92374 C_bot  0.15 k_c 0.0\n",
      "Loss  387.08972 C_bot  0.15 k_c 0.0\n",
      "31 Train Loss 429.74265\n",
      "Loss  387.08972 C_bot  0.15 k_c 0.0\n",
      "Loss  298.6906 C_bot  0.15 k_c 0.0\n",
      "32 Train Loss 341.3526\n",
      "Loss  298.6906 C_bot  0.15 k_c 0.0\n",
      "Loss  116.60685 C_bot  0.15 k_c 0.0\n",
      "33 Train Loss 160.2432\n",
      "Loss  116.60685 C_bot  0.15 k_c 0.0\n",
      "Loss  89.02808 C_bot  0.15 k_c 0.0\n",
      "34 Train Loss 133.97008\n",
      "Loss  89.02808 C_bot  0.15 k_c 0.0\n",
      "Loss  174.13902 C_bot  0.15 k_c 0.0\n",
      "35 Train Loss 219.82507\n",
      "Loss  174.13902 C_bot  0.15 k_c 0.0\n",
      "Loss  176.467 C_bot  0.15 k_c 0.0\n",
      "36 Train Loss 221.68521\n",
      "Loss  176.467 C_bot  0.15 k_c 0.0\n",
      "Loss  78.35405 C_bot  0.15 k_c 0.0\n",
      "37 Train Loss 121.971275\n",
      "Loss  78.35405 C_bot  0.15 k_c 0.0\n",
      "Loss  37.403362 C_bot  0.15 k_c 0.0\n",
      "38 Train Loss 78.98476\n",
      "Loss  37.403362 C_bot  0.15 k_c 0.0\n",
      "Loss  111.45334 C_bot  0.15 k_c 0.0\n",
      "39 Train Loss 151.32\n",
      "Loss  111.45334 C_bot  0.15 k_c 0.0\n",
      "Loss  169.01195 C_bot  0.15 k_c 0.0\n",
      "40 Train Loss 207.88953\n",
      "Loss  169.01195 C_bot  0.15 k_c 0.0\n",
      "Loss  113.57938 C_bot  0.15 k_c 0.0\n",
      "41 Train Loss 152.23044\n",
      "Loss  113.57938 C_bot  0.15 k_c 0.0\n",
      "Loss  31.16603 C_bot  0.15 k_c 0.0\n",
      "42 Train Loss 70.138664\n",
      "Loss  31.16603 C_bot  0.15 k_c 0.0\n",
      "Loss  30.254696 C_bot  0.15 k_c 0.0\n",
      "43 Train Loss 69.672745\n",
      "Loss  30.254696 C_bot  0.15 k_c 0.0\n",
      "Loss  74.73822 C_bot  0.15 k_c 0.0\n",
      "44 Train Loss 114.21092\n",
      "Loss  74.73822 C_bot  0.15 k_c 0.0\n",
      "Loss  72.97666 C_bot  0.15 k_c 0.0\n",
      "45 Train Loss 111.815216\n",
      "Loss  72.97666 C_bot  0.15 k_c 0.0\n",
      "Loss  34.630257 C_bot  0.15 k_c 0.0\n",
      "46 Train Loss 72.28445\n",
      "Loss  34.630257 C_bot  0.15 k_c 0.0\n",
      "Loss  33.142628 C_bot  0.15 k_c 0.0\n",
      "47 Train Loss 69.50011\n",
      "Loss  33.142628 C_bot  0.15 k_c 0.0\n",
      "Loss  66.667305 C_bot  0.15 k_c 0.0\n",
      "48 Train Loss 102.020996\n",
      "Loss  66.667305 C_bot  0.15 k_c 0.0\n",
      "Loss  67.373726 C_bot  0.15 k_c 0.0\n",
      "49 Train Loss 102.193886\n",
      "Loss  67.373726 C_bot  0.15 k_c 0.0\n",
      "Loss  28.23047 C_bot  0.15 k_c 0.0\n",
      "50 Train Loss 62.94258\n",
      "Loss  28.23047 C_bot  0.15 k_c 0.0\n",
      "Loss  10.472352 C_bot  0.15 k_c 0.0\n",
      "51 Train Loss 45.291027\n",
      "Loss  10.472352 C_bot  0.15 k_c 0.0\n",
      "Loss  31.864761 C_bot  0.15 k_c 0.0\n",
      "52 Train Loss 66.69443\n",
      "Loss  31.864761 C_bot  0.15 k_c 0.0\n",
      "Loss  45.346737 C_bot  0.15 k_c 0.0\n",
      "53 Train Loss 79.83756\n",
      "Loss  45.346737 C_bot  0.15 k_c 0.0\n",
      "Loss  29.508284 C_bot  0.15 k_c 0.0\n",
      "54 Train Loss 63.28755\n",
      "Loss  29.508284 C_bot  0.15 k_c 0.0\n",
      "Loss  18.048622 C_bot  0.15 k_c 0.0\n",
      "55 Train Loss 50.955524\n",
      "Loss  18.048622 C_bot  0.15 k_c 0.0\n",
      "Loss  28.801104 C_bot  0.15 k_c 0.0\n",
      "56 Train Loss 60.942913\n",
      "Loss  28.801104 C_bot  0.15 k_c 0.0\n",
      "Loss  33.65592 C_bot  0.15 k_c 0.0\n",
      "57 Train Loss 65.298706\n",
      "Loss  33.65592 C_bot  0.15 k_c 0.0\n",
      "Loss  18.147593 C_bot  0.15 k_c 0.0\n",
      "58 Train Loss 49.567417\n",
      "Loss  18.147593 C_bot  0.15 k_c 0.0\n",
      "Loss  8.404229 C_bot  0.15 k_c 0.0\n",
      "59 Train Loss 39.764374\n",
      "Loss  8.404229 C_bot  0.15 k_c 0.0\n",
      "Loss  18.42375 C_bot  0.15 k_c 0.0\n",
      "60 Train Loss 49.697395\n",
      "Loss  18.42375 C_bot  0.15 k_c 0.0\n",
      "Loss  25.97175 C_bot  0.15 k_c 0.0\n",
      "61 Train Loss 56.96321\n",
      "Loss  25.97175 C_bot  0.15 k_c 0.0\n",
      "Loss  17.738297 C_bot  0.15 k_c 0.0\n",
      "62 Train Loss 48.220734\n",
      "Loss  17.738297 C_bot  0.15 k_c 0.0\n",
      "Loss  10.882258 C_bot  0.15 k_c 0.0\n",
      "63 Train Loss 40.750626\n",
      "Loss  10.882258 C_bot  0.15 k_c 0.0\n",
      "Loss  15.624862 C_bot  0.15 k_c 0.0\n",
      "64 Train Loss 44.942627\n",
      "Loss  15.624862 C_bot  0.15 k_c 0.0\n",
      "Loss  17.92412 C_bot  0.15 k_c 0.0\n",
      "65 Train Loss 46.8585\n",
      "Loss  17.92412 C_bot  0.15 k_c 0.0\n",
      "Loss  11.417723 C_bot  0.15 k_c 0.0\n",
      "66 Train Loss 40.138725\n",
      "Loss  11.417723 C_bot  0.15 k_c 0.0\n",
      "Loss  9.03204 C_bot  0.15 k_c 0.0\n",
      "67 Train Loss 37.627693\n",
      "Loss  9.03204 C_bot  0.15 k_c 0.0\n",
      "Loss  14.548074 C_bot  0.15 k_c 0.0\n",
      "68 Train Loss 42.981987\n",
      "Loss  14.548074 C_bot  0.15 k_c 0.0\n",
      "Loss  15.92994 C_bot  0.15 k_c 0.0\n",
      "69 Train Loss 44.072323\n",
      "Loss  15.92994 C_bot  0.15 k_c 0.0\n",
      "Loss  10.555381 C_bot  0.15 k_c 0.0\n",
      "70 Train Loss 38.27898\n",
      "Loss  10.555381 C_bot  0.15 k_c 0.0\n",
      "Loss  8.706864 C_bot  0.15 k_c 0.0\n",
      "71 Train Loss 35.972893\n",
      "Loss  8.706864 C_bot  0.15 k_c 0.0\n",
      "Loss  11.869927 C_bot  0.15 k_c 0.0\n",
      "72 Train Loss 38.740036\n",
      "Loss  11.869927 C_bot  0.15 k_c 0.0\n",
      "Loss  11.89083 C_bot  0.15 k_c 0.0\n",
      "73 Train Loss 38.47626\n",
      "Loss  11.89083 C_bot  0.15 k_c 0.0\n",
      "Loss  8.962068 C_bot  0.15 k_c 0.0\n",
      "74 Train Loss 35.3573\n",
      "Loss  8.962068 C_bot  0.15 k_c 0.0\n",
      "Loss  9.421407 C_bot  0.15 k_c 0.0\n",
      "75 Train Loss 35.654427\n",
      "Loss  9.421407 C_bot  0.15 k_c 0.0\n",
      "Loss  11.429569 C_bot  0.15 k_c 0.0\n",
      "76 Train Loss 37.451595\n",
      "Loss  11.429569 C_bot  0.15 k_c 0.0\n",
      "Loss  9.889876 C_bot  0.15 k_c 0.0\n",
      "77 Train Loss 35.61654\n",
      "Loss  9.889876 C_bot  0.15 k_c 0.0\n",
      "Loss  7.5708537 C_bot  0.15 k_c 0.0\n",
      "78 Train Loss 32.946915\n",
      "Loss  7.5708537 C_bot  0.15 k_c 0.0\n",
      "Loss  8.5581875 C_bot  0.15 k_c 0.0\n",
      "79 Train Loss 33.59305\n",
      "Loss  8.5581875 C_bot  0.15 k_c 0.0\n",
      "Loss  9.9051695 C_bot  0.15 k_c 0.0\n",
      "80 Train Loss 34.658546\n",
      "Loss  9.9051695 C_bot  0.15 k_c 0.0\n",
      "Loss  8.770624 C_bot  0.15 k_c 0.0\n",
      "81 Train Loss 33.310944\n",
      "Loss  8.770624 C_bot  0.15 k_c 0.0\n",
      "Loss  7.9228964 C_bot  0.15 k_c 0.0\n",
      "82 Train Loss 32.287613\n",
      "Loss  7.9228964 C_bot  0.15 k_c 0.0\n",
      "Loss  8.750194 C_bot  0.15 k_c 0.0\n",
      "83 Train Loss 32.927017\n",
      "Loss  8.750194 C_bot  0.15 k_c 0.0\n",
      "Loss  8.654468 C_bot  0.15 k_c 0.0\n",
      "84 Train Loss 32.5957\n",
      "Loss  8.654468 C_bot  0.15 k_c 0.0\n",
      "Loss  7.4651637 C_bot  0.15 k_c 0.0\n",
      "85 Train Loss 31.126926\n",
      "Loss  7.4651637 C_bot  0.15 k_c 0.0\n",
      "Loss  7.6174273 C_bot  0.15 k_c 0.0\n",
      "86 Train Loss 30.992382\n",
      "Loss  7.6174273 C_bot  0.15 k_c 0.0\n",
      "Loss  8.547898 C_bot  0.15 k_c 0.0\n",
      "87 Train Loss 31.667242\n",
      "Loss  8.547898 C_bot  0.15 k_c 0.0\n",
      "Loss  8.19623 C_bot  0.15 k_c 0.0\n",
      "88 Train Loss 31.10637\n",
      "Loss  8.19623 C_bot  0.15 k_c 0.0\n",
      "Loss  7.448305 C_bot  0.15 k_c 0.0\n",
      "89 Train Loss 30.182024\n",
      "Loss  7.448305 C_bot  0.15 k_c 0.0\n",
      "Loss  7.6625285 C_bot  0.15 k_c 0.0\n",
      "90 Train Loss 30.221344\n",
      "Loss  7.6625285 C_bot  0.15 k_c 0.0\n",
      "Loss  7.7973013 C_bot  0.15 k_c 0.0\n",
      "91 Train Loss 30.155525\n",
      "Loss  7.7973013 C_bot  0.15 k_c 0.0\n",
      "Loss  7.2893677 C_bot  0.15 k_c 0.0\n",
      "92 Train Loss 29.417753\n",
      "Loss  7.2893677 C_bot  0.15 k_c 0.0\n",
      "Loss  7.275038 C_bot  0.15 k_c 0.0\n",
      "93 Train Loss 29.164839\n",
      "Loss  7.275038 C_bot  0.15 k_c 0.0\n",
      "Loss  7.7160535 C_bot  0.15 k_c 0.0\n",
      "94 Train Loss 29.384575\n",
      "Loss  7.7160535 C_bot  0.15 k_c 0.0\n",
      "Loss  7.52529 C_bot  0.15 k_c 0.0\n",
      "95 Train Loss 29.002466\n",
      "Loss  7.52529 C_bot  0.15 k_c 0.0\n",
      "Loss  7.068646 C_bot  0.15 k_c 0.0\n",
      "96 Train Loss 28.377338\n",
      "Loss  7.068646 C_bot  0.15 k_c 0.0\n",
      "Loss  7.150669 C_bot  0.15 k_c 0.0\n",
      "97 Train Loss 28.293629\n",
      "Loss  7.150669 C_bot  0.15 k_c 0.0\n",
      "Loss  7.267571 C_bot  0.15 k_c 0.0\n",
      "98 Train Loss 28.229254\n",
      "Loss  7.267571 C_bot  0.15 k_c 0.0\n",
      "Loss  7.044876 C_bot  0.15 k_c 0.0\n",
      "99 Train Loss 27.807098\n",
      "Loss  7.044876 C_bot  0.15 k_c 0.0\n",
      "Loss  7.0322022 C_bot  0.15 k_c 0.0\n",
      "100 Train Loss 27.590536\n",
      "Loss  7.0322022 C_bot  0.15 k_c 0.0\n",
      "Loss  7.20736 C_bot  0.15 k_c 0.0\n",
      "101 Train Loss 27.575062\n",
      "Loss  7.20736 C_bot  0.15 k_c 0.0\n",
      "Loss  7.0531745 C_bot  0.15 k_c 0.0\n",
      "102 Train Loss 27.25174\n",
      "Loss  7.0531745 C_bot  0.15 k_c 0.0\n",
      "Loss  6.828717 C_bot  0.15 k_c 0.0\n",
      "103 Train Loss 26.873957\n",
      "Loss  6.828717 C_bot  0.15 k_c 0.0\n",
      "Loss  6.9021506 C_bot  0.15 k_c 0.0\n",
      "104 Train Loss 26.79552\n",
      "Loss  6.9021506 C_bot  0.15 k_c 0.0\n",
      "Loss  6.9417815 C_bot  0.15 k_c 0.0\n",
      "105 Train Loss 26.672686\n",
      "Loss  6.9417815 C_bot  0.15 k_c 0.0\n",
      "Loss  6.8065557 C_bot  0.15 k_c 0.0\n",
      "106 Train Loss 26.363705\n",
      "Loss  6.8065557 C_bot  0.15 k_c 0.0\n",
      "Loss  6.799003 C_bot  0.15 k_c 0.0\n",
      "107 Train Loss 26.181082\n",
      "Loss  6.799003 C_bot  0.15 k_c 0.0\n",
      "Loss  6.8431063 C_bot  0.15 k_c 0.0\n",
      "108 Train Loss 26.060299\n",
      "Loss  6.8431063 C_bot  0.15 k_c 0.0\n",
      "Loss  6.7223997 C_bot  0.15 k_c 0.0\n",
      "109 Train Loss 25.789148\n",
      "Loss  6.7223997 C_bot  0.15 k_c 0.0\n",
      "Loss  6.6371083 C_bot  0.15 k_c 0.0\n",
      "110 Train Loss 25.562683\n",
      "Loss  6.6371083 C_bot  0.15 k_c 0.0\n",
      "Loss  6.6850743 C_bot  0.15 k_c 0.0\n",
      "111 Train Loss 25.468931\n",
      "Loss  6.6850743 C_bot  0.15 k_c 0.0\n",
      "Loss  6.6543407 C_bot  0.15 k_c 0.0\n",
      "112 Train Loss 25.289448\n",
      "Loss  6.6543407 C_bot  0.15 k_c 0.0\n",
      "Loss  6.565398 C_bot  0.15 k_c 0.0\n",
      "113 Train Loss 25.046618\n",
      "Loss  6.565398 C_bot  0.15 k_c 0.0\n",
      "Loss  6.567214 C_bot  0.15 k_c 0.0\n",
      "114 Train Loss 24.89734\n",
      "Loss  6.567214 C_bot  0.15 k_c 0.0\n",
      "Loss  6.555672 C_bot  0.15 k_c 0.0\n",
      "115 Train Loss 24.744408\n",
      "Loss  6.555672 C_bot  0.15 k_c 0.0\n",
      "Loss  6.474853 C_bot  0.15 k_c 0.0\n",
      "116 Train Loss 24.532557\n",
      "Loss  6.474853 C_bot  0.15 k_c 0.0\n",
      "Loss  6.4486046 C_bot  0.15 k_c 0.0\n",
      "117 Train Loss 24.380135\n",
      "Loss  6.4486046 C_bot  0.15 k_c 0.0\n",
      "Loss  6.4499197 C_bot  0.15 k_c 0.0\n",
      "118 Train Loss 24.253235\n",
      "Loss  6.4499197 C_bot  0.15 k_c 0.0\n",
      "Loss  6.3907123 C_bot  0.15 k_c 0.0\n",
      "119 Train Loss 24.061148\n",
      "Loss  6.3907123 C_bot  0.15 k_c 0.0\n",
      "Loss  6.347406 C_bot  0.15 k_c 0.0\n",
      "120 Train Loss 23.883781\n",
      "Loss  6.347406 C_bot  0.15 k_c 0.0\n",
      "Loss  6.3475347 C_bot  0.15 k_c 0.0\n",
      "121 Train Loss 23.75477\n",
      "Loss  6.3475347 C_bot  0.15 k_c 0.0\n",
      "Loss  6.3104353 C_bot  0.15 k_c 0.0\n",
      "122 Train Loss 23.59699\n",
      "Loss  6.3104353 C_bot  0.15 k_c 0.0\n",
      "Loss  6.2596726 C_bot  0.15 k_c 0.0\n",
      "123 Train Loss 23.432457\n",
      "Loss  6.2596726 C_bot  0.15 k_c 0.0\n",
      "Loss  6.2417164 C_bot  0.15 k_c 0.0\n",
      "124 Train Loss 23.302742\n",
      "Loss  6.2417164 C_bot  0.15 k_c 0.0\n",
      "Loss  6.2091055 C_bot  0.15 k_c 0.0\n",
      "125 Train Loss 23.15643\n",
      "Loss  6.2091055 C_bot  0.15 k_c 0.0\n",
      "Loss  6.162041 C_bot  0.15 k_c 0.0\n",
      "126 Train Loss 22.993893\n",
      "Loss  6.162041 C_bot  0.15 k_c 0.0\n",
      "Loss  6.1452193 C_bot  0.15 k_c 0.0\n",
      "127 Train Loss 22.863508\n",
      "Loss  6.1452193 C_bot  0.15 k_c 0.0\n",
      "Loss  6.1259437 C_bot  0.15 k_c 0.0\n",
      "128 Train Loss 22.73612\n",
      "Loss  6.1259437 C_bot  0.15 k_c 0.0\n",
      "Loss  6.082014 C_bot  0.15 k_c 0.0\n",
      "129 Train Loss 22.589893\n",
      "Loss  6.082014 C_bot  0.15 k_c 0.0\n",
      "Loss  6.0504565 C_bot  0.15 k_c 0.0\n",
      "130 Train Loss 22.458954\n",
      "Loss  6.0504565 C_bot  0.15 k_c 0.0\n",
      "Loss  6.026506 C_bot  0.15 k_c 0.0\n",
      "131 Train Loss 22.33502\n",
      "Loss  6.026506 C_bot  0.15 k_c 0.0\n",
      "Loss  5.9913735 C_bot  0.15 k_c 0.0\n",
      "132 Train Loss 22.19819\n",
      "Loss  5.9913735 C_bot  0.15 k_c 0.0\n",
      "Loss  5.9673114 C_bot  0.15 k_c 0.0\n",
      "133 Train Loss 22.072739\n",
      "Loss  5.9673114 C_bot  0.15 k_c 0.0\n",
      "Loss  5.9507833 C_bot  0.15 k_c 0.0\n",
      "134 Train Loss 21.958153\n",
      "Loss  5.9507833 C_bot  0.15 k_c 0.0\n",
      "Loss  5.9183555 C_bot  0.15 k_c 0.0\n",
      "135 Train Loss 21.83231\n",
      "Loss  5.9183555 C_bot  0.15 k_c 0.0\n",
      "Loss  5.8855515 C_bot  0.15 k_c 0.0\n",
      "136 Train Loss 21.709307\n",
      "Loss  5.8855515 C_bot  0.15 k_c 0.0\n",
      "Loss  5.8628263 C_bot  0.15 k_c 0.0\n",
      "137 Train Loss 21.596905\n",
      "Loss  5.8628263 C_bot  0.15 k_c 0.0\n",
      "Loss  5.836575 C_bot  0.15 k_c 0.0\n",
      "138 Train Loss 21.479908\n",
      "Loss  5.836575 C_bot  0.15 k_c 0.0\n",
      "Loss  5.8127494 C_bot  0.15 k_c 0.0\n",
      "139 Train Loss 21.36502\n",
      "Loss  5.8127494 C_bot  0.15 k_c 0.0\n",
      "Loss  5.795552 C_bot  0.15 k_c 0.0\n",
      "140 Train Loss 21.258553\n",
      "Loss  5.795552 C_bot  0.15 k_c 0.0\n",
      "Loss  5.7713094 C_bot  0.15 k_c 0.0\n",
      "141 Train Loss 21.148224\n",
      "Loss  5.7713094 C_bot  0.15 k_c 0.0\n",
      "Loss  5.7444215 C_bot  0.15 k_c 0.0\n",
      "142 Train Loss 21.037916\n",
      "Loss  5.7444215 C_bot  0.15 k_c 0.0\n",
      "Loss  5.7247057 C_bot  0.15 k_c 0.0\n",
      "143 Train Loss 20.935654\n",
      "Loss  5.7247057 C_bot  0.15 k_c 0.0\n",
      "Loss  5.7049475 C_bot  0.15 k_c 0.0\n",
      "144 Train Loss 20.83287\n",
      "Loss  5.7049475 C_bot  0.15 k_c 0.0\n",
      "Loss  5.6852636 C_bot  0.15 k_c 0.0\n",
      "145 Train Loss 20.729961\n",
      "Loss  5.6852636 C_bot  0.15 k_c 0.0\n",
      "Loss  5.6697316 C_bot  0.15 k_c 0.0\n",
      "146 Train Loss 20.63249\n",
      "Loss  5.6697316 C_bot  0.15 k_c 0.0\n",
      "Loss  5.651432 C_bot  0.15 k_c 0.0\n",
      "147 Train Loss 20.534746\n",
      "Loss  5.651432 C_bot  0.15 k_c 0.0\n",
      "Loss  5.630724 C_bot  0.15 k_c 0.0\n",
      "148 Train Loss 20.436913\n",
      "Loss  5.630724 C_bot  0.15 k_c 0.0\n",
      "Loss  5.614446 C_bot  0.15 k_c 0.0\n",
      "149 Train Loss 20.344532\n",
      "Loss  5.614446 C_bot  0.15 k_c 0.0\n",
      "Loss  5.599051 C_bot  0.15 k_c 0.0\n",
      "150 Train Loss 20.252897\n",
      "Loss  5.599051 C_bot  0.15 k_c 0.0\n",
      "Loss  5.5834227 C_bot  0.15 k_c 0.0\n",
      "151 Train Loss 20.160908\n",
      "Loss  5.5834227 C_bot  0.15 k_c 0.0\n",
      "Loss  5.5704975 C_bot  0.15 k_c 0.0\n",
      "152 Train Loss 20.07252\n",
      "Loss  5.5704975 C_bot  0.15 k_c 0.0\n",
      "Loss  5.557068 C_bot  0.15 k_c 0.0\n",
      "153 Train Loss 19.98551\n",
      "Loss  5.557068 C_bot  0.15 k_c 0.0\n",
      "Loss  5.542063 C_bot  0.15 k_c 0.0\n",
      "154 Train Loss 19.898823\n",
      "Loss  5.542063 C_bot  0.15 k_c 0.0\n",
      "Loss  5.529423 C_bot  0.15 k_c 0.0\n",
      "155 Train Loss 19.815544\n",
      "Loss  5.529423 C_bot  0.15 k_c 0.0\n",
      "Loss  5.5175743 C_bot  0.15 k_c 0.0\n",
      "156 Train Loss 19.733246\n",
      "Loss  5.5175743 C_bot  0.15 k_c 0.0\n",
      "Loss  5.5057645 C_bot  0.15 k_c 0.0\n",
      "157 Train Loss 19.651152\n",
      "Loss  5.5057645 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4959836 C_bot  0.15 k_c 0.0\n",
      "158 Train Loss 19.57197\n",
      "Loss  5.4959836 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4854684 C_bot  0.15 k_c 0.0\n",
      "159 Train Loss 19.493631\n",
      "Loss  5.4854684 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4745207 C_bot  0.15 k_c 0.0\n",
      "160 Train Loss 19.41641\n",
      "Loss  5.4745207 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4648476 C_bot  0.15 k_c 0.0\n",
      "161 Train Loss 19.341333\n",
      "Loss  5.4648476 C_bot  0.15 k_c 0.0\n",
      "Loss  5.455892 C_bot  0.15 k_c 0.0\n",
      "162 Train Loss 19.267223\n",
      "Loss  5.455892 C_bot  0.15 k_c 0.0\n",
      "Loss  5.447341 C_bot  0.15 k_c 0.0\n",
      "163 Train Loss 19.19377\n",
      "Loss  5.447341 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4400005 C_bot  0.15 k_c 0.0\n",
      "164 Train Loss 19.122358\n",
      "Loss  5.4400005 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4323263 C_bot  0.15 k_c 0.0\n",
      "165 Train Loss 19.051952\n",
      "Loss  5.4323263 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4239836 C_bot  0.15 k_c 0.0\n",
      "166 Train Loss 18.98218\n",
      "Loss  5.4239836 C_bot  0.15 k_c 0.0\n",
      "Loss  5.416613 C_bot  0.15 k_c 0.0\n",
      "167 Train Loss 18.914162\n",
      "Loss  5.416613 C_bot  0.15 k_c 0.0\n",
      "Loss  5.409901 C_bot  0.15 k_c 0.0\n",
      "168 Train Loss 18.84714\n",
      "Loss  5.409901 C_bot  0.15 k_c 0.0\n",
      "Loss  5.403526 C_bot  0.15 k_c 0.0\n",
      "169 Train Loss 18.78084\n",
      "Loss  5.403526 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3977966 C_bot  0.15 k_c 0.0\n",
      "170 Train Loss 18.715986\n",
      "Loss  5.3977966 C_bot  0.15 k_c 0.0\n",
      "Loss  5.391804 C_bot  0.15 k_c 0.0\n",
      "171 Train Loss 18.651995\n",
      "Loss  5.391804 C_bot  0.15 k_c 0.0\n",
      "Loss  5.385613 C_bot  0.15 k_c 0.0\n",
      "172 Train Loss 18.588812\n",
      "Loss  5.385613 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3800254 C_bot  0.15 k_c 0.0\n",
      "173 Train Loss 18.526842\n",
      "Loss  5.3800254 C_bot  0.15 k_c 0.0\n",
      "Loss  5.374824 C_bot  0.15 k_c 0.0\n",
      "174 Train Loss 18.465593\n",
      "Loss  5.374824 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3700967 C_bot  0.15 k_c 0.0\n",
      "175 Train Loss 18.405277\n",
      "Loss  5.3700967 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3654623 C_bot  0.15 k_c 0.0\n",
      "176 Train Loss 18.345846\n",
      "Loss  5.3654623 C_bot  0.15 k_c 0.0\n",
      "Loss  5.360727 C_bot  0.15 k_c 0.0\n",
      "177 Train Loss 18.287292\n",
      "Loss  5.360727 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3558125 C_bot  0.15 k_c 0.0\n",
      "178 Train Loss 18.22938\n",
      "Loss  5.3558125 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3514476 C_bot  0.15 k_c 0.0\n",
      "179 Train Loss 18.172514\n",
      "Loss  5.3514476 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3473315 C_bot  0.15 k_c 0.0\n",
      "180 Train Loss 18.116241\n",
      "Loss  5.3473315 C_bot  0.15 k_c 0.0\n",
      "Loss  5.343459 C_bot  0.15 k_c 0.0\n",
      "181 Train Loss 18.0607\n",
      "Loss  5.343459 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3396707 C_bot  0.15 k_c 0.0\n",
      "182 Train Loss 18.005974\n",
      "Loss  5.3396707 C_bot  0.15 k_c 0.0\n",
      "Loss  5.335749 C_bot  0.15 k_c 0.0\n",
      "183 Train Loss 17.95192\n",
      "Loss  5.335749 C_bot  0.15 k_c 0.0\n",
      "Loss  5.331913 C_bot  0.15 k_c 0.0\n",
      "184 Train Loss 17.898584\n",
      "Loss  5.331913 C_bot  0.15 k_c 0.0\n",
      "Loss  5.328283 C_bot  0.15 k_c 0.0\n",
      "185 Train Loss 17.845863\n",
      "Loss  5.328283 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3248897 C_bot  0.15 k_c 0.0\n",
      "186 Train Loss 17.793753\n",
      "Loss  5.3248897 C_bot  0.15 k_c 0.0\n",
      "Loss  5.321684 C_bot  0.15 k_c 0.0\n",
      "187 Train Loss 17.742357\n",
      "Loss  5.321684 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3184566 C_bot  0.15 k_c 0.0\n",
      "188 Train Loss 17.691624\n",
      "Loss  5.3184566 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3151326 C_bot  0.15 k_c 0.0\n",
      "189 Train Loss 17.641453\n",
      "Loss  5.3151326 C_bot  0.15 k_c 0.0\n",
      "Loss  5.311905 C_bot  0.15 k_c 0.0\n",
      "190 Train Loss 17.59187\n",
      "Loss  5.311905 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3088226 C_bot  0.15 k_c 0.0\n",
      "191 Train Loss 17.542782\n",
      "Loss  5.3088226 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3060246 C_bot  0.15 k_c 0.0\n",
      "192 Train Loss 17.494362\n",
      "Loss  5.3060246 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3030324 C_bot  0.15 k_c 0.0\n",
      "193 Train Loss 17.446278\n",
      "Loss  5.3030324 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3001857 C_bot  0.15 k_c 0.0\n",
      "194 Train Loss 17.398943\n",
      "Loss  5.3001857 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2971516 C_bot  0.15 k_c 0.0\n",
      "195 Train Loss 17.351952\n",
      "Loss  5.2971516 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2942014 C_bot  0.15 k_c 0.0\n",
      "196 Train Loss 17.305431\n",
      "Loss  5.2942014 C_bot  0.15 k_c 0.0\n",
      "Loss  5.291652 C_bot  0.15 k_c 0.0\n",
      "197 Train Loss 17.259655\n",
      "Loss  5.291652 C_bot  0.15 k_c 0.0\n",
      "Loss  5.289038 C_bot  0.15 k_c 0.0\n",
      "198 Train Loss 17.214228\n",
      "Loss  5.289038 C_bot  0.15 k_c 0.0\n",
      "Loss  5.28643 C_bot  0.15 k_c 0.0\n",
      "199 Train Loss 17.169315\n",
      "Loss  5.28643 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2836466 C_bot  0.15 k_c 0.0\n",
      "200 Train Loss 17.124735\n",
      "Loss  5.2836466 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2811394 C_bot  0.15 k_c 0.0\n",
      "201 Train Loss 17.080841\n",
      "Loss  5.2811394 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2785277 C_bot  0.15 k_c 0.0\n",
      "202 Train Loss 17.037176\n",
      "Loss  5.2785277 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2761273 C_bot  0.15 k_c 0.0\n",
      "203 Train Loss 16.994064\n",
      "Loss  5.2761273 C_bot  0.15 k_c 0.0\n",
      "Loss  5.273644 C_bot  0.15 k_c 0.0\n",
      "204 Train Loss 16.951288\n",
      "Loss  5.273644 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2712994 C_bot  0.15 k_c 0.0\n",
      "205 Train Loss 16.909115\n",
      "Loss  5.2712994 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2686677 C_bot  0.15 k_c 0.0\n",
      "206 Train Loss 16.867067\n",
      "Loss  5.2686677 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2663045 C_bot  0.15 k_c 0.0\n",
      "207 Train Loss 16.825624\n",
      "Loss  5.2663045 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2639446 C_bot  0.15 k_c 0.0\n",
      "208 Train Loss 16.784492\n",
      "Loss  5.2639446 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2616425 C_bot  0.15 k_c 0.0\n",
      "209 Train Loss 16.743769\n",
      "Loss  5.2616425 C_bot  0.15 k_c 0.0\n",
      "Loss  5.259262 C_bot  0.15 k_c 0.0\n",
      "210 Train Loss 16.703367\n",
      "Loss  5.259262 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2568393 C_bot  0.15 k_c 0.0\n",
      "211 Train Loss 16.663315\n",
      "Loss  5.2568393 C_bot  0.15 k_c 0.0\n",
      "Loss  5.254604 C_bot  0.15 k_c 0.0\n",
      "212 Train Loss 16.623781\n",
      "Loss  5.254604 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2523293 C_bot  0.15 k_c 0.0\n",
      "213 Train Loss 16.5845\n",
      "Loss  5.2523293 C_bot  0.15 k_c 0.0\n",
      "Loss  5.250106 C_bot  0.15 k_c 0.0\n",
      "214 Train Loss 16.545578\n",
      "Loss  5.250106 C_bot  0.15 k_c 0.0\n",
      "Loss  5.247848 C_bot  0.15 k_c 0.0\n",
      "215 Train Loss 16.506962\n",
      "Loss  5.247848 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2455616 C_bot  0.15 k_c 0.0\n",
      "216 Train Loss 16.468672\n",
      "Loss  5.2455616 C_bot  0.15 k_c 0.0\n",
      "Loss  5.243213 C_bot  0.15 k_c 0.0\n",
      "217 Train Loss 16.430645\n",
      "Loss  5.243213 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2411604 C_bot  0.15 k_c 0.0\n",
      "218 Train Loss 16.393187\n",
      "Loss  5.2411604 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2388988 C_bot  0.15 k_c 0.0\n",
      "219 Train Loss 16.355791\n",
      "Loss  5.2388988 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2367887 C_bot  0.15 k_c 0.0\n",
      "220 Train Loss 16.318846\n",
      "Loss  5.2367887 C_bot  0.15 k_c 0.0\n",
      "Loss  5.234544 C_bot  0.15 k_c 0.0\n",
      "221 Train Loss 16.282076\n",
      "Loss  5.234544 C_bot  0.15 k_c 0.0\n",
      "Loss  5.232363 C_bot  0.15 k_c 0.0\n",
      "222 Train Loss 16.245667\n",
      "Loss  5.232363 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2301965 C_bot  0.15 k_c 0.0\n",
      "223 Train Loss 16.209534\n",
      "Loss  5.2301965 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2280674 C_bot  0.15 k_c 0.0\n",
      "224 Train Loss 16.173681\n",
      "Loss  5.2280674 C_bot  0.15 k_c 0.0\n",
      "Loss  5.225926 C_bot  0.15 k_c 0.0\n",
      "225 Train Loss 16.13807\n",
      "Loss  5.225926 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2238445 C_bot  0.15 k_c 0.0\n",
      "226 Train Loss 16.102795\n",
      "Loss  5.2238445 C_bot  0.15 k_c 0.0\n",
      "Loss  5.221665 C_bot  0.15 k_c 0.0\n",
      "227 Train Loss 16.067688\n",
      "Loss  5.221665 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2194924 C_bot  0.15 k_c 0.0\n",
      "228 Train Loss 16.032822\n",
      "Loss  5.2194924 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2173514 C_bot  0.15 k_c 0.0\n",
      "229 Train Loss 15.998205\n",
      "Loss  5.2173514 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2153687 C_bot  0.15 k_c 0.0\n",
      "230 Train Loss 15.963972\n",
      "Loss  5.2153687 C_bot  0.15 k_c 0.0\n",
      "Loss  5.21319 C_bot  0.15 k_c 0.0\n",
      "231 Train Loss 15.929776\n",
      "Loss  5.21319 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2111 C_bot  0.15 k_c 0.0\n",
      "232 Train Loss 15.8959055\n",
      "Loss  5.2111 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2091136 C_bot  0.15 k_c 0.0\n",
      "233 Train Loss 15.862356\n",
      "Loss  5.2091136 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2070084 C_bot  0.15 k_c 0.0\n",
      "234 Train Loss 15.828886\n",
      "Loss  5.2070084 C_bot  0.15 k_c 0.0\n",
      "Loss  5.204915 C_bot  0.15 k_c 0.0\n",
      "235 Train Loss 15.795631\n",
      "Loss  5.204915 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2028294 C_bot  0.15 k_c 0.0\n",
      "236 Train Loss 15.762603\n",
      "Loss  5.2028294 C_bot  0.15 k_c 0.0\n",
      "Loss  5.200757 C_bot  0.15 k_c 0.0\n",
      "237 Train Loss 15.72981\n",
      "Loss  5.200757 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1986003 C_bot  0.15 k_c 0.0\n",
      "238 Train Loss 15.697144\n",
      "Loss  5.1986003 C_bot  0.15 k_c 0.0\n",
      "Loss  5.196655 C_bot  0.15 k_c 0.0\n",
      "239 Train Loss 15.664896\n",
      "Loss  5.196655 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1946697 C_bot  0.15 k_c 0.0\n",
      "240 Train Loss 15.632818\n",
      "Loss  5.1946697 C_bot  0.15 k_c 0.0\n",
      "Loss  5.192587 C_bot  0.15 k_c 0.0\n",
      "241 Train Loss 15.600871\n",
      "Loss  5.192587 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1905537 C_bot  0.15 k_c 0.0\n",
      "242 Train Loss 15.569213\n",
      "Loss  5.1905537 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1886077 C_bot  0.15 k_c 0.0\n",
      "243 Train Loss 15.537879\n",
      "Loss  5.1886077 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1864133 C_bot  0.15 k_c 0.0\n",
      "244 Train Loss 15.506532\n",
      "Loss  5.1864133 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1843047 C_bot  0.15 k_c 0.0\n",
      "245 Train Loss 15.475514\n",
      "Loss  5.1843047 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1823277 C_bot  0.15 k_c 0.0\n",
      "246 Train Loss 15.444888\n",
      "Loss  5.1823277 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1802416 C_bot  0.15 k_c 0.0\n",
      "247 Train Loss 15.414421\n",
      "Loss  5.1802416 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1783376 C_bot  0.15 k_c 0.0\n",
      "248 Train Loss 15.384401\n",
      "Loss  5.1783376 C_bot  0.15 k_c 0.0\n",
      "Loss  5.176274 C_bot  0.15 k_c 0.0\n",
      "249 Train Loss 15.354485\n",
      "Loss  5.176274 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1741834 C_bot  0.15 k_c 0.0\n",
      "250 Train Loss 15.324807\n",
      "Loss  5.1741834 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1722493 C_bot  0.15 k_c 0.0\n",
      "251 Train Loss 15.295559\n",
      "Loss  5.1722493 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1703043 C_bot  0.15 k_c 0.0\n",
      "252 Train Loss 15.266577\n",
      "Loss  5.1703043 C_bot  0.15 k_c 0.0\n",
      "Loss  5.168263 C_bot  0.15 k_c 0.0\n",
      "253 Train Loss 15.237765\n",
      "Loss  5.168263 C_bot  0.15 k_c 0.0\n",
      "Loss  5.166207 C_bot  0.15 k_c 0.0\n",
      "254 Train Loss 15.209198\n",
      "Loss  5.166207 C_bot  0.15 k_c 0.0\n",
      "Loss  5.164204 C_bot  0.15 k_c 0.0\n",
      "255 Train Loss 15.180942\n",
      "Loss  5.164204 C_bot  0.15 k_c 0.0\n",
      "Loss  5.162071 C_bot  0.15 k_c 0.0\n",
      "256 Train Loss 15.152818\n",
      "Loss  5.162071 C_bot  0.15 k_c 0.0\n",
      "Loss  5.160147 C_bot  0.15 k_c 0.0\n",
      "257 Train Loss 15.125157\n",
      "Loss  5.160147 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1580396 C_bot  0.15 k_c 0.0\n",
      "258 Train Loss 15.097559\n",
      "Loss  5.1580396 C_bot  0.15 k_c 0.0\n",
      "Loss  5.156043 C_bot  0.15 k_c 0.0\n",
      "259 Train Loss 15.07031\n",
      "Loss  5.156043 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1540246 C_bot  0.15 k_c 0.0\n",
      "260 Train Loss 15.043271\n",
      "Loss  5.1540246 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1521297 C_bot  0.15 k_c 0.0\n",
      "261 Train Loss 15.016593\n",
      "Loss  5.1521297 C_bot  0.15 k_c 0.0\n",
      "Loss  5.149935 C_bot  0.15 k_c 0.0\n",
      "262 Train Loss 14.989842\n",
      "Loss  5.149935 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1478887 C_bot  0.15 k_c 0.0\n",
      "263 Train Loss 14.96346\n",
      "Loss  5.1478887 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1459055 C_bot  0.15 k_c 0.0\n",
      "264 Train Loss 14.937354\n",
      "Loss  5.1459055 C_bot  0.15 k_c 0.0\n",
      "Loss  5.143855 C_bot  0.15 k_c 0.0\n",
      "265 Train Loss 14.911391\n",
      "Loss  5.143855 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1417427 C_bot  0.15 k_c 0.0\n",
      "266 Train Loss 14.885577\n",
      "Loss  5.1417427 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1397815 C_bot  0.15 k_c 0.0\n",
      "267 Train Loss 14.860121\n",
      "Loss  5.1397815 C_bot  0.15 k_c 0.0\n",
      "Loss  5.13781 C_bot  0.15 k_c 0.0\n",
      "268 Train Loss 14.834855\n",
      "Loss  5.13781 C_bot  0.15 k_c 0.0\n",
      "Loss  5.135806 C_bot  0.15 k_c 0.0\n",
      "269 Train Loss 14.809749\n",
      "Loss  5.135806 C_bot  0.15 k_c 0.0\n",
      "Loss  5.133728 C_bot  0.15 k_c 0.0\n",
      "270 Train Loss 14.784761\n",
      "Loss  5.133728 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1317 C_bot  0.15 k_c 0.0\n",
      "271 Train Loss 14.760012\n",
      "Loss  5.1317 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1295652 C_bot  0.15 k_c 0.0\n",
      "272 Train Loss 14.735351\n",
      "Loss  5.1295652 C_bot  0.15 k_c 0.0\n",
      "Loss  5.127417 C_bot  0.15 k_c 0.0\n",
      "273 Train Loss 14.710858\n",
      "Loss  5.127417 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1256347 C_bot  0.15 k_c 0.0\n",
      "274 Train Loss 14.686913\n",
      "Loss  5.1256347 C_bot  0.15 k_c 0.0\n",
      "Loss  5.123386 C_bot  0.15 k_c 0.0\n",
      "275 Train Loss 14.662677\n",
      "Loss  5.123386 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1213512 C_bot  0.15 k_c 0.0\n",
      "276 Train Loss 14.638836\n",
      "Loss  5.1213512 C_bot  0.15 k_c 0.0\n",
      "Loss  5.11927 C_bot  0.15 k_c 0.0\n",
      "277 Train Loss 14.615126\n",
      "Loss  5.11927 C_bot  0.15 k_c 0.0\n",
      "Loss  5.117157 C_bot  0.15 k_c 0.0\n",
      "278 Train Loss 14.591554\n",
      "Loss  5.117157 C_bot  0.15 k_c 0.0\n",
      "Loss  5.115219 C_bot  0.15 k_c 0.0\n",
      "279 Train Loss 14.568325\n",
      "Loss  5.115219 C_bot  0.15 k_c 0.0\n",
      "Loss  5.113072 C_bot  0.15 k_c 0.0\n",
      "280 Train Loss 14.545055\n",
      "Loss  5.113072 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1108837 C_bot  0.15 k_c 0.0\n",
      "281 Train Loss 14.521914\n",
      "Loss  5.1108837 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1089177 C_bot  0.15 k_c 0.0\n",
      "282 Train Loss 14.499157\n",
      "Loss  5.1089177 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1067824 C_bot  0.15 k_c 0.0\n",
      "283 Train Loss 14.476395\n",
      "Loss  5.1067824 C_bot  0.15 k_c 0.0\n",
      "Loss  5.104661 C_bot  0.15 k_c 0.0\n",
      "284 Train Loss 14.453806\n",
      "Loss  5.104661 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1027703 C_bot  0.15 k_c 0.0\n",
      "285 Train Loss 14.431603\n",
      "Loss  5.1027703 C_bot  0.15 k_c 0.0\n",
      "Loss  5.100569 C_bot  0.15 k_c 0.0\n",
      "286 Train Loss 14.409247\n",
      "Loss  5.100569 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0984797 C_bot  0.15 k_c 0.0\n",
      "287 Train Loss 14.387163\n",
      "Loss  5.0984797 C_bot  0.15 k_c 0.0\n",
      "Loss  5.09633 C_bot  0.15 k_c 0.0\n",
      "288 Train Loss 14.365171\n",
      "Loss  5.09633 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0941052 C_bot  0.15 k_c 0.0\n",
      "289 Train Loss 14.343253\n",
      "Loss  5.0941052 C_bot  0.15 k_c 0.0\n",
      "Loss  5.092017 C_bot  0.15 k_c 0.0\n",
      "290 Train Loss 14.321619\n",
      "Loss  5.092017 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0899534 C_bot  0.15 k_c 0.0\n",
      "291 Train Loss 14.300164\n",
      "Loss  5.0899534 C_bot  0.15 k_c 0.0\n",
      "Loss  5.087892 C_bot  0.15 k_c 0.0\n",
      "292 Train Loss 14.278857\n",
      "Loss  5.087892 C_bot  0.15 k_c 0.0\n",
      "Loss  5.085574 C_bot  0.15 k_c 0.0\n",
      "293 Train Loss 14.257433\n",
      "Loss  5.085574 C_bot  0.15 k_c 0.0\n",
      "Loss  5.083645 C_bot  0.15 k_c 0.0\n",
      "294 Train Loss 14.236545\n",
      "Loss  5.083645 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0812793 C_bot  0.15 k_c 0.0\n",
      "295 Train Loss 14.215363\n",
      "Loss  5.0812793 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0793123 C_bot  0.15 k_c 0.0\n",
      "296 Train Loss 14.194719\n",
      "Loss  5.0793123 C_bot  0.15 k_c 0.0\n",
      "Loss  5.077214 C_bot  0.15 k_c 0.0\n",
      "297 Train Loss 14.174084\n",
      "Loss  5.077214 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0750566 C_bot  0.15 k_c 0.0\n",
      "298 Train Loss 14.153526\n",
      "Loss  5.0750566 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0729065 C_bot  0.15 k_c 0.0\n",
      "299 Train Loss 14.133112\n",
      "Loss  5.0729065 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0709243 C_bot  0.15 k_c 0.0\n",
      "300 Train Loss 14.113001\n",
      "Loss  5.0709243 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0687084 C_bot  0.15 k_c 0.0\n",
      "301 Train Loss 14.09279\n",
      "Loss  5.0687084 C_bot  0.15 k_c 0.0\n",
      "Loss  5.066454 C_bot  0.15 k_c 0.0\n",
      "302 Train Loss 14.072672\n",
      "Loss  5.066454 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0643806 C_bot  0.15 k_c 0.0\n",
      "303 Train Loss 14.052865\n",
      "Loss  5.0643806 C_bot  0.15 k_c 0.0\n",
      "Loss  5.062282 C_bot  0.15 k_c 0.0\n",
      "304 Train Loss 14.033162\n",
      "Loss  5.062282 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0601687 C_bot  0.15 k_c 0.0\n",
      "305 Train Loss 14.013574\n",
      "Loss  5.0601687 C_bot  0.15 k_c 0.0\n",
      "Loss  5.057934 C_bot  0.15 k_c 0.0\n",
      "306 Train Loss 13.993991\n",
      "Loss  5.057934 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0557384 C_bot  0.15 k_c 0.0\n",
      "307 Train Loss 13.974573\n",
      "Loss  5.0557384 C_bot  0.15 k_c 0.0\n",
      "Loss  5.053593 C_bot  0.15 k_c 0.0\n",
      "308 Train Loss 13.95533\n",
      "Loss  5.053593 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0515585 C_bot  0.15 k_c 0.0\n",
      "309 Train Loss 13.93632\n",
      "Loss  5.0515585 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0493145 C_bot  0.15 k_c 0.0\n",
      "310 Train Loss 13.917223\n",
      "Loss  5.0493145 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0472164 C_bot  0.15 k_c 0.0\n",
      "311 Train Loss 13.898394\n",
      "Loss  5.0472164 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0450783 C_bot  0.15 k_c 0.0\n",
      "312 Train Loss 13.879644\n",
      "Loss  5.0450783 C_bot  0.15 k_c 0.0\n",
      "Loss  5.043018 C_bot  0.15 k_c 0.0\n",
      "313 Train Loss 13.861092\n",
      "Loss  5.043018 C_bot  0.15 k_c 0.0\n",
      "Loss  5.040872 C_bot  0.15 k_c 0.0\n",
      "314 Train Loss 13.842567\n",
      "Loss  5.040872 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0387926 C_bot  0.15 k_c 0.0\n",
      "315 Train Loss 13.824229\n",
      "Loss  5.0387926 C_bot  0.15 k_c 0.0\n",
      "Loss  5.036675 C_bot  0.15 k_c 0.0\n",
      "316 Train Loss 13.80597\n",
      "Loss  5.036675 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0345907 C_bot  0.15 k_c 0.0\n",
      "317 Train Loss 13.787859\n",
      "Loss  5.0345907 C_bot  0.15 k_c 0.0\n",
      "Loss  5.032591 C_bot  0.15 k_c 0.0\n",
      "318 Train Loss 13.769944\n",
      "Loss  5.032591 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0303845 C_bot  0.15 k_c 0.0\n",
      "319 Train Loss 13.751933\n",
      "Loss  5.0303845 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0283856 C_bot  0.15 k_c 0.0\n",
      "320 Train Loss 13.734245\n",
      "Loss  5.0283856 C_bot  0.15 k_c 0.0\n",
      "Loss  5.026312 C_bot  0.15 k_c 0.0\n",
      "321 Train Loss 13.716596\n",
      "Loss  5.026312 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0243 C_bot  0.15 k_c 0.0\n",
      "322 Train Loss 13.699114\n",
      "Loss  5.0243 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0221853 C_bot  0.15 k_c 0.0\n",
      "323 Train Loss 13.681636\n",
      "Loss  5.0221853 C_bot  0.15 k_c 0.0\n",
      "Loss  5.020155 C_bot  0.15 k_c 0.0\n",
      "324 Train Loss 13.664352\n",
      "Loss  5.020155 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0182314 C_bot  0.15 k_c 0.0\n",
      "325 Train Loss 13.647286\n",
      "Loss  5.0182314 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0161495 C_bot  0.15 k_c 0.0\n",
      "326 Train Loss 13.630167\n",
      "Loss  5.0161495 C_bot  0.15 k_c 0.0\n",
      "Loss  5.014155 C_bot  0.15 k_c 0.0\n",
      "327 Train Loss 13.613238\n",
      "Loss  5.014155 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0121627 C_bot  0.15 k_c 0.0\n",
      "328 Train Loss 13.596416\n",
      "Loss  5.0121627 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0100756 C_bot  0.15 k_c 0.0\n",
      "329 Train Loss 13.579607\n",
      "Loss  5.0100756 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0081234 C_bot  0.15 k_c 0.0\n",
      "330 Train Loss 13.563035\n",
      "Loss  5.0081234 C_bot  0.15 k_c 0.0\n",
      "Loss  5.006215 C_bot  0.15 k_c 0.0\n",
      "331 Train Loss 13.546603\n",
      "Loss  5.006215 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0041957 C_bot  0.15 k_c 0.0\n",
      "332 Train Loss 13.530164\n",
      "Loss  5.0041957 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0023217 C_bot  0.15 k_c 0.0\n",
      "333 Train Loss 13.513973\n",
      "Loss  5.0023217 C_bot  0.15 k_c 0.0\n",
      "Loss  5.000534 C_bot  0.15 k_c 0.0\n",
      "334 Train Loss 13.49797\n",
      "Loss  5.000534 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9986324 C_bot  0.15 k_c 0.0\n",
      "335 Train Loss 13.481948\n",
      "Loss  4.9986324 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9966855 C_bot  0.15 k_c 0.0\n",
      "336 Train Loss 13.465976\n",
      "Loss  4.9966855 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9947834 C_bot  0.15 k_c 0.0\n",
      "337 Train Loss 13.450148\n",
      "Loss  4.9947834 C_bot  0.15 k_c 0.0\n",
      "Loss  4.993153 C_bot  0.15 k_c 0.0\n",
      "338 Train Loss 13.4346895\n",
      "Loss  4.993153 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9911976 C_bot  0.15 k_c 0.0\n",
      "339 Train Loss 13.419002\n",
      "Loss  4.9911976 C_bot  0.15 k_c 0.0\n",
      "Loss  4.989445 C_bot  0.15 k_c 0.0\n",
      "340 Train Loss 13.403605\n",
      "Loss  4.989445 C_bot  0.15 k_c 0.0\n",
      "Loss  4.987795 C_bot  0.15 k_c 0.0\n",
      "341 Train Loss 13.388407\n",
      "Loss  4.987795 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9859405 C_bot  0.15 k_c 0.0\n",
      "342 Train Loss 13.373101\n",
      "Loss  4.9859405 C_bot  0.15 k_c 0.0\n",
      "Loss  4.984208 C_bot  0.15 k_c 0.0\n",
      "343 Train Loss 13.358006\n",
      "Loss  4.984208 C_bot  0.15 k_c 0.0\n",
      "Loss  4.982541 C_bot  0.15 k_c 0.0\n",
      "344 Train Loss 13.343065\n",
      "Loss  4.982541 C_bot  0.15 k_c 0.0\n",
      "Loss  4.980879 C_bot  0.15 k_c 0.0\n",
      "345 Train Loss 13.328218\n",
      "Loss  4.980879 C_bot  0.15 k_c 0.0\n",
      "Loss  4.979135 C_bot  0.15 k_c 0.0\n",
      "346 Train Loss 13.313382\n",
      "Loss  4.979135 C_bot  0.15 k_c 0.0\n",
      "Loss  4.977357 C_bot  0.15 k_c 0.0\n",
      "347 Train Loss 13.298598\n",
      "Loss  4.977357 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9759917 C_bot  0.15 k_c 0.0\n",
      "348 Train Loss 13.284312\n",
      "Loss  4.9759917 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9742336 C_bot  0.15 k_c 0.0\n",
      "349 Train Loss 13.269718\n",
      "Loss  4.9742336 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9727 C_bot  0.15 k_c 0.0\n",
      "350 Train Loss 13.255437\n",
      "Loss  4.9727 C_bot  0.15 k_c 0.0\n",
      "Loss  4.971125 C_bot  0.15 k_c 0.0\n",
      "351 Train Loss 13.241197\n",
      "Loss  4.971125 C_bot  0.15 k_c 0.0\n",
      "Loss  4.969656 C_bot  0.15 k_c 0.0\n",
      "352 Train Loss 13.227147\n",
      "Loss  4.969656 C_bot  0.15 k_c 0.0\n",
      "Loss  4.968222 C_bot  0.15 k_c 0.0\n",
      "353 Train Loss 13.213211\n",
      "Loss  4.968222 C_bot  0.15 k_c 0.0\n",
      "Loss  4.966632 C_bot  0.15 k_c 0.0\n",
      "354 Train Loss 13.199201\n",
      "Loss  4.966632 C_bot  0.15 k_c 0.0\n",
      "Loss  4.965107 C_bot  0.15 k_c 0.0\n",
      "355 Train Loss 13.185338\n",
      "Loss  4.965107 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9637003 C_bot  0.15 k_c 0.0\n",
      "356 Train Loss 13.171672\n",
      "Loss  4.9637003 C_bot  0.15 k_c 0.0\n",
      "Loss  4.962272 C_bot  0.15 k_c 0.0\n",
      "357 Train Loss 13.158059\n",
      "Loss  4.962272 C_bot  0.15 k_c 0.0\n",
      "Loss  4.960751 C_bot  0.15 k_c 0.0\n",
      "358 Train Loss 13.144432\n",
      "Loss  4.960751 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9594803 C_bot  0.15 k_c 0.0\n",
      "359 Train Loss 13.131134\n",
      "Loss  4.9594803 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9581 C_bot  0.15 k_c 0.0\n",
      "360 Train Loss 13.117801\n",
      "Loss  4.9581 C_bot  0.15 k_c 0.0\n",
      "Loss  4.956671 C_bot  0.15 k_c 0.0\n",
      "361 Train Loss 13.104492\n",
      "Loss  4.956671 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9553037 C_bot  0.15 k_c 0.0\n",
      "362 Train Loss 13.091318\n",
      "Loss  4.9553037 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9542155 C_bot  0.15 k_c 0.0\n",
      "363 Train Loss 13.078497\n",
      "Loss  4.9542155 C_bot  0.15 k_c 0.0\n",
      "Loss  4.952853 C_bot  0.15 k_c 0.0\n",
      "364 Train Loss 13.065475\n",
      "Loss  4.952853 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9512873 C_bot  0.15 k_c 0.0\n",
      "365 Train Loss 13.052317\n",
      "Loss  4.9512873 C_bot  0.15 k_c 0.0\n",
      "Loss  4.950167 C_bot  0.15 k_c 0.0\n",
      "366 Train Loss 13.039677\n",
      "Loss  4.950167 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9489665 C_bot  0.15 k_c 0.0\n",
      "367 Train Loss 13.027023\n",
      "Loss  4.9489665 C_bot  0.15 k_c 0.0\n",
      "Loss  4.947702 C_bot  0.15 k_c 0.0\n",
      "368 Train Loss 13.014379\n",
      "Loss  4.947702 C_bot  0.15 k_c 0.0\n",
      "Loss  4.946518 C_bot  0.15 k_c 0.0\n",
      "369 Train Loss 13.001878\n",
      "Loss  4.946518 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9453187 C_bot  0.15 k_c 0.0\n",
      "370 Train Loss 12.98943\n",
      "Loss  4.9453187 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9440494 C_bot  0.15 k_c 0.0\n",
      "371 Train Loss 12.976979\n",
      "Loss  4.9440494 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9428043 C_bot  0.15 k_c 0.0\n",
      "372 Train Loss 12.964617\n",
      "Loss  4.9428043 C_bot  0.15 k_c 0.0\n",
      "Loss  4.941752 C_bot  0.15 k_c 0.0\n",
      "373 Train Loss 12.952512\n",
      "Loss  4.941752 C_bot  0.15 k_c 0.0\n",
      "Loss  4.940604 C_bot  0.15 k_c 0.0\n",
      "374 Train Loss 12.940374\n",
      "Loss  4.940604 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9394283 C_bot  0.15 k_c 0.0\n",
      "375 Train Loss 12.928272\n",
      "Loss  4.9394283 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9382944 C_bot  0.15 k_c 0.0\n",
      "376 Train Loss 12.916275\n",
      "Loss  4.9382944 C_bot  0.15 k_c 0.0\n",
      "Loss  4.937315 C_bot  0.15 k_c 0.0\n",
      "377 Train Loss 12.904494\n",
      "Loss  4.937315 C_bot  0.15 k_c 0.0\n",
      "Loss  4.936044 C_bot  0.15 k_c 0.0\n",
      "378 Train Loss 12.892485\n",
      "Loss  4.936044 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9351735 C_bot  0.15 k_c 0.0\n",
      "379 Train Loss 12.880932\n",
      "Loss  4.9351735 C_bot  0.15 k_c 0.0\n",
      "Loss  4.933889 C_bot  0.15 k_c 0.0\n",
      "380 Train Loss 12.869028\n",
      "Loss  4.933889 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9328613 C_bot  0.15 k_c 0.0\n",
      "381 Train Loss 12.857439\n",
      "Loss  4.9328613 C_bot  0.15 k_c 0.0\n",
      "Loss  4.931628 C_bot  0.15 k_c 0.0\n",
      "382 Train Loss 12.845704\n",
      "Loss  4.931628 C_bot  0.15 k_c 0.0\n",
      "Loss  4.930608 C_bot  0.15 k_c 0.0\n",
      "383 Train Loss 12.834241\n",
      "Loss  4.930608 C_bot  0.15 k_c 0.0\n",
      "Loss  4.929525 C_bot  0.15 k_c 0.0\n",
      "384 Train Loss 12.82277\n",
      "Loss  4.929525 C_bot  0.15 k_c 0.0\n",
      "Loss  4.928555 C_bot  0.15 k_c 0.0\n",
      "385 Train Loss 12.811472\n",
      "Loss  4.928555 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9273167 C_bot  0.15 k_c 0.0\n",
      "386 Train Loss 12.799961\n",
      "Loss  4.9273167 C_bot  0.15 k_c 0.0\n",
      "Loss  4.926345 C_bot  0.15 k_c 0.0\n",
      "387 Train Loss 12.788773\n",
      "Loss  4.926345 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9253416 C_bot  0.15 k_c 0.0\n",
      "388 Train Loss 12.777608\n",
      "Loss  4.9253416 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9242363 C_bot  0.15 k_c 0.0\n",
      "389 Train Loss 12.7663965\n",
      "Loss  4.9242363 C_bot  0.15 k_c 0.0\n",
      "Loss  4.923204 C_bot  0.15 k_c 0.0\n",
      "390 Train Loss 12.755312\n",
      "Loss  4.923204 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9221234 C_bot  0.15 k_c 0.0\n",
      "391 Train Loss 12.744234\n",
      "Loss  4.9221234 C_bot  0.15 k_c 0.0\n",
      "Loss  4.921043 C_bot  0.15 k_c 0.0\n",
      "392 Train Loss 12.733209\n",
      "Loss  4.921043 C_bot  0.15 k_c 0.0\n",
      "Loss  4.920013 C_bot  0.15 k_c 0.0\n",
      "393 Train Loss 12.722288\n",
      "Loss  4.920013 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9189067 C_bot  0.15 k_c 0.0\n",
      "394 Train Loss 12.711346\n",
      "Loss  4.9189067 C_bot  0.15 k_c 0.0\n",
      "Loss  4.917863 C_bot  0.15 k_c 0.0\n",
      "395 Train Loss 12.700514\n",
      "Loss  4.917863 C_bot  0.15 k_c 0.0\n",
      "Loss  4.916803 C_bot  0.15 k_c 0.0\n",
      "396 Train Loss 12.689719\n",
      "Loss  4.916803 C_bot  0.15 k_c 0.0\n",
      "Loss  4.915672 C_bot  0.15 k_c 0.0\n",
      "397 Train Loss 12.678908\n",
      "Loss  4.915672 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9144306 C_bot  0.15 k_c 0.0\n",
      "398 Train Loss 12.668038\n",
      "Loss  4.9144306 C_bot  0.15 k_c 0.0\n",
      "Loss  4.913296 C_bot  0.15 k_c 0.0\n",
      "399 Train Loss 12.657323\n",
      "Loss  4.913296 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9123545 C_bot  0.15 k_c 0.0\n",
      "400 Train Loss 12.646851\n",
      "Loss  4.9123545 C_bot  0.15 k_c 0.0\n",
      "Loss  4.911175 C_bot  0.15 k_c 0.0\n",
      "401 Train Loss 12.636194\n",
      "Loss  4.911175 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9100757 C_bot  0.15 k_c 0.0\n",
      "402 Train Loss 12.625669\n",
      "Loss  4.9100757 C_bot  0.15 k_c 0.0\n",
      "Loss  4.908941 C_bot  0.15 k_c 0.0\n",
      "403 Train Loss 12.615153\n",
      "Loss  4.908941 C_bot  0.15 k_c 0.0\n",
      "Loss  4.90772 C_bot  0.15 k_c 0.0\n",
      "404 Train Loss 12.604602\n",
      "Loss  4.90772 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9065585 C_bot  0.15 k_c 0.0\n",
      "405 Train Loss 12.594161\n",
      "Loss  4.9065585 C_bot  0.15 k_c 0.0\n",
      "Loss  4.905389 C_bot  0.15 k_c 0.0\n",
      "406 Train Loss 12.58376\n",
      "Loss  4.905389 C_bot  0.15 k_c 0.0\n",
      "Loss  4.904126 C_bot  0.15 k_c 0.0\n",
      "407 Train Loss 12.573314\n",
      "Loss  4.904126 C_bot  0.15 k_c 0.0\n",
      "Loss  4.903007 C_bot  0.15 k_c 0.0\n",
      "408 Train Loss 12.563056\n",
      "Loss  4.903007 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9016786 C_bot  0.15 k_c 0.0\n",
      "409 Train Loss 12.552642\n",
      "Loss  4.9016786 C_bot  0.15 k_c 0.0\n",
      "Loss  4.900428 C_bot  0.15 k_c 0.0\n",
      "410 Train Loss 12.542354\n",
      "Loss  4.900428 C_bot  0.15 k_c 0.0\n",
      "Loss  4.899282 C_bot  0.15 k_c 0.0\n",
      "411 Train Loss 12.532216\n",
      "Loss  4.899282 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8978724 C_bot  0.15 k_c 0.0\n",
      "412 Train Loss 12.521859\n",
      "Loss  4.8978724 C_bot  0.15 k_c 0.0\n",
      "Loss  4.896385 C_bot  0.15 k_c 0.0\n",
      "413 Train Loss 12.511475\n",
      "Loss  4.896385 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8951883 C_bot  0.15 k_c 0.0\n",
      "414 Train Loss 12.5014305\n",
      "Loss  4.8951883 C_bot  0.15 k_c 0.0\n",
      "Loss  4.893878 C_bot  0.15 k_c 0.0\n",
      "415 Train Loss 12.491316\n",
      "Loss  4.893878 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8923707 C_bot  0.15 k_c 0.0\n",
      "416 Train Loss 12.481047\n",
      "Loss  4.8923707 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8910546 C_bot  0.15 k_c 0.0\n",
      "417 Train Loss 12.471023\n",
      "Loss  4.8910546 C_bot  0.15 k_c 0.0\n",
      "Loss  4.889486 C_bot  0.15 k_c 0.0\n",
      "418 Train Loss 12.46079\n",
      "Loss  4.889486 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8879695 C_bot  0.15 k_c 0.0\n",
      "419 Train Loss 12.450654\n",
      "Loss  4.8879695 C_bot  0.15 k_c 0.0\n",
      "Loss  4.886567 C_bot  0.15 k_c 0.0\n",
      "420 Train Loss 12.440678\n",
      "Loss  4.886567 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8849425 C_bot  0.15 k_c 0.0\n",
      "421 Train Loss 12.430525\n",
      "Loss  4.8849425 C_bot  0.15 k_c 0.0\n",
      "Loss  4.883326 C_bot  0.15 k_c 0.0\n",
      "422 Train Loss 12.420428\n",
      "Loss  4.883326 C_bot  0.15 k_c 0.0\n",
      "Loss  4.881704 C_bot  0.15 k_c 0.0\n",
      "423 Train Loss 12.410368\n",
      "Loss  4.881704 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8802376 C_bot  0.15 k_c 0.0\n",
      "424 Train Loss 12.400513\n",
      "Loss  4.8802376 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8783336 C_bot  0.15 k_c 0.0\n",
      "425 Train Loss 12.390265\n",
      "Loss  4.8783336 C_bot  0.15 k_c 0.0\n",
      "Loss  4.876611 C_bot  0.15 k_c 0.0\n",
      "426 Train Loss 12.380243\n",
      "Loss  4.876611 C_bot  0.15 k_c 0.0\n",
      "Loss  4.874724 C_bot  0.15 k_c 0.0\n",
      "427 Train Loss 12.370102\n",
      "Loss  4.874724 C_bot  0.15 k_c 0.0\n",
      "Loss  4.872873 C_bot  0.15 k_c 0.0\n",
      "428 Train Loss 12.360042\n",
      "Loss  4.872873 C_bot  0.15 k_c 0.0\n",
      "Loss  4.871056 C_bot  0.15 k_c 0.0\n",
      "429 Train Loss 12.350061\n",
      "Loss  4.871056 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8691416 C_bot  0.15 k_c 0.0\n",
      "430 Train Loss 12.34003\n",
      "Loss  4.8691416 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8671336 C_bot  0.15 k_c 0.0\n",
      "431 Train Loss 12.329948\n",
      "Loss  4.8671336 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8651447 C_bot  0.15 k_c 0.0\n",
      "432 Train Loss 12.319933\n",
      "Loss  4.8651447 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8630285 C_bot  0.15 k_c 0.0\n",
      "433 Train Loss 12.309835\n",
      "Loss  4.8630285 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8607883 C_bot  0.15 k_c 0.0\n",
      "434 Train Loss 12.29966\n",
      "Loss  4.8607883 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8585587 C_bot  0.15 k_c 0.0\n",
      "435 Train Loss 12.289539\n",
      "Loss  4.8585587 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8562865 C_bot  0.15 k_c 0.0\n",
      "436 Train Loss 12.279425\n",
      "Loss  4.8562865 C_bot  0.15 k_c 0.0\n",
      "Loss  4.853873 C_bot  0.15 k_c 0.0\n",
      "437 Train Loss 12.269214\n",
      "Loss  4.853873 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8514867 C_bot  0.15 k_c 0.0\n",
      "438 Train Loss 12.259075\n",
      "Loss  4.8514867 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8489594 C_bot  0.15 k_c 0.0\n",
      "439 Train Loss 12.248843\n",
      "Loss  4.8489594 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8463783 C_bot  0.15 k_c 0.0\n",
      "440 Train Loss 12.238605\n",
      "Loss  4.8463783 C_bot  0.15 k_c 0.0\n",
      "Loss  4.843548 C_bot  0.15 k_c 0.0\n",
      "441 Train Loss 12.228162\n",
      "Loss  4.843548 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8407254 C_bot  0.15 k_c 0.0\n",
      "442 Train Loss 12.217776\n",
      "Loss  4.8407254 C_bot  0.15 k_c 0.0\n",
      "Loss  4.837941 C_bot  0.15 k_c 0.0\n",
      "443 Train Loss 12.207473\n",
      "Loss  4.837941 C_bot  0.15 k_c 0.0\n",
      "Loss  4.835034 C_bot  0.15 k_c 0.0\n",
      "444 Train Loss 12.197097\n",
      "Loss  4.835034 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8318453 C_bot  0.15 k_c 0.0\n",
      "445 Train Loss 12.186488\n",
      "Loss  4.8318453 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8287015 C_bot  0.15 k_c 0.0\n",
      "446 Train Loss 12.175972\n",
      "Loss  4.8287015 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8254614 C_bot  0.15 k_c 0.0\n",
      "447 Train Loss 12.165409\n",
      "Loss  4.8254614 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8220863 C_bot  0.15 k_c 0.0\n",
      "448 Train Loss 12.1547575\n",
      "Loss  4.8220863 C_bot  0.15 k_c 0.0\n",
      "Loss  4.818605 C_bot  0.15 k_c 0.0\n",
      "449 Train Loss 12.144052\n",
      "Loss  4.818605 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8150706 C_bot  0.15 k_c 0.0\n",
      "450 Train Loss 12.133341\n",
      "Loss  4.8150706 C_bot  0.15 k_c 0.0\n",
      "Loss  4.811217 C_bot  0.15 k_c 0.0\n",
      "451 Train Loss 12.122362\n",
      "Loss  4.811217 C_bot  0.15 k_c 0.0\n",
      "Loss  4.807373 C_bot  0.15 k_c 0.0\n",
      "452 Train Loss 12.111443\n",
      "Loss  4.807373 C_bot  0.15 k_c 0.0\n",
      "Loss  4.803168 C_bot  0.15 k_c 0.0\n",
      "453 Train Loss 12.100213\n",
      "Loss  4.803168 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7990627 C_bot  0.15 k_c 0.0\n",
      "454 Train Loss 12.089136\n",
      "Loss  4.7990627 C_bot  0.15 k_c 0.0\n",
      "Loss  4.794824 C_bot  0.15 k_c 0.0\n",
      "455 Train Loss 12.077977\n",
      "Loss  4.794824 C_bot  0.15 k_c 0.0\n",
      "Loss  4.790353 C_bot  0.15 k_c 0.0\n",
      "456 Train Loss 12.066638\n",
      "Loss  4.790353 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7856317 C_bot  0.15 k_c 0.0\n",
      "457 Train Loss 12.0550995\n",
      "Loss  4.7856317 C_bot  0.15 k_c 0.0\n",
      "Loss  4.780741 C_bot  0.15 k_c 0.0\n",
      "458 Train Loss 12.0434475\n",
      "Loss  4.780741 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7757735 C_bot  0.15 k_c 0.0\n",
      "459 Train Loss 12.031769\n",
      "Loss  4.7757735 C_bot  0.15 k_c 0.0\n",
      "Loss  4.770809 C_bot  0.15 k_c 0.0\n",
      "460 Train Loss 12.020148\n",
      "Loss  4.770809 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7655063 C_bot  0.15 k_c 0.0\n",
      "461 Train Loss 12.0082445\n",
      "Loss  4.7655063 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7600427 C_bot  0.15 k_c 0.0\n",
      "462 Train Loss 11.996233\n",
      "Loss  4.7600427 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7543774 C_bot  0.15 k_c 0.0\n",
      "463 Train Loss 11.98407\n",
      "Loss  4.7543774 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7485414 C_bot  0.15 k_c 0.0\n",
      "464 Train Loss 11.971794\n",
      "Loss  4.7485414 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7426033 C_bot  0.15 k_c 0.0\n",
      "465 Train Loss 11.959471\n",
      "Loss  4.7426033 C_bot  0.15 k_c 0.0\n",
      "Loss  4.736262 C_bot  0.15 k_c 0.0\n",
      "466 Train Loss 11.9467945\n",
      "Loss  4.736262 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7297955 C_bot  0.15 k_c 0.0\n",
      "467 Train Loss 11.934048\n",
      "Loss  4.7297955 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7231994 C_bot  0.15 k_c 0.0\n",
      "468 Train Loss 11.921227\n",
      "Loss  4.7231994 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7165046 C_bot  0.15 k_c 0.0\n",
      "469 Train Loss 11.908358\n",
      "Loss  4.7165046 C_bot  0.15 k_c 0.0\n",
      "Loss  4.709557 C_bot  0.15 k_c 0.0\n",
      "470 Train Loss 11.895286\n",
      "Loss  4.709557 C_bot  0.15 k_c 0.0\n",
      "Loss  4.70223 C_bot  0.15 k_c 0.0\n",
      "471 Train Loss 11.881888\n",
      "Loss  4.70223 C_bot  0.15 k_c 0.0\n",
      "Loss  4.69509 C_bot  0.15 k_c 0.0\n",
      "472 Train Loss 11.868727\n",
      "Loss  4.69509 C_bot  0.15 k_c 0.0\n",
      "Loss  4.687527 C_bot  0.15 k_c 0.0\n",
      "473 Train Loss 11.855188\n",
      "Loss  4.687527 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6799383 C_bot  0.15 k_c 0.0\n",
      "474 Train Loss 11.84167\n",
      "Loss  4.6799383 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6720953 C_bot  0.15 k_c 0.0\n",
      "475 Train Loss 11.827948\n",
      "Loss  4.6720953 C_bot  0.15 k_c 0.0\n",
      "Loss  4.664128 C_bot  0.15 k_c 0.0\n",
      "476 Train Loss 11.814143\n",
      "Loss  4.664128 C_bot  0.15 k_c 0.0\n",
      "Loss  4.656131 C_bot  0.15 k_c 0.0\n",
      "477 Train Loss 11.800345\n",
      "Loss  4.656131 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6479115 C_bot  0.15 k_c 0.0\n",
      "478 Train Loss 11.786367\n",
      "Loss  4.6479115 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6395483 C_bot  0.15 k_c 0.0\n",
      "479 Train Loss 11.772285\n",
      "Loss  4.6395483 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6313057 C_bot  0.15 k_c 0.0\n",
      "480 Train Loss 11.758353\n",
      "Loss  4.6313057 C_bot  0.15 k_c 0.0\n",
      "Loss  4.622786 C_bot  0.15 k_c 0.0\n",
      "481 Train Loss 11.744173\n",
      "Loss  4.622786 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6142983 C_bot  0.15 k_c 0.0\n",
      "482 Train Loss 11.730056\n",
      "Loss  4.6142983 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6058936 C_bot  0.15 k_c 0.0\n",
      "483 Train Loss 11.716047\n",
      "Loss  4.6058936 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5973053 C_bot  0.15 k_c 0.0\n",
      "484 Train Loss 11.701873\n",
      "Loss  4.5973053 C_bot  0.15 k_c 0.0\n",
      "Loss  4.588748 C_bot  0.15 k_c 0.0\n",
      "485 Train Loss 11.687747\n",
      "Loss  4.588748 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5805078 C_bot  0.15 k_c 0.0\n",
      "486 Train Loss 11.673953\n",
      "Loss  4.5805078 C_bot  0.15 k_c 0.0\n",
      "Loss  4.572178 C_bot  0.15 k_c 0.0\n",
      "487 Train Loss 11.660081\n",
      "Loss  4.572178 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5639925 C_bot  0.15 k_c 0.0\n",
      "488 Train Loss 11.6463585\n",
      "Loss  4.5639925 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5558357 C_bot  0.15 k_c 0.0\n",
      "489 Train Loss 11.632662\n",
      "Loss  4.5558357 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5480404 C_bot  0.15 k_c 0.0\n",
      "490 Train Loss 11.619333\n",
      "Loss  4.5480404 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5403295 C_bot  0.15 k_c 0.0\n",
      "491 Train Loss 11.606083\n",
      "Loss  4.5403295 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5325413 C_bot  0.15 k_c 0.0\n",
      "492 Train Loss 11.592742\n",
      "Loss  4.5325413 C_bot  0.15 k_c 0.0\n",
      "Loss  4.525174 C_bot  0.15 k_c 0.0\n",
      "493 Train Loss 11.579814\n",
      "Loss  4.525174 C_bot  0.15 k_c 0.0\n",
      "Loss  4.518044 C_bot  0.15 k_c 0.0\n",
      "494 Train Loss 11.567113\n",
      "Loss  4.518044 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5112343 C_bot  0.15 k_c 0.0\n",
      "495 Train Loss 11.554707\n",
      "Loss  4.5112343 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5046277 C_bot  0.15 k_c 0.0\n",
      "496 Train Loss 11.542486\n",
      "Loss  4.5046277 C_bot  0.15 k_c 0.0\n",
      "Loss  4.498271 C_bot  0.15 k_c 0.0\n",
      "497 Train Loss 11.530495\n",
      "Loss  4.498271 C_bot  0.15 k_c 0.0\n",
      "Loss  4.492217 C_bot  0.15 k_c 0.0\n",
      "498 Train Loss 11.518781\n",
      "Loss  4.492217 C_bot  0.15 k_c 0.0\n",
      "Loss  4.486467 C_bot  0.15 k_c 0.0\n",
      "499 Train Loss 11.5073395\n",
      "Loss  4.486467 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4808645 C_bot  0.15 k_c 0.0\n",
      "500 Train Loss 11.49602\n",
      "Loss  4.4808645 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4757943 C_bot  0.15 k_c 0.0\n",
      "501 Train Loss 11.485204\n",
      "Loss  4.4757943 C_bot  0.15 k_c 0.0\n",
      "Loss  4.470914 C_bot  0.15 k_c 0.0\n",
      "502 Train Loss 11.474545\n",
      "Loss  4.470914 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4660916 C_bot  0.15 k_c 0.0\n",
      "503 Train Loss 11.463911\n",
      "Loss  4.4660916 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4617305 C_bot  0.15 k_c 0.0\n",
      "504 Train Loss 11.453708\n",
      "Loss  4.4617305 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4576826 C_bot  0.15 k_c 0.0\n",
      "505 Train Loss 11.443787\n",
      "Loss  4.4576826 C_bot  0.15 k_c 0.0\n",
      "Loss  4.453672 C_bot  0.15 k_c 0.0\n",
      "506 Train Loss 11.433869\n",
      "Loss  4.453672 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4500666 C_bot  0.15 k_c 0.0\n",
      "507 Train Loss 11.424326\n",
      "Loss  4.4500666 C_bot  0.15 k_c 0.0\n",
      "Loss  4.446575 C_bot  0.15 k_c 0.0\n",
      "508 Train Loss 11.414867\n",
      "Loss  4.446575 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4433146 C_bot  0.15 k_c 0.0\n",
      "509 Train Loss 11.40561\n",
      "Loss  4.4433146 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4403024 C_bot  0.15 k_c 0.0\n",
      "510 Train Loss 11.396574\n",
      "Loss  4.4403024 C_bot  0.15 k_c 0.0\n",
      "Loss  4.437405 C_bot  0.15 k_c 0.0\n",
      "511 Train Loss 11.387626\n",
      "Loss  4.437405 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4346075 C_bot  0.15 k_c 0.0\n",
      "512 Train Loss 11.378754\n",
      "Loss  4.4346075 C_bot  0.15 k_c 0.0\n",
      "Loss  4.432034 C_bot  0.15 k_c 0.0\n",
      "513 Train Loss 11.370086\n",
      "Loss  4.432034 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4295635 C_bot  0.15 k_c 0.0\n",
      "514 Train Loss 11.361499\n",
      "Loss  4.4295635 C_bot  0.15 k_c 0.0\n",
      "Loss  4.427081 C_bot  0.15 k_c 0.0\n",
      "515 Train Loss 11.352884\n",
      "Loss  4.427081 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4248624 C_bot  0.15 k_c 0.0\n",
      "516 Train Loss 11.344519\n",
      "Loss  4.4248624 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4226475 C_bot  0.15 k_c 0.0\n",
      "517 Train Loss 11.336145\n",
      "Loss  4.4226475 C_bot  0.15 k_c 0.0\n",
      "Loss  4.420684 C_bot  0.15 k_c 0.0\n",
      "518 Train Loss 11.328018\n",
      "Loss  4.420684 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4186687 C_bot  0.15 k_c 0.0\n",
      "519 Train Loss 11.319833\n",
      "Loss  4.4186687 C_bot  0.15 k_c 0.0\n",
      "Loss  4.416694 C_bot  0.15 k_c 0.0\n",
      "520 Train Loss 11.311684\n",
      "Loss  4.416694 C_bot  0.15 k_c 0.0\n",
      "Loss  4.414745 C_bot  0.15 k_c 0.0\n",
      "521 Train Loss 11.30356\n",
      "Loss  4.414745 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4128475 C_bot  0.15 k_c 0.0\n",
      "522 Train Loss 11.295498\n",
      "Loss  4.4128475 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4111185 C_bot  0.15 k_c 0.0\n",
      "523 Train Loss 11.287607\n",
      "Loss  4.4111185 C_bot  0.15 k_c 0.0\n",
      "Loss  4.409352 C_bot  0.15 k_c 0.0\n",
      "524 Train Loss 11.279687\n",
      "Loss  4.409352 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4076953 C_bot  0.15 k_c 0.0\n",
      "525 Train Loss 11.271894\n",
      "Loss  4.4076953 C_bot  0.15 k_c 0.0\n",
      "Loss  4.406023 C_bot  0.15 k_c 0.0\n",
      "526 Train Loss 11.264101\n",
      "Loss  4.406023 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4043317 C_bot  0.15 k_c 0.0\n",
      "527 Train Loss 11.256306\n",
      "Loss  4.4043317 C_bot  0.15 k_c 0.0\n",
      "Loss  4.402757 C_bot  0.15 k_c 0.0\n",
      "528 Train Loss 11.248651\n",
      "Loss  4.402757 C_bot  0.15 k_c 0.0\n",
      "Loss  4.401064 C_bot  0.15 k_c 0.0\n",
      "529 Train Loss 11.240899\n",
      "Loss  4.401064 C_bot  0.15 k_c 0.0\n",
      "Loss  4.399657 C_bot  0.15 k_c 0.0\n",
      "530 Train Loss 11.23346\n",
      "Loss  4.399657 C_bot  0.15 k_c 0.0\n",
      "Loss  4.398168 C_bot  0.15 k_c 0.0\n",
      "531 Train Loss 11.225969\n",
      "Loss  4.398168 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3967047 C_bot  0.15 k_c 0.0\n",
      "532 Train Loss 11.218534\n",
      "Loss  4.3967047 C_bot  0.15 k_c 0.0\n",
      "Loss  4.395215 C_bot  0.15 k_c 0.0\n",
      "533 Train Loss 11.2111025\n",
      "Loss  4.395215 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3938785 C_bot  0.15 k_c 0.0\n",
      "534 Train Loss 11.203859\n",
      "Loss  4.3938785 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3924327 C_bot  0.15 k_c 0.0\n",
      "535 Train Loss 11.196545\n",
      "Loss  4.3924327 C_bot  0.15 k_c 0.0\n",
      "Loss  4.391029 C_bot  0.15 k_c 0.0\n",
      "536 Train Loss 11.189305\n",
      "Loss  4.391029 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3895974 C_bot  0.15 k_c 0.0\n",
      "537 Train Loss 11.1820755\n",
      "Loss  4.3895974 C_bot  0.15 k_c 0.0\n",
      "Loss  4.388399 C_bot  0.15 k_c 0.0\n",
      "538 Train Loss 11.175119\n",
      "Loss  4.388399 C_bot  0.15 k_c 0.0\n",
      "Loss  4.386934 C_bot  0.15 k_c 0.0\n",
      "539 Train Loss 11.167933\n",
      "Loss  4.386934 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3857193 C_bot  0.15 k_c 0.0\n",
      "540 Train Loss 11.161034\n",
      "Loss  4.3857193 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3844814 C_bot  0.15 k_c 0.0\n",
      "541 Train Loss 11.154156\n",
      "Loss  4.3844814 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3832407 C_bot  0.15 k_c 0.0\n",
      "542 Train Loss 11.147311\n",
      "Loss  4.3832407 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3818636 C_bot  0.15 k_c 0.0\n",
      "543 Train Loss 11.140367\n",
      "Loss  4.3818636 C_bot  0.15 k_c 0.0\n",
      "Loss  4.380679 C_bot  0.15 k_c 0.0\n",
      "544 Train Loss 11.133655\n",
      "Loss  4.380679 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3793454 C_bot  0.15 k_c 0.0\n",
      "545 Train Loss 11.126832\n",
      "Loss  4.3793454 C_bot  0.15 k_c 0.0\n",
      "Loss  4.37816 C_bot  0.15 k_c 0.0\n",
      "546 Train Loss 11.120193\n",
      "Loss  4.37816 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3769317 C_bot  0.15 k_c 0.0\n",
      "547 Train Loss 11.113545\n",
      "Loss  4.3769317 C_bot  0.15 k_c 0.0\n",
      "Loss  4.375741 C_bot  0.15 k_c 0.0\n",
      "548 Train Loss 11.106976\n",
      "Loss  4.375741 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3745027 C_bot  0.15 k_c 0.0\n",
      "549 Train Loss 11.10039\n",
      "Loss  4.3745027 C_bot  0.15 k_c 0.0\n",
      "Loss  4.373374 C_bot  0.15 k_c 0.0\n",
      "550 Train Loss 11.093948\n",
      "Loss  4.373374 C_bot  0.15 k_c 0.0\n",
      "Loss  4.372125 C_bot  0.15 k_c 0.0\n",
      "551 Train Loss 11.08742\n",
      "Loss  4.372125 C_bot  0.15 k_c 0.0\n",
      "Loss  4.37094 C_bot  0.15 k_c 0.0\n",
      "552 Train Loss 11.080984\n",
      "Loss  4.37094 C_bot  0.15 k_c 0.0\n",
      "Loss  4.369769 C_bot  0.15 k_c 0.0\n",
      "553 Train Loss 11.074595\n",
      "Loss  4.369769 C_bot  0.15 k_c 0.0\n",
      "Loss  4.368506 C_bot  0.15 k_c 0.0\n",
      "554 Train Loss 11.068143\n",
      "Loss  4.368506 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3674345 C_bot  0.15 k_c 0.0\n",
      "555 Train Loss 11.061909\n",
      "Loss  4.3674345 C_bot  0.15 k_c 0.0\n",
      "Loss  4.366204 C_bot  0.15 k_c 0.0\n",
      "556 Train Loss 11.055547\n",
      "Loss  4.366204 C_bot  0.15 k_c 0.0\n",
      "Loss  4.36503 C_bot  0.15 k_c 0.0\n",
      "557 Train Loss 11.049267\n",
      "Loss  4.36503 C_bot  0.15 k_c 0.0\n",
      "Loss  4.363879 C_bot  0.15 k_c 0.0\n",
      "558 Train Loss 11.043035\n",
      "Loss  4.363879 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3627286 C_bot  0.15 k_c 0.0\n",
      "559 Train Loss 11.036826\n",
      "Loss  4.3627286 C_bot  0.15 k_c 0.0\n",
      "Loss  4.361661 C_bot  0.15 k_c 0.0\n",
      "560 Train Loss 11.030727\n",
      "Loss  4.361661 C_bot  0.15 k_c 0.0\n",
      "Loss  4.360549 C_bot  0.15 k_c 0.0\n",
      "561 Train Loss 11.024605\n",
      "Loss  4.360549 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3593383 C_bot  0.15 k_c 0.0\n",
      "562 Train Loss 11.018404\n",
      "Loss  4.3593383 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3583894 C_bot  0.15 k_c 0.0\n",
      "563 Train Loss 11.012489\n",
      "Loss  4.3583894 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3573346 C_bot  0.15 k_c 0.0\n",
      "564 Train Loss 11.006487\n",
      "Loss  4.3573346 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3561516 C_bot  0.15 k_c 0.0\n",
      "565 Train Loss 11.000375\n",
      "Loss  4.3561516 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3550305 C_bot  0.15 k_c 0.0\n",
      "566 Train Loss 10.994347\n",
      "Loss  4.3550305 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3540363 C_bot  0.15 k_c 0.0\n",
      "567 Train Loss 10.9884615\n",
      "Loss  4.3540363 C_bot  0.15 k_c 0.0\n",
      "Loss  4.352968 C_bot  0.15 k_c 0.0\n",
      "568 Train Loss 10.982521\n",
      "Loss  4.352968 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3519673 C_bot  0.15 k_c 0.0\n",
      "569 Train Loss 10.9766655\n",
      "Loss  4.3519673 C_bot  0.15 k_c 0.0\n",
      "Loss  4.350921 C_bot  0.15 k_c 0.0\n",
      "570 Train Loss 10.970781\n",
      "Loss  4.350921 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3497367 C_bot  0.15 k_c 0.0\n",
      "571 Train Loss 10.964776\n",
      "Loss  4.3497367 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3488045 C_bot  0.15 k_c 0.0\n",
      "572 Train Loss 10.959037\n",
      "Loss  4.3488045 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3477507 C_bot  0.15 k_c 0.0\n",
      "573 Train Loss 10.953196\n",
      "Loss  4.3477507 C_bot  0.15 k_c 0.0\n",
      "Loss  4.346801 C_bot  0.15 k_c 0.0\n",
      "574 Train Loss 10.94747\n",
      "Loss  4.346801 C_bot  0.15 k_c 0.0\n",
      "Loss  4.345729 C_bot  0.15 k_c 0.0\n",
      "575 Train Loss 10.941637\n",
      "Loss  4.345729 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3445706 C_bot  0.15 k_c 0.0\n",
      "576 Train Loss 10.935737\n",
      "Loss  4.3445706 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3437924 C_bot  0.15 k_c 0.0\n",
      "577 Train Loss 10.930225\n",
      "Loss  4.3437924 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3426266 C_bot  0.15 k_c 0.0\n",
      "578 Train Loss 10.924344\n",
      "Loss  4.3426266 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3418736 C_bot  0.15 k_c 0.0\n",
      "579 Train Loss 10.91889\n",
      "Loss  4.3418736 C_bot  0.15 k_c 0.0\n",
      "Loss  4.340779 C_bot  0.15 k_c 0.0\n",
      "580 Train Loss 10.913104\n",
      "Loss  4.340779 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3398027 C_bot  0.15 k_c 0.0\n",
      "581 Train Loss 10.907455\n",
      "Loss  4.3398027 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3388724 C_bot  0.15 k_c 0.0\n",
      "582 Train Loss 10.901864\n",
      "Loss  4.3388724 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3379436 C_bot  0.15 k_c 0.0\n",
      "583 Train Loss 10.896286\n",
      "Loss  4.3379436 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3369107 C_bot  0.15 k_c 0.0\n",
      "584 Train Loss 10.890619\n",
      "Loss  4.3369107 C_bot  0.15 k_c 0.0\n",
      "Loss  4.335951 C_bot  0.15 k_c 0.0\n",
      "585 Train Loss 10.885038\n",
      "Loss  4.335951 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3349576 C_bot  0.15 k_c 0.0\n",
      "586 Train Loss 10.879434\n",
      "Loss  4.3349576 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3341045 C_bot  0.15 k_c 0.0\n",
      "587 Train Loss 10.873987\n",
      "Loss  4.3341045 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3331347 C_bot  0.15 k_c 0.0\n",
      "588 Train Loss 10.868436\n",
      "Loss  4.3331347 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3321767 C_bot  0.15 k_c 0.0\n",
      "589 Train Loss 10.862907\n",
      "Loss  4.3321767 C_bot  0.15 k_c 0.0\n",
      "Loss  4.331145 C_bot  0.15 k_c 0.0\n",
      "590 Train Loss 10.857319\n",
      "Loss  4.331145 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3302593 C_bot  0.15 k_c 0.0\n",
      "591 Train Loss 10.851887\n",
      "Loss  4.3302593 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3293424 C_bot  0.15 k_c 0.0\n",
      "592 Train Loss 10.8464365\n",
      "Loss  4.3293424 C_bot  0.15 k_c 0.0\n",
      "Loss  4.328519 C_bot  0.15 k_c 0.0\n",
      "593 Train Loss 10.841097\n",
      "Loss  4.328519 C_bot  0.15 k_c 0.0\n",
      "Loss  4.327414 C_bot  0.15 k_c 0.0\n",
      "594 Train Loss 10.835484\n",
      "Loss  4.327414 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3266373 C_bot  0.15 k_c 0.0\n",
      "595 Train Loss 10.830212\n",
      "Loss  4.3266373 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3256307 C_bot  0.15 k_c 0.0\n",
      "596 Train Loss 10.824724\n",
      "Loss  4.3256307 C_bot  0.15 k_c 0.0\n",
      "Loss  4.324866 C_bot  0.15 k_c 0.0\n",
      "597 Train Loss 10.819488\n",
      "Loss  4.324866 C_bot  0.15 k_c 0.0\n",
      "Loss  4.323846 C_bot  0.15 k_c 0.0\n",
      "598 Train Loss 10.814009\n",
      "Loss  4.323846 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3228374 C_bot  0.15 k_c 0.0\n",
      "599 Train Loss 10.808557\n",
      "Loss  4.3228374 C_bot  0.15 k_c 0.0\n",
      "Loss  4.321919 C_bot  0.15 k_c 0.0\n",
      "600 Train Loss 10.803204\n",
      "Loss  4.321919 C_bot  0.15 k_c 0.0\n",
      "Loss  4.321046 C_bot  0.15 k_c 0.0\n",
      "601 Train Loss 10.797908\n",
      "Loss  4.321046 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3201885 C_bot  0.15 k_c 0.0\n",
      "602 Train Loss 10.792642\n",
      "Loss  4.3201885 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3192487 C_bot  0.15 k_c 0.0\n",
      "603 Train Loss 10.787304\n",
      "Loss  4.3192487 C_bot  0.15 k_c 0.0\n",
      "Loss  4.318258 C_bot  0.15 k_c 0.0\n",
      "604 Train Loss 10.781926\n",
      "Loss  4.318258 C_bot  0.15 k_c 0.0\n",
      "Loss  4.317455 C_bot  0.15 k_c 0.0\n",
      "605 Train Loss 10.776751\n",
      "Loss  4.317455 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3166037 C_bot  0.15 k_c 0.0\n",
      "606 Train Loss 10.771537\n",
      "Loss  4.3166037 C_bot  0.15 k_c 0.0\n",
      "Loss  4.315549 C_bot  0.15 k_c 0.0\n",
      "607 Train Loss 10.766131\n",
      "Loss  4.315549 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3146343 C_bot  0.15 k_c 0.0\n",
      "608 Train Loss 10.760878\n",
      "Loss  4.3146343 C_bot  0.15 k_c 0.0\n",
      "Loss  4.31372 C_bot  0.15 k_c 0.0\n",
      "609 Train Loss 10.755635\n",
      "Loss  4.31372 C_bot  0.15 k_c 0.0\n",
      "Loss  4.312874 C_bot  0.15 k_c 0.0\n",
      "610 Train Loss 10.750475\n",
      "Loss  4.312874 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3119845 C_bot  0.15 k_c 0.0\n",
      "611 Train Loss 10.745279\n",
      "Loss  4.3119845 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3111095 C_bot  0.15 k_c 0.0\n",
      "612 Train Loss 10.740112\n",
      "Loss  4.3111095 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3100734 C_bot  0.15 k_c 0.0\n",
      "613 Train Loss 10.734796\n",
      "Loss  4.3100734 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3092976 C_bot  0.15 k_c 0.0\n",
      "614 Train Loss 10.729747\n",
      "Loss  4.3092976 C_bot  0.15 k_c 0.0\n",
      "Loss  4.308307 C_bot  0.15 k_c 0.0\n",
      "615 Train Loss 10.724499\n",
      "Loss  4.308307 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3075004 C_bot  0.15 k_c 0.0\n",
      "616 Train Loss 10.719443\n",
      "Loss  4.3075004 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3066173 C_bot  0.15 k_c 0.0\n",
      "617 Train Loss 10.714322\n",
      "Loss  4.3066173 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3055654 C_bot  0.15 k_c 0.0\n",
      "618 Train Loss 10.709045\n",
      "Loss  4.3055654 C_bot  0.15 k_c 0.0\n",
      "Loss  4.304774 C_bot  0.15 k_c 0.0\n",
      "619 Train Loss 10.704038\n",
      "Loss  4.304774 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3037777 C_bot  0.15 k_c 0.0\n",
      "620 Train Loss 10.698837\n",
      "Loss  4.3037777 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3029227 C_bot  0.15 k_c 0.0\n",
      "621 Train Loss 10.6937895\n",
      "Loss  4.3029227 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3021536 C_bot  0.15 k_c 0.0\n",
      "622 Train Loss 10.688837\n",
      "Loss  4.3021536 C_bot  0.15 k_c 0.0\n",
      "Loss  4.301077 C_bot  0.15 k_c 0.0\n",
      "623 Train Loss 10.683589\n",
      "Loss  4.301077 C_bot  0.15 k_c 0.0\n",
      "Loss  4.300177 C_bot  0.15 k_c 0.0\n",
      "624 Train Loss 10.678526\n",
      "Loss  4.300177 C_bot  0.15 k_c 0.0\n",
      "Loss  4.299342 C_bot  0.15 k_c 0.0\n",
      "625 Train Loss 10.67354\n",
      "Loss  4.299342 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2983365 C_bot  0.15 k_c 0.0\n",
      "626 Train Loss 10.668394\n",
      "Loss  4.2983365 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2975063 C_bot  0.15 k_c 0.0\n",
      "627 Train Loss 10.663431\n",
      "Loss  4.2975063 C_bot  0.15 k_c 0.0\n",
      "Loss  4.296509 C_bot  0.15 k_c 0.0\n",
      "628 Train Loss 10.658315\n",
      "Loss  4.296509 C_bot  0.15 k_c 0.0\n",
      "Loss  4.295621 C_bot  0.15 k_c 0.0\n",
      "629 Train Loss 10.653317\n",
      "Loss  4.295621 C_bot  0.15 k_c 0.0\n",
      "Loss  4.294756 C_bot  0.15 k_c 0.0\n",
      "630 Train Loss 10.64835\n",
      "Loss  4.294756 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2937994 C_bot  0.15 k_c 0.0\n",
      "631 Train Loss 10.643303\n",
      "Loss  4.2937994 C_bot  0.15 k_c 0.0\n",
      "Loss  4.292892 C_bot  0.15 k_c 0.0\n",
      "632 Train Loss 10.638317\n",
      "Loss  4.292892 C_bot  0.15 k_c 0.0\n",
      "Loss  4.291996 C_bot  0.15 k_c 0.0\n",
      "633 Train Loss 10.633348\n",
      "Loss  4.291996 C_bot  0.15 k_c 0.0\n",
      "Loss  4.291003 C_bot  0.15 k_c 0.0\n",
      "634 Train Loss 10.628297\n",
      "Loss  4.291003 C_bot  0.15 k_c 0.0\n",
      "Loss  4.290174 C_bot  0.15 k_c 0.0\n",
      "635 Train Loss 10.623417\n",
      "Loss  4.290174 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2893143 C_bot  0.15 k_c 0.0\n",
      "636 Train Loss 10.618514\n",
      "Loss  4.2893143 C_bot  0.15 k_c 0.0\n",
      "Loss  4.288267 C_bot  0.15 k_c 0.0\n",
      "637 Train Loss 10.613437\n",
      "Loss  4.288267 C_bot  0.15 k_c 0.0\n",
      "Loss  4.287393 C_bot  0.15 k_c 0.0\n",
      "638 Train Loss 10.608539\n",
      "Loss  4.287393 C_bot  0.15 k_c 0.0\n",
      "Loss  4.286503 C_bot  0.15 k_c 0.0\n",
      "639 Train Loss 10.603636\n",
      "Loss  4.286503 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2856345 C_bot  0.15 k_c 0.0\n",
      "640 Train Loss 10.5987625\n",
      "Loss  4.2856345 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2847524 C_bot  0.15 k_c 0.0\n",
      "641 Train Loss 10.593886\n",
      "Loss  4.2847524 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2836576 C_bot  0.15 k_c 0.0\n",
      "642 Train Loss 10.588805\n",
      "Loss  4.2836576 C_bot  0.15 k_c 0.0\n",
      "Loss  4.282805 C_bot  0.15 k_c 0.0\n",
      "643 Train Loss 10.583977\n",
      "Loss  4.282805 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2818723 C_bot  0.15 k_c 0.0\n",
      "644 Train Loss 10.579076\n",
      "Loss  4.2818723 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2810335 C_bot  0.15 k_c 0.0\n",
      "645 Train Loss 10.574277\n",
      "Loss  4.2810335 C_bot  0.15 k_c 0.0\n",
      "Loss  4.279957 C_bot  0.15 k_c 0.0\n",
      "646 Train Loss 10.569252\n",
      "Loss  4.279957 C_bot  0.15 k_c 0.0\n",
      "Loss  4.279095 C_bot  0.15 k_c 0.0\n",
      "647 Train Loss 10.564446\n",
      "Loss  4.279095 C_bot  0.15 k_c 0.0\n",
      "Loss  4.27812 C_bot  0.15 k_c 0.0\n",
      "648 Train Loss 10.55954\n",
      "Loss  4.27812 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2772818 C_bot  0.15 k_c 0.0\n",
      "649 Train Loss 10.554776\n",
      "Loss  4.2772818 C_bot  0.15 k_c 0.0\n",
      "Loss  4.276335 C_bot  0.15 k_c 0.0\n",
      "650 Train Loss 10.549913\n",
      "Loss  4.276335 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2753754 C_bot  0.15 k_c 0.0\n",
      "651 Train Loss 10.545047\n",
      "Loss  4.2753754 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2743793 C_bot  0.15 k_c 0.0\n",
      "652 Train Loss 10.54015\n",
      "Loss  4.2743793 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2735133 C_bot  0.15 k_c 0.0\n",
      "653 Train Loss 10.535394\n",
      "Loss  4.2735133 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2725806 C_bot  0.15 k_c 0.0\n",
      "654 Train Loss 10.530578\n",
      "Loss  4.2725806 C_bot  0.15 k_c 0.0\n",
      "Loss  4.271585 C_bot  0.15 k_c 0.0\n",
      "655 Train Loss 10.525705\n",
      "Loss  4.271585 C_bot  0.15 k_c 0.0\n",
      "Loss  4.270777 C_bot  0.15 k_c 0.0\n",
      "656 Train Loss 10.521032\n",
      "Loss  4.270777 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2696543 C_bot  0.15 k_c 0.0\n",
      "657 Train Loss 10.516048\n",
      "Loss  4.2696543 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2687497 C_bot  0.15 k_c 0.0\n",
      "658 Train Loss 10.511292\n",
      "Loss  4.2687497 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2676725 C_bot  0.15 k_c 0.0\n",
      "659 Train Loss 10.506371\n",
      "Loss  4.2676725 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2668242 C_bot  0.15 k_c 0.0\n",
      "660 Train Loss 10.501686\n",
      "Loss  4.2668242 C_bot  0.15 k_c 0.0\n",
      "Loss  4.266004 C_bot  0.15 k_c 0.0\n",
      "661 Train Loss 10.497038\n",
      "Loss  4.266004 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2648916 C_bot  0.15 k_c 0.0\n",
      "662 Train Loss 10.492104\n",
      "Loss  4.2648916 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2640767 C_bot  0.15 k_c 0.0\n",
      "663 Train Loss 10.4874735\n",
      "Loss  4.2640767 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2631526 C_bot  0.15 k_c 0.0\n",
      "664 Train Loss 10.482743\n",
      "Loss  4.2631526 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2621684 C_bot  0.15 k_c 0.0\n",
      "665 Train Loss 10.477959\n",
      "Loss  4.2621684 C_bot  0.15 k_c 0.0\n",
      "Loss  4.261198 C_bot  0.15 k_c 0.0\n",
      "666 Train Loss 10.473195\n",
      "Loss  4.261198 C_bot  0.15 k_c 0.0\n",
      "Loss  4.260204 C_bot  0.15 k_c 0.0\n",
      "667 Train Loss 10.468413\n",
      "Loss  4.260204 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2592726 C_bot  0.15 k_c 0.0\n",
      "668 Train Loss 10.463705\n",
      "Loss  4.2592726 C_bot  0.15 k_c 0.0\n",
      "Loss  4.258385 C_bot  0.15 k_c 0.0\n",
      "669 Train Loss 10.459045\n",
      "Loss  4.258385 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2573457 C_bot  0.15 k_c 0.0\n",
      "670 Train Loss 10.454239\n",
      "Loss  4.2573457 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2564626 C_bot  0.15 k_c 0.0\n",
      "671 Train Loss 10.449597\n",
      "Loss  4.2564626 C_bot  0.15 k_c 0.0\n",
      "Loss  4.255489 C_bot  0.15 k_c 0.0\n",
      "672 Train Loss 10.44487\n",
      "Loss  4.255489 C_bot  0.15 k_c 0.0\n",
      "Loss  4.254477 C_bot  0.15 k_c 0.0\n",
      "673 Train Loss 10.440114\n",
      "Loss  4.254477 C_bot  0.15 k_c 0.0\n",
      "Loss  4.253565 C_bot  0.15 k_c 0.0\n",
      "674 Train Loss 10.43546\n",
      "Loss  4.253565 C_bot  0.15 k_c 0.0\n",
      "Loss  4.252469 C_bot  0.15 k_c 0.0\n",
      "675 Train Loss 10.430633\n",
      "Loss  4.252469 C_bot  0.15 k_c 0.0\n",
      "Loss  4.251512 C_bot  0.15 k_c 0.0\n",
      "676 Train Loss 10.425948\n",
      "Loss  4.251512 C_bot  0.15 k_c 0.0\n",
      "Loss  4.250581 C_bot  0.15 k_c 0.0\n",
      "677 Train Loss 10.421297\n",
      "Loss  4.250581 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2497344 C_bot  0.15 k_c 0.0\n",
      "678 Train Loss 10.416735\n",
      "Loss  4.2497344 C_bot  0.15 k_c 0.0\n",
      "Loss  4.24871 C_bot  0.15 k_c 0.0\n",
      "679 Train Loss 10.4120035\n",
      "Loss  4.24871 C_bot  0.15 k_c 0.0\n",
      "Loss  4.247676 C_bot  0.15 k_c 0.0\n",
      "680 Train Loss 10.407267\n",
      "Loss  4.247676 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2466855 C_bot  0.15 k_c 0.0\n",
      "681 Train Loss 10.402577\n",
      "Loss  4.2466855 C_bot  0.15 k_c 0.0\n",
      "Loss  4.245696 C_bot  0.15 k_c 0.0\n",
      "682 Train Loss 10.397899\n",
      "Loss  4.245696 C_bot  0.15 k_c 0.0\n",
      "Loss  4.244815 C_bot  0.15 k_c 0.0\n",
      "683 Train Loss 10.39333\n",
      "Loss  4.244815 C_bot  0.15 k_c 0.0\n",
      "Loss  4.243759 C_bot  0.15 k_c 0.0\n",
      "684 Train Loss 10.388597\n",
      "Loss  4.243759 C_bot  0.15 k_c 0.0\n",
      "Loss  4.242778 C_bot  0.15 k_c 0.0\n",
      "685 Train Loss 10.383941\n",
      "Loss  4.242778 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2417393 C_bot  0.15 k_c 0.0\n",
      "686 Train Loss 10.379233\n",
      "Loss  4.2417393 C_bot  0.15 k_c 0.0\n",
      "Loss  4.240855 C_bot  0.15 k_c 0.0\n",
      "687 Train Loss 10.374688\n",
      "Loss  4.240855 C_bot  0.15 k_c 0.0\n",
      "Loss  4.239979 C_bot  0.15 k_c 0.0\n",
      "688 Train Loss 10.370153\n",
      "Loss  4.239979 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2389884 C_bot  0.15 k_c 0.0\n",
      "689 Train Loss 10.365511\n",
      "Loss  4.2389884 C_bot  0.15 k_c 0.0\n",
      "Loss  4.237927 C_bot  0.15 k_c 0.0\n",
      "690 Train Loss 10.360806\n",
      "Loss  4.237927 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2368226 C_bot  0.15 k_c 0.0\n",
      "691 Train Loss 10.356059\n",
      "Loss  4.2368226 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2358775 C_bot  0.15 k_c 0.0\n",
      "692 Train Loss 10.351481\n",
      "Loss  4.2358775 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2348485 C_bot  0.15 k_c 0.0\n",
      "693 Train Loss 10.346823\n",
      "Loss  4.2348485 C_bot  0.15 k_c 0.0\n",
      "Loss  4.233885 C_bot  0.15 k_c 0.0\n",
      "694 Train Loss 10.342234\n",
      "Loss  4.233885 C_bot  0.15 k_c 0.0\n",
      "Loss  4.232945 C_bot  0.15 k_c 0.0\n",
      "695 Train Loss 10.337676\n",
      "Loss  4.232945 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2318463 C_bot  0.15 k_c 0.0\n",
      "696 Train Loss 10.332963\n",
      "Loss  4.2318463 C_bot  0.15 k_c 0.0\n",
      "Loss  4.230861 C_bot  0.15 k_c 0.0\n",
      "697 Train Loss 10.328371\n",
      "Loss  4.230861 C_bot  0.15 k_c 0.0\n",
      "Loss  4.22993 C_bot  0.15 k_c 0.0\n",
      "698 Train Loss 10.323835\n",
      "Loss  4.22993 C_bot  0.15 k_c 0.0\n",
      "Loss  4.228903 C_bot  0.15 k_c 0.0\n",
      "699 Train Loss 10.31921\n",
      "Loss  4.228903 C_bot  0.15 k_c 0.0\n",
      "Loss  4.227961 C_bot  0.15 k_c 0.0\n",
      "700 Train Loss 10.314677\n",
      "Loss  4.227961 C_bot  0.15 k_c 0.0\n",
      "Loss  4.226772 C_bot  0.15 k_c 0.0\n",
      "701 Train Loss 10.3099\n",
      "Loss  4.226772 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2258554 C_bot  0.15 k_c 0.0\n",
      "702 Train Loss 10.305403\n",
      "Loss  4.2258554 C_bot  0.15 k_c 0.0\n",
      "Loss  4.224831 C_bot  0.15 k_c 0.0\n",
      "703 Train Loss 10.300802\n",
      "Loss  4.224831 C_bot  0.15 k_c 0.0\n",
      "Loss  4.223881 C_bot  0.15 k_c 0.0\n",
      "704 Train Loss 10.296283\n",
      "Loss  4.223881 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2229133 C_bot  0.15 k_c 0.0\n",
      "705 Train Loss 10.291749\n",
      "Loss  4.2229133 C_bot  0.15 k_c 0.0\n",
      "Loss  4.221722 C_bot  0.15 k_c 0.0\n",
      "706 Train Loss 10.286999\n",
      "Loss  4.221722 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2206974 C_bot  0.15 k_c 0.0\n",
      "707 Train Loss 10.282418\n",
      "Loss  4.2206974 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2197165 C_bot  0.15 k_c 0.0\n",
      "708 Train Loss 10.27789\n",
      "Loss  4.2197165 C_bot  0.15 k_c 0.0\n",
      "Loss  4.218699 C_bot  0.15 k_c 0.0\n",
      "709 Train Loss 10.273331\n",
      "Loss  4.218699 C_bot  0.15 k_c 0.0\n",
      "Loss  4.217632 C_bot  0.15 k_c 0.0\n",
      "710 Train Loss 10.268723\n",
      "Loss  4.217632 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2165446 C_bot  0.15 k_c 0.0\n",
      "711 Train Loss 10.264108\n",
      "Loss  4.2165446 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2155375 C_bot  0.15 k_c 0.0\n",
      "712 Train Loss 10.259569\n",
      "Loss  4.2155375 C_bot  0.15 k_c 0.0\n",
      "Loss  4.214565 C_bot  0.15 k_c 0.0\n",
      "713 Train Loss 10.255079\n",
      "Loss  4.214565 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2135077 C_bot  0.15 k_c 0.0\n",
      "714 Train Loss 10.250505\n",
      "Loss  4.2135077 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2125773 C_bot  0.15 k_c 0.0\n",
      "715 Train Loss 10.246066\n",
      "Loss  4.2125773 C_bot  0.15 k_c 0.0\n",
      "Loss  4.211464 C_bot  0.15 k_c 0.0\n",
      "716 Train Loss 10.241451\n",
      "Loss  4.211464 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2103796 C_bot  0.15 k_c 0.0\n",
      "717 Train Loss 10.236866\n",
      "Loss  4.2103796 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2093005 C_bot  0.15 k_c 0.0\n",
      "718 Train Loss 10.232297\n",
      "Loss  4.2093005 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2082334 C_bot  0.15 k_c 0.0\n",
      "719 Train Loss 10.227746\n",
      "Loss  4.2082334 C_bot  0.15 k_c 0.0\n",
      "Loss  4.207308 C_bot  0.15 k_c 0.0\n",
      "720 Train Loss 10.223338\n",
      "Loss  4.207308 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2064075 C_bot  0.15 k_c 0.0\n",
      "721 Train Loss 10.218969\n",
      "Loss  4.2064075 C_bot  0.15 k_c 0.0\n",
      "Loss  4.205114 C_bot  0.15 k_c 0.0\n",
      "722 Train Loss 10.214203\n",
      "Loss  4.205114 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2041025 C_bot  0.15 k_c 0.0\n",
      "723 Train Loss 10.209735\n",
      "Loss  4.2041025 C_bot  0.15 k_c 0.0\n",
      "Loss  4.202969 C_bot  0.15 k_c 0.0\n",
      "724 Train Loss 10.205143\n",
      "Loss  4.202969 C_bot  0.15 k_c 0.0\n",
      "Loss  4.201931 C_bot  0.15 k_c 0.0\n",
      "725 Train Loss 10.200657\n",
      "Loss  4.201931 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2008586 C_bot  0.15 k_c 0.0\n",
      "726 Train Loss 10.196145\n",
      "Loss  4.2008586 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1998224 C_bot  0.15 k_c 0.0\n",
      "727 Train Loss 10.1916685\n",
      "Loss  4.1998224 C_bot  0.15 k_c 0.0\n",
      "Loss  4.198794 C_bot  0.15 k_c 0.0\n",
      "728 Train Loss 10.187218\n",
      "Loss  4.198794 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1977506 C_bot  0.15 k_c 0.0\n",
      "729 Train Loss 10.182746\n",
      "Loss  4.1977506 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1966677 C_bot  0.15 k_c 0.0\n",
      "730 Train Loss 10.178249\n",
      "Loss  4.1966677 C_bot  0.15 k_c 0.0\n",
      "Loss  4.195657 C_bot  0.15 k_c 0.0\n",
      "731 Train Loss 10.173829\n",
      "Loss  4.195657 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1944084 C_bot  0.15 k_c 0.0\n",
      "732 Train Loss 10.169174\n",
      "Loss  4.1944084 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1935086 C_bot  0.15 k_c 0.0\n",
      "733 Train Loss 10.164884\n",
      "Loss  4.1935086 C_bot  0.15 k_c 0.0\n",
      "Loss  4.192271 C_bot  0.15 k_c 0.0\n",
      "734 Train Loss 10.16025\n",
      "Loss  4.192271 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1912704 C_bot  0.15 k_c 0.0\n",
      "735 Train Loss 10.15587\n",
      "Loss  4.1912704 C_bot  0.15 k_c 0.0\n",
      "Loss  4.190163 C_bot  0.15 k_c 0.0\n",
      "736 Train Loss 10.151384\n",
      "Loss  4.190163 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1891665 C_bot  0.15 k_c 0.0\n",
      "737 Train Loss 10.147016\n",
      "Loss  4.1891665 C_bot  0.15 k_c 0.0\n",
      "Loss  4.188045 C_bot  0.15 k_c 0.0\n",
      "738 Train Loss 10.142538\n",
      "Loss  4.188045 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1869335 C_bot  0.15 k_c 0.0\n",
      "739 Train Loss 10.1380625\n",
      "Loss  4.1869335 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1857963 C_bot  0.15 k_c 0.0\n",
      "740 Train Loss 10.133586\n",
      "Loss  4.1857963 C_bot  0.15 k_c 0.0\n",
      "Loss  4.184918 C_bot  0.15 k_c 0.0\n",
      "741 Train Loss 10.1293545\n",
      "Loss  4.184918 C_bot  0.15 k_c 0.0\n",
      "Loss  4.183608 C_bot  0.15 k_c 0.0\n",
      "742 Train Loss 10.124717\n",
      "Loss  4.183608 C_bot  0.15 k_c 0.0\n",
      "Loss  4.182446 C_bot  0.15 k_c 0.0\n",
      "743 Train Loss 10.120222\n",
      "Loss  4.182446 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1814055 C_bot  0.15 k_c 0.0\n",
      "744 Train Loss 10.11586\n",
      "Loss  4.1814055 C_bot  0.15 k_c 0.0\n",
      "Loss  4.180322 C_bot  0.15 k_c 0.0\n",
      "745 Train Loss 10.1114645\n",
      "Loss  4.180322 C_bot  0.15 k_c 0.0\n",
      "Loss  4.179223 C_bot  0.15 k_c 0.0\n",
      "746 Train Loss 10.107051\n",
      "Loss  4.179223 C_bot  0.15 k_c 0.0\n",
      "Loss  4.178145 C_bot  0.15 k_c 0.0\n",
      "747 Train Loss 10.102679\n",
      "Loss  4.178145 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1770296 C_bot  0.15 k_c 0.0\n",
      "748 Train Loss 10.098262\n",
      "Loss  4.1770296 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1759152 C_bot  0.15 k_c 0.0\n",
      "749 Train Loss 10.093867\n",
      "Loss  4.1759152 C_bot  0.15 k_c 0.0\n",
      "Loss  4.174847 C_bot  0.15 k_c 0.0\n",
      "750 Train Loss 10.089513\n",
      "Loss  4.174847 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1736317 C_bot  0.15 k_c 0.0\n",
      "751 Train Loss 10.085027\n",
      "Loss  4.1736317 C_bot  0.15 k_c 0.0\n",
      "Loss  4.172578 C_bot  0.15 k_c 0.0\n",
      "752 Train Loss 10.080707\n",
      "Loss  4.172578 C_bot  0.15 k_c 0.0\n",
      "Loss  4.171394 C_bot  0.15 k_c 0.0\n",
      "753 Train Loss 10.076257\n",
      "Loss  4.171394 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1703153 C_bot  0.15 k_c 0.0\n",
      "754 Train Loss 10.071935\n",
      "Loss  4.1703153 C_bot  0.15 k_c 0.0\n",
      "Loss  4.169291 C_bot  0.15 k_c 0.0\n",
      "755 Train Loss 10.067654\n",
      "Loss  4.169291 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1680393 C_bot  0.15 k_c 0.0\n",
      "756 Train Loss 10.063175\n",
      "Loss  4.1680393 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1669292 C_bot  0.15 k_c 0.0\n",
      "757 Train Loss 10.058821\n",
      "Loss  4.1669292 C_bot  0.15 k_c 0.0\n",
      "Loss  4.165913 C_bot  0.15 k_c 0.0\n",
      "758 Train Loss 10.054588\n",
      "Loss  4.165913 C_bot  0.15 k_c 0.0\n",
      "Loss  4.164728 C_bot  0.15 k_c 0.0\n",
      "759 Train Loss 10.050179\n",
      "Loss  4.164728 C_bot  0.15 k_c 0.0\n",
      "Loss  4.163539 C_bot  0.15 k_c 0.0\n",
      "760 Train Loss 10.045783\n",
      "Loss  4.163539 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1624956 C_bot  0.15 k_c 0.0\n",
      "761 Train Loss 10.0415325\n",
      "Loss  4.1624956 C_bot  0.15 k_c 0.0\n",
      "Loss  4.161284 C_bot  0.15 k_c 0.0\n",
      "762 Train Loss 10.037125\n",
      "Loss  4.161284 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1601915 C_bot  0.15 k_c 0.0\n",
      "763 Train Loss 10.032842\n",
      "Loss  4.1601915 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1589794 C_bot  0.15 k_c 0.0\n",
      "764 Train Loss 10.028444\n",
      "Loss  4.1589794 C_bot  0.15 k_c 0.0\n",
      "Loss  4.157864 C_bot  0.15 k_c 0.0\n",
      "765 Train Loss 10.024157\n",
      "Loss  4.157864 C_bot  0.15 k_c 0.0\n",
      "Loss  4.156636 C_bot  0.15 k_c 0.0\n",
      "766 Train Loss 10.019753\n",
      "Loss  4.156636 C_bot  0.15 k_c 0.0\n",
      "Loss  4.155619 C_bot  0.15 k_c 0.0\n",
      "767 Train Loss 10.015581\n",
      "Loss  4.155619 C_bot  0.15 k_c 0.0\n",
      "Loss  4.154437 C_bot  0.15 k_c 0.0\n",
      "768 Train Loss 10.011236\n",
      "Loss  4.154437 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1531734 C_bot  0.15 k_c 0.0\n",
      "769 Train Loss 10.006831\n",
      "Loss  4.1531734 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1521177 C_bot  0.15 k_c 0.0\n",
      "770 Train Loss 10.002628\n",
      "Loss  4.1521177 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1509085 C_bot  0.15 k_c 0.0\n",
      "771 Train Loss 9.998289\n",
      "Loss  4.1509085 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1497993 C_bot  0.15 k_c 0.0\n",
      "772 Train Loss 9.994052\n",
      "Loss  4.1497993 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1486096 C_bot  0.15 k_c 0.0\n",
      "773 Train Loss 9.989739\n",
      "Loss  4.1486096 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1474805 C_bot  0.15 k_c 0.0\n",
      "774 Train Loss 9.985504\n",
      "Loss  4.1474805 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1464024 C_bot  0.15 k_c 0.0\n",
      "775 Train Loss 9.98131\n",
      "Loss  4.1464024 C_bot  0.15 k_c 0.0\n",
      "Loss  4.14517 C_bot  0.15 k_c 0.0\n",
      "776 Train Loss 9.97699\n",
      "Loss  4.14517 C_bot  0.15 k_c 0.0\n",
      "Loss  4.143993 C_bot  0.15 k_c 0.0\n",
      "777 Train Loss 9.972711\n",
      "Loss  4.143993 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1428657 C_bot  0.15 k_c 0.0\n",
      "778 Train Loss 9.968509\n",
      "Loss  4.1428657 C_bot  0.15 k_c 0.0\n",
      "Loss  4.141594 C_bot  0.15 k_c 0.0\n",
      "779 Train Loss 9.9641485\n",
      "Loss  4.141594 C_bot  0.15 k_c 0.0\n",
      "Loss  4.140357 C_bot  0.15 k_c 0.0\n",
      "780 Train Loss 9.959853\n",
      "Loss  4.140357 C_bot  0.15 k_c 0.0\n",
      "Loss  4.139218 C_bot  0.15 k_c 0.0\n",
      "781 Train Loss 9.955638\n",
      "Loss  4.139218 C_bot  0.15 k_c 0.0\n",
      "Loss  4.138035 C_bot  0.15 k_c 0.0\n",
      "782 Train Loss 9.951415\n",
      "Loss  4.138035 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1368995 C_bot  0.15 k_c 0.0\n",
      "783 Train Loss 9.947212\n",
      "Loss  4.1368995 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1356053 C_bot  0.15 k_c 0.0\n",
      "784 Train Loss 9.942898\n",
      "Loss  4.1356053 C_bot  0.15 k_c 0.0\n",
      "Loss  4.134734 C_bot  0.15 k_c 0.0\n",
      "785 Train Loss 9.938969\n",
      "Loss  4.134734 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1333733 C_bot  0.15 k_c 0.0\n",
      "786 Train Loss 9.934607\n",
      "Loss  4.1333733 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1320877 C_bot  0.15 k_c 0.0\n",
      "787 Train Loss 9.930272\n",
      "Loss  4.1320877 C_bot  0.15 k_c 0.0\n",
      "Loss  4.130884 C_bot  0.15 k_c 0.0\n",
      "788 Train Loss 9.926087\n",
      "Loss  4.130884 C_bot  0.15 k_c 0.0\n",
      "Loss  4.129717 C_bot  0.15 k_c 0.0\n",
      "789 Train Loss 9.92188\n",
      "Loss  4.129717 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1283593 C_bot  0.15 k_c 0.0\n",
      "790 Train Loss 9.9175625\n",
      "Loss  4.1283593 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1272783 C_bot  0.15 k_c 0.0\n",
      "791 Train Loss 9.913447\n",
      "Loss  4.1272783 C_bot  0.15 k_c 0.0\n",
      "Loss  4.126218 C_bot  0.15 k_c 0.0\n",
      "792 Train Loss 9.909453\n",
      "Loss  4.126218 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1249237 C_bot  0.15 k_c 0.0\n",
      "793 Train Loss 9.905126\n",
      "Loss  4.1249237 C_bot  0.15 k_c 0.0\n",
      "Loss  4.12359 C_bot  0.15 k_c 0.0\n",
      "794 Train Loss 9.900888\n",
      "Loss  4.12359 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1227083 C_bot  0.15 k_c 0.0\n",
      "795 Train Loss 9.896968\n",
      "Loss  4.1227083 C_bot  0.15 k_c 0.0\n",
      "Loss  4.121289 C_bot  0.15 k_c 0.0\n",
      "796 Train Loss 9.892682\n",
      "Loss  4.121289 C_bot  0.15 k_c 0.0\n",
      "Loss  4.120277 C_bot  0.15 k_c 0.0\n",
      "797 Train Loss 9.88862\n",
      "Loss  4.120277 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1188006 C_bot  0.15 k_c 0.0\n",
      "798 Train Loss 9.884327\n",
      "Loss  4.1188006 C_bot  0.15 k_c 0.0\n",
      "Loss  4.117804 C_bot  0.15 k_c 0.0\n",
      "799 Train Loss 9.880248\n",
      "Loss  4.117804 C_bot  0.15 k_c 0.0\n",
      "Loss  4.116688 C_bot  0.15 k_c 0.0\n",
      "800 Train Loss 9.87639\n",
      "Loss  4.116688 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1159606 C_bot  0.15 k_c 0.0\n",
      "801 Train Loss 9.872517\n",
      "Loss  4.1159606 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1146774 C_bot  0.15 k_c 0.0\n",
      "802 Train Loss 9.868609\n",
      "Loss  4.1146774 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1142883 C_bot  0.15 k_c 0.0\n",
      "803 Train Loss 9.864954\n",
      "Loss  4.1142883 C_bot  0.15 k_c 0.0\n",
      "Loss  4.113432 C_bot  0.15 k_c 0.0\n",
      "804 Train Loss 9.861665\n",
      "Loss  4.113432 C_bot  0.15 k_c 0.0\n",
      "Loss  4.114529 C_bot  0.15 k_c 0.0\n",
      "805 Train Loss 9.859275\n",
      "Loss  4.114529 C_bot  0.15 k_c 0.0\n",
      "Loss  4.115326 C_bot  0.15 k_c 0.0\n",
      "806 Train Loss 9.857973\n",
      "Loss  4.115326 C_bot  0.15 k_c 0.0\n",
      "Loss  4.120225 C_bot  0.15 k_c 0.0\n",
      "807 Train Loss 9.858973\n",
      "Loss  4.120225 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1268797 C_bot  0.15 k_c 0.0\n",
      "808 Train Loss 9.864125\n",
      "Loss  4.1268797 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1440706 C_bot  0.15 k_c 0.0\n",
      "809 Train Loss 9.876654\n",
      "Loss  4.1440706 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1715293 C_bot  0.15 k_c 0.0\n",
      "810 Train Loss 9.903715\n",
      "Loss  4.1715293 C_bot  0.15 k_c 0.0\n",
      "Loss  4.230827 C_bot  0.15 k_c 0.0\n",
      "811 Train Loss 9.956917\n",
      "Loss  4.230827 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3350806 C_bot  0.15 k_c 0.0\n",
      "812 Train Loss 10.062882\n",
      "Loss  4.3350806 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5441113 C_bot  0.15 k_c 0.0\n",
      "813 Train Loss 10.263098\n",
      "Loss  4.5441113 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9391046 C_bot  0.15 k_c 0.0\n",
      "814 Train Loss 10.663982\n",
      "Loss  4.9391046 C_bot  0.15 k_c 0.0\n",
      "Loss  5.6874056 C_bot  0.15 k_c 0.0\n",
      "815 Train Loss 11.398302\n",
      "Loss  5.6874056 C_bot  0.15 k_c 0.0\n",
      "Loss  7.151293 C_bot  0.15 k_c 0.0\n",
      "816 Train Loss 12.876648\n",
      "Loss  7.151293 C_bot  0.15 k_c 0.0\n",
      "Loss  9.632651 C_bot  0.15 k_c 0.0\n",
      "817 Train Loss 15.334347\n",
      "Loss  9.632651 C_bot  0.15 k_c 0.0\n",
      "Loss  14.288882 C_bot  0.15 k_c 0.0\n",
      "818 Train Loss 20.022486\n",
      "Loss  14.288882 C_bot  0.15 k_c 0.0\n",
      "Loss  20.014053 C_bot  0.15 k_c 0.0\n",
      "819 Train Loss 25.706064\n",
      "Loss  20.014053 C_bot  0.15 k_c 0.0\n",
      "Loss  28.362467 C_bot  0.15 k_c 0.0\n",
      "820 Train Loss 34.115494\n",
      "Loss  28.362467 C_bot  0.15 k_c 0.0\n",
      "Loss  30.62231 C_bot  0.15 k_c 0.0\n",
      "821 Train Loss 36.30309\n",
      "Loss  30.62231 C_bot  0.15 k_c 0.0\n",
      "Loss  28.569609 C_bot  0.15 k_c 0.0\n",
      "822 Train Loss 34.33032\n",
      "Loss  28.569609 C_bot  0.15 k_c 0.0\n",
      "Loss  16.241007 C_bot  0.15 k_c 0.0\n",
      "823 Train Loss 21.917282\n",
      "Loss  16.241007 C_bot  0.15 k_c 0.0\n",
      "Loss  5.887732 C_bot  0.15 k_c 0.0\n",
      "824 Train Loss 11.6086235\n",
      "Loss  5.887732 C_bot  0.15 k_c 0.0\n",
      "Loss  5.265633 C_bot  0.15 k_c 0.0\n",
      "825 Train Loss 10.988613\n",
      "Loss  5.265633 C_bot  0.15 k_c 0.0\n",
      "Loss  11.771352 C_bot  0.15 k_c 0.0\n",
      "826 Train Loss 17.454498\n",
      "Loss  11.771352 C_bot  0.15 k_c 0.0\n",
      "Loss  14.555308 C_bot  0.15 k_c 0.0\n",
      "827 Train Loss 20.326756\n",
      "Loss  14.555308 C_bot  0.15 k_c 0.0\n",
      "Loss  8.850969 C_bot  0.15 k_c 0.0\n",
      "828 Train Loss 14.545589\n",
      "Loss  8.850969 C_bot  0.15 k_c 0.0\n",
      "Loss  4.290968 C_bot  0.15 k_c 0.0\n",
      "829 Train Loss 10.020953\n",
      "Loss  4.290968 C_bot  0.15 k_c 0.0\n",
      "Loss  6.873171 C_bot  0.15 k_c 0.0\n",
      "830 Train Loss 12.630692\n",
      "Loss  6.873171 C_bot  0.15 k_c 0.0\n",
      "Loss  9.991676 C_bot  0.15 k_c 0.0\n",
      "831 Train Loss 15.687758\n",
      "Loss  9.991676 C_bot  0.15 k_c 0.0\n",
      "Loss  7.4966855 C_bot  0.15 k_c 0.0\n",
      "832 Train Loss 13.260394\n",
      "Loss  7.4966855 C_bot  0.15 k_c 0.0\n",
      "Loss  4.324821 C_bot  0.15 k_c 0.0\n",
      "833 Train Loss 10.051597\n",
      "Loss  4.324821 C_bot  0.15 k_c 0.0\n",
      "Loss  5.9940367 C_bot  0.15 k_c 0.0\n",
      "834 Train Loss 11.702487\n",
      "Loss  5.9940367 C_bot  0.15 k_c 0.0\n",
      "Loss  7.92842 C_bot  0.15 k_c 0.0\n",
      "835 Train Loss 13.689447\n",
      "Loss  7.92842 C_bot  0.15 k_c 0.0\n",
      "Loss  5.895599 C_bot  0.15 k_c 0.0\n",
      "836 Train Loss 11.599248\n",
      "Loss  5.895599 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2611575 C_bot  0.15 k_c 0.0\n",
      "837 Train Loss 9.979165\n",
      "Loss  4.2611575 C_bot  0.15 k_c 0.0\n",
      "Loss  5.7993093 C_bot  0.15 k_c 0.0\n",
      "838 Train Loss 11.535754\n",
      "Loss  5.7993093 C_bot  0.15 k_c 0.0\n",
      "Loss  6.495295 C_bot  0.15 k_c 0.0\n",
      "839 Train Loss 12.184727\n",
      "Loss  6.495295 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8143954 C_bot  0.15 k_c 0.0\n",
      "840 Train Loss 10.532946\n",
      "Loss  4.8143954 C_bot  0.15 k_c 0.0\n",
      "Loss  4.321593 C_bot  0.15 k_c 0.0\n",
      "841 Train Loss 10.028526\n",
      "Loss  4.321593 C_bot  0.15 k_c 0.0\n",
      "Loss  5.5557637 C_bot  0.15 k_c 0.0\n",
      "842 Train Loss 11.237559\n",
      "Loss  5.5557637 C_bot  0.15 k_c 0.0\n",
      "Loss  5.51615 C_bot  0.15 k_c 0.0\n",
      "843 Train Loss 11.224911\n",
      "Loss  5.51615 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3493 C_bot  0.15 k_c 0.0\n",
      "844 Train Loss 10.031774\n",
      "Loss  4.3493 C_bot  0.15 k_c 0.0\n",
      "Loss  4.417037 C_bot  0.15 k_c 0.0\n",
      "845 Train Loss 10.094438\n",
      "Loss  4.417037 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1958594 C_bot  0.15 k_c 0.0\n",
      "846 Train Loss 10.88781\n",
      "Loss  5.1958594 C_bot  0.15 k_c 0.0\n",
      "Loss  4.876702 C_bot  0.15 k_c 0.0\n",
      "847 Train Loss 10.543451\n",
      "Loss  4.876702 C_bot  0.15 k_c 0.0\n",
      "Loss  4.160923 C_bot  0.15 k_c 0.0\n",
      "848 Train Loss 9.834885\n",
      "Loss  4.160923 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4227824 C_bot  0.15 k_c 0.0\n",
      "849 Train Loss 10.097532\n",
      "Loss  4.4227824 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8726025 C_bot  0.15 k_c 0.0\n",
      "850 Train Loss 10.531465\n",
      "Loss  4.8726025 C_bot  0.15 k_c 0.0\n",
      "Loss  4.509025 C_bot  0.15 k_c 0.0\n",
      "851 Train Loss 10.178796\n",
      "Loss  4.509025 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1164265 C_bot  0.15 k_c 0.0\n",
      "852 Train Loss 9.777483\n",
      "Loss  4.1164265 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3798795 C_bot  0.15 k_c 0.0\n",
      "853 Train Loss 10.034841\n",
      "Loss  4.3798795 C_bot  0.15 k_c 0.0\n",
      "Loss  4.618122 C_bot  0.15 k_c 0.0\n",
      "854 Train Loss 10.280479\n",
      "Loss  4.618122 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3441567 C_bot  0.15 k_c 0.0\n",
      "855 Train Loss 9.994219\n",
      "Loss  4.3441567 C_bot  0.15 k_c 0.0\n",
      "Loss  4.108571 C_bot  0.15 k_c 0.0\n",
      "856 Train Loss 9.759018\n",
      "Loss  4.108571 C_bot  0.15 k_c 0.0\n",
      "Loss  4.297756 C_bot  0.15 k_c 0.0\n",
      "857 Train Loss 9.948357\n",
      "Loss  4.297756 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4470034 C_bot  0.15 k_c 0.0\n",
      "858 Train Loss 10.08699\n",
      "Loss  4.4470034 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2484965 C_bot  0.15 k_c 0.0\n",
      "859 Train Loss 9.891726\n",
      "Loss  4.2484965 C_bot  0.15 k_c 0.0\n",
      "Loss  4.101896 C_bot  0.15 k_c 0.0\n",
      "860 Train Loss 9.738865\n",
      "Loss  4.101896 C_bot  0.15 k_c 0.0\n",
      "Loss  4.23017 C_bot  0.15 k_c 0.0\n",
      "861 Train Loss 9.861192\n",
      "Loss  4.23017 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3212113 C_bot  0.15 k_c 0.0\n",
      "862 Train Loss 9.954753\n",
      "Loss  4.3212113 C_bot  0.15 k_c 0.0\n",
      "Loss  4.200276 C_bot  0.15 k_c 0.0\n",
      "863 Train Loss 9.824693\n",
      "Loss  4.200276 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0905976 C_bot  0.15 k_c 0.0\n",
      "864 Train Loss 9.713967\n",
      "Loss  4.0905976 C_bot  0.15 k_c 0.0\n",
      "Loss  4.162499 C_bot  0.15 k_c 0.0\n",
      "865 Train Loss 9.784568\n",
      "Loss  4.162499 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2384214 C_bot  0.15 k_c 0.0\n",
      "866 Train Loss 9.852414\n",
      "Loss  4.2384214 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1609015 C_bot  0.15 k_c 0.0\n",
      "867 Train Loss 9.776584\n",
      "Loss  4.1609015 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0817733 C_bot  0.15 k_c 0.0\n",
      "868 Train Loss 9.691797\n",
      "Loss  4.0817733 C_bot  0.15 k_c 0.0\n",
      "Loss  4.118592 C_bot  0.15 k_c 0.0\n",
      "869 Train Loss 9.724308\n",
      "Loss  4.118592 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1702547 C_bot  0.15 k_c 0.0\n",
      "870 Train Loss 9.776954\n",
      "Loss  4.1702547 C_bot  0.15 k_c 0.0\n",
      "Loss  4.138501 C_bot  0.15 k_c 0.0\n",
      "871 Train Loss 9.737807\n",
      "Loss  4.138501 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0746408 C_bot  0.15 k_c 0.0\n",
      "872 Train Loss 9.673604\n",
      "Loss  4.0746408 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0839663 C_bot  0.15 k_c 0.0\n",
      "873 Train Loss 9.680651\n",
      "Loss  4.0839663 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1280293 C_bot  0.15 k_c 0.0\n",
      "874 Train Loss 9.719021\n",
      "Loss  4.1280293 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1136026 C_bot  0.15 k_c 0.0\n",
      "875 Train Loss 9.705764\n",
      "Loss  4.1136026 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0721617 C_bot  0.15 k_c 0.0\n",
      "876 Train Loss 9.659013\n",
      "Loss  4.0721617 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0647755 C_bot  0.15 k_c 0.0\n",
      "877 Train Loss 9.649264\n",
      "Loss  4.0647755 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0895143 C_bot  0.15 k_c 0.0\n",
      "878 Train Loss 9.673439\n",
      "Loss  4.0895143 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0973616 C_bot  0.15 k_c 0.0\n",
      "879 Train Loss 9.675516\n",
      "Loss  4.0973616 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0674977 C_bot  0.15 k_c 0.0\n",
      "880 Train Loss 9.645597\n",
      "Loss  4.0674977 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0523114 C_bot  0.15 k_c 0.0\n",
      "881 Train Loss 9.626797\n",
      "Loss  4.0523114 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0654855 C_bot  0.15 k_c 0.0\n",
      "882 Train Loss 9.636245\n",
      "Loss  4.0654855 C_bot  0.15 k_c 0.0\n",
      "Loss  4.074324 C_bot  0.15 k_c 0.0\n",
      "883 Train Loss 9.644698\n",
      "Loss  4.074324 C_bot  0.15 k_c 0.0\n",
      "Loss  4.064875 C_bot  0.15 k_c 0.0\n",
      "884 Train Loss 9.630124\n",
      "Loss  4.064875 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0462713 C_bot  0.15 k_c 0.0\n",
      "885 Train Loss 9.610185\n",
      "Loss  4.0462713 C_bot  0.15 k_c 0.0\n",
      "Loss  4.045933 C_bot  0.15 k_c 0.0\n",
      "886 Train Loss 9.607265\n",
      "Loss  4.045933 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0568757 C_bot  0.15 k_c 0.0\n",
      "887 Train Loss 9.614085\n",
      "Loss  4.0568757 C_bot  0.15 k_c 0.0\n",
      "Loss  4.054327 C_bot  0.15 k_c 0.0\n",
      "888 Train Loss 9.610795\n",
      "Loss  4.054327 C_bot  0.15 k_c 0.0\n",
      "Loss  4.044075 C_bot  0.15 k_c 0.0\n",
      "889 Train Loss 9.596251\n",
      "Loss  4.044075 C_bot  0.15 k_c 0.0\n",
      "Loss  4.035811 C_bot  0.15 k_c 0.0\n",
      "890 Train Loss 9.585881\n",
      "Loss  4.035811 C_bot  0.15 k_c 0.0\n",
      "Loss  4.038471 C_bot  0.15 k_c 0.0\n",
      "891 Train Loss 9.586404\n",
      "Loss  4.038471 C_bot  0.15 k_c 0.0\n",
      "Loss  4.04396 C_bot  0.15 k_c 0.0\n",
      "892 Train Loss 9.587893\n",
      "Loss  4.04396 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0385528 C_bot  0.15 k_c 0.0\n",
      "893 Train Loss 9.581286\n",
      "Loss  4.0385528 C_bot  0.15 k_c 0.0\n",
      "Loss  4.031379 C_bot  0.15 k_c 0.0\n",
      "894 Train Loss 9.57042\n",
      "Loss  4.031379 C_bot  0.15 k_c 0.0\n",
      "Loss  4.027781 C_bot  0.15 k_c 0.0\n",
      "895 Train Loss 9.564385\n",
      "Loss  4.027781 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0293345 C_bot  0.15 k_c 0.0\n",
      "896 Train Loss 9.563894\n",
      "Loss  4.0293345 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0316467 C_bot  0.15 k_c 0.0\n",
      "897 Train Loss 9.562526\n",
      "Loss  4.0316467 C_bot  0.15 k_c 0.0\n",
      "Loss  4.02673 C_bot  0.15 k_c 0.0\n",
      "898 Train Loss 9.556109\n",
      "Loss  4.02673 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0220757 C_bot  0.15 k_c 0.0\n",
      "899 Train Loss 9.548172\n",
      "Loss  4.0220757 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0196085 C_bot  0.15 k_c 0.0\n",
      "900 Train Loss 9.543255\n",
      "Loss  4.0196085 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0199294 C_bot  0.15 k_c 0.0\n",
      "901 Train Loss 9.541515\n",
      "Loss  4.0199294 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0207844 C_bot  0.15 k_c 0.0\n",
      "902 Train Loss 9.539024\n",
      "Loss  4.0207844 C_bot  0.15 k_c 0.0\n",
      "Loss  4.017089 C_bot  0.15 k_c 0.0\n",
      "903 Train Loss 9.533656\n",
      "Loss  4.017089 C_bot  0.15 k_c 0.0\n",
      "Loss  4.013793 C_bot  0.15 k_c 0.0\n",
      "904 Train Loss 9.527287\n",
      "Loss  4.013793 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0113354 C_bot  0.15 k_c 0.0\n",
      "905 Train Loss 9.522454\n",
      "Loss  4.0113354 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0106344 C_bot  0.15 k_c 0.0\n",
      "906 Train Loss 9.519581\n",
      "Loss  4.0106344 C_bot  0.15 k_c 0.0\n",
      "Loss  4.01077 C_bot  0.15 k_c 0.0\n",
      "907 Train Loss 9.516643\n",
      "Loss  4.01077 C_bot  0.15 k_c 0.0\n",
      "Loss  4.00816 C_bot  0.15 k_c 0.0\n",
      "908 Train Loss 9.512225\n",
      "Loss  4.00816 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0056286 C_bot  0.15 k_c 0.0\n",
      "909 Train Loss 9.506747\n",
      "Loss  4.0056286 C_bot  0.15 k_c 0.0\n",
      "Loss  4.00299 C_bot  0.15 k_c 0.0\n",
      "910 Train Loss 9.501861\n",
      "Loss  4.00299 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0018697 C_bot  0.15 k_c 0.0\n",
      "911 Train Loss 9.498461\n",
      "Loss  4.0018697 C_bot  0.15 k_c 0.0\n",
      "Loss  4.001325 C_bot  0.15 k_c 0.0\n",
      "912 Train Loss 9.495105\n",
      "Loss  4.001325 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9993458 C_bot  0.15 k_c 0.0\n",
      "913 Train Loss 9.491231\n",
      "Loss  3.9993458 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9975758 C_bot  0.15 k_c 0.0\n",
      "914 Train Loss 9.486616\n",
      "Loss  3.9975758 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9950583 C_bot  0.15 k_c 0.0\n",
      "915 Train Loss 9.481979\n",
      "Loss  3.9950583 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9933705 C_bot  0.15 k_c 0.0\n",
      "916 Train Loss 9.477891\n",
      "Loss  3.9933705 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9923384 C_bot  0.15 k_c 0.0\n",
      "917 Train Loss 9.474282\n",
      "Loss  3.9923384 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9907541 C_bot  0.15 k_c 0.0\n",
      "918 Train Loss 9.470684\n",
      "Loss  3.9907541 C_bot  0.15 k_c 0.0\n",
      "Loss  3.98938 C_bot  0.15 k_c 0.0\n",
      "919 Train Loss 9.466568\n",
      "Loss  3.98938 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9871633 C_bot  0.15 k_c 0.0\n",
      "920 Train Loss 9.462314\n",
      "Loss  3.9871633 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9852717 C_bot  0.15 k_c 0.0\n",
      "921 Train Loss 9.457924\n",
      "Loss  3.9852717 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9837809 C_bot  0.15 k_c 0.0\n",
      "922 Train Loss 9.454085\n",
      "Loss  3.9837809 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9822676 C_bot  0.15 k_c 0.0\n",
      "923 Train Loss 9.450429\n",
      "Loss  3.9822676 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9809966 C_bot  0.15 k_c 0.0\n",
      "924 Train Loss 9.446578\n",
      "Loss  3.9809966 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9791133 C_bot  0.15 k_c 0.0\n",
      "925 Train Loss 9.442685\n",
      "Loss  3.9791133 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9776306 C_bot  0.15 k_c 0.0\n",
      "926 Train Loss 9.438677\n",
      "Loss  3.9776306 C_bot  0.15 k_c 0.0\n",
      "Loss  3.975593 C_bot  0.15 k_c 0.0\n",
      "927 Train Loss 9.434483\n",
      "Loss  3.975593 C_bot  0.15 k_c 0.0\n",
      "Loss  3.974174 C_bot  0.15 k_c 0.0\n",
      "928 Train Loss 9.430788\n",
      "Loss  3.974174 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9726176 C_bot  0.15 k_c 0.0\n",
      "929 Train Loss 9.426851\n",
      "Loss  3.9726176 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9709985 C_bot  0.15 k_c 0.0\n",
      "930 Train Loss 9.4231615\n",
      "Loss  3.9709985 C_bot  0.15 k_c 0.0\n",
      "Loss  3.969618 C_bot  0.15 k_c 0.0\n",
      "931 Train Loss 9.419317\n",
      "Loss  3.969618 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9677718 C_bot  0.15 k_c 0.0\n",
      "932 Train Loss 9.415424\n",
      "Loss  3.9677718 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9662547 C_bot  0.15 k_c 0.0\n",
      "933 Train Loss 9.411547\n",
      "Loss  3.9662547 C_bot  0.15 k_c 0.0\n",
      "Loss  3.964472 C_bot  0.15 k_c 0.0\n",
      "934 Train Loss 9.407596\n",
      "Loss  3.964472 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9628327 C_bot  0.15 k_c 0.0\n",
      "935 Train Loss 9.403788\n",
      "Loss  3.9628327 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9615004 C_bot  0.15 k_c 0.0\n",
      "936 Train Loss 9.400151\n",
      "Loss  3.9615004 C_bot  0.15 k_c 0.0\n",
      "Loss  3.959751 C_bot  0.15 k_c 0.0\n",
      "937 Train Loss 9.396366\n",
      "Loss  3.959751 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9582157 C_bot  0.15 k_c 0.0\n",
      "938 Train Loss 9.392495\n",
      "Loss  3.9582157 C_bot  0.15 k_c 0.0\n",
      "Loss  3.956442 C_bot  0.15 k_c 0.0\n",
      "939 Train Loss 9.388697\n",
      "Loss  3.956442 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9548411 C_bot  0.15 k_c 0.0\n",
      "940 Train Loss 9.38484\n",
      "Loss  3.9548411 C_bot  0.15 k_c 0.0\n",
      "Loss  3.953332 C_bot  0.15 k_c 0.0\n",
      "941 Train Loss 9.381226\n",
      "Loss  3.953332 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9516315 C_bot  0.15 k_c 0.0\n",
      "942 Train Loss 9.377403\n",
      "Loss  3.9516315 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9500518 C_bot  0.15 k_c 0.0\n",
      "943 Train Loss 9.3736315\n",
      "Loss  3.9500518 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9483154 C_bot  0.15 k_c 0.0\n",
      "944 Train Loss 9.369876\n",
      "Loss  3.9483154 C_bot  0.15 k_c 0.0\n",
      "Loss  3.946936 C_bot  0.15 k_c 0.0\n",
      "945 Train Loss 9.366276\n",
      "Loss  3.946936 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9451537 C_bot  0.15 k_c 0.0\n",
      "946 Train Loss 9.362508\n",
      "Loss  3.9451537 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9434493 C_bot  0.15 k_c 0.0\n",
      "947 Train Loss 9.358623\n",
      "Loss  3.9434493 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9417627 C_bot  0.15 k_c 0.0\n",
      "948 Train Loss 9.354924\n",
      "Loss  3.9417627 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9403331 C_bot  0.15 k_c 0.0\n",
      "949 Train Loss 9.351394\n",
      "Loss  3.9403331 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9384797 C_bot  0.15 k_c 0.0\n",
      "950 Train Loss 9.347478\n",
      "Loss  3.9384797 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9369233 C_bot  0.15 k_c 0.0\n",
      "951 Train Loss 9.3439045\n",
      "Loss  3.9369233 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9353774 C_bot  0.15 k_c 0.0\n",
      "952 Train Loss 9.34026\n",
      "Loss  3.9353774 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9336228 C_bot  0.15 k_c 0.0\n",
      "953 Train Loss 9.336541\n",
      "Loss  3.9336228 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9321227 C_bot  0.15 k_c 0.0\n",
      "954 Train Loss 9.332938\n",
      "Loss  3.9321227 C_bot  0.15 k_c 0.0\n",
      "Loss  3.930344 C_bot  0.15 k_c 0.0\n",
      "955 Train Loss 9.329215\n",
      "Loss  3.930344 C_bot  0.15 k_c 0.0\n",
      "Loss  3.928775 C_bot  0.15 k_c 0.0\n",
      "956 Train Loss 9.325574\n",
      "Loss  3.928775 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9270604 C_bot  0.15 k_c 0.0\n",
      "957 Train Loss 9.321903\n",
      "Loss  3.9270604 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9253478 C_bot  0.15 k_c 0.0\n",
      "958 Train Loss 9.31817\n",
      "Loss  3.9253478 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9237869 C_bot  0.15 k_c 0.0\n",
      "959 Train Loss 9.31463\n",
      "Loss  3.9237869 C_bot  0.15 k_c 0.0\n",
      "Loss  3.922104 C_bot  0.15 k_c 0.0\n",
      "960 Train Loss 9.31098\n",
      "Loss  3.922104 C_bot  0.15 k_c 0.0\n",
      "Loss  3.920578 C_bot  0.15 k_c 0.0\n",
      "961 Train Loss 9.307452\n",
      "Loss  3.920578 C_bot  0.15 k_c 0.0\n",
      "Loss  3.918873 C_bot  0.15 k_c 0.0\n",
      "962 Train Loss 9.303823\n",
      "Loss  3.918873 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9172292 C_bot  0.15 k_c 0.0\n",
      "963 Train Loss 9.30017\n",
      "Loss  3.9172292 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9154394 C_bot  0.15 k_c 0.0\n",
      "964 Train Loss 9.296485\n",
      "Loss  3.9154394 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9139526 C_bot  0.15 k_c 0.0\n",
      "965 Train Loss 9.292992\n",
      "Loss  3.9139526 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9121053 C_bot  0.15 k_c 0.0\n",
      "966 Train Loss 9.289265\n",
      "Loss  3.9121053 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9105215 C_bot  0.15 k_c 0.0\n",
      "967 Train Loss 9.285692\n",
      "Loss  3.9105215 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9088547 C_bot  0.15 k_c 0.0\n",
      "968 Train Loss 9.282146\n",
      "Loss  3.9088547 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9071207 C_bot  0.15 k_c 0.0\n",
      "969 Train Loss 9.278451\n",
      "Loss  3.9071207 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9054933 C_bot  0.15 k_c 0.0\n",
      "970 Train Loss 9.274939\n",
      "Loss  3.9054933 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9038253 C_bot  0.15 k_c 0.0\n",
      "971 Train Loss 9.271339\n",
      "Loss  3.9038253 C_bot  0.15 k_c 0.0\n",
      "Loss  3.902036 C_bot  0.15 k_c 0.0\n",
      "972 Train Loss 9.267658\n",
      "Loss  3.902036 C_bot  0.15 k_c 0.0\n",
      "Loss  3.90059 C_bot  0.15 k_c 0.0\n",
      "973 Train Loss 9.264308\n",
      "Loss  3.90059 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8988245 C_bot  0.15 k_c 0.0\n",
      "974 Train Loss 9.26065\n",
      "Loss  3.8988245 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8970363 C_bot  0.15 k_c 0.0\n",
      "975 Train Loss 9.256977\n",
      "Loss  3.8970363 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8954158 C_bot  0.15 k_c 0.0\n",
      "976 Train Loss 9.253464\n",
      "Loss  3.8954158 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8937342 C_bot  0.15 k_c 0.0\n",
      "977 Train Loss 9.249917\n",
      "Loss  3.8937342 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8919983 C_bot  0.15 k_c 0.0\n",
      "978 Train Loss 9.24629\n",
      "Loss  3.8919983 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8903127 C_bot  0.15 k_c 0.0\n",
      "979 Train Loss 9.242757\n",
      "Loss  3.8903127 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8886445 C_bot  0.15 k_c 0.0\n",
      "980 Train Loss 9.239202\n",
      "Loss  3.8886445 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8869307 C_bot  0.15 k_c 0.0\n",
      "981 Train Loss 9.235653\n",
      "Loss  3.8869307 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8853168 C_bot  0.15 k_c 0.0\n",
      "982 Train Loss 9.232158\n",
      "Loss  3.8853168 C_bot  0.15 k_c 0.0\n",
      "Loss  3.883605 C_bot  0.15 k_c 0.0\n",
      "983 Train Loss 9.228624\n",
      "Loss  3.883605 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8818395 C_bot  0.15 k_c 0.0\n",
      "984 Train Loss 9.224983\n",
      "Loss  3.8818395 C_bot  0.15 k_c 0.0\n",
      "Loss  3.880045 C_bot  0.15 k_c 0.0\n",
      "985 Train Loss 9.221379\n",
      "Loss  3.880045 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8784916 C_bot  0.15 k_c 0.0\n",
      "986 Train Loss 9.217953\n",
      "Loss  3.8784916 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8766353 C_bot  0.15 k_c 0.0\n",
      "987 Train Loss 9.214303\n",
      "Loss  3.8766353 C_bot  0.15 k_c 0.0\n",
      "Loss  3.875115 C_bot  0.15 k_c 0.0\n",
      "988 Train Loss 9.210908\n",
      "Loss  3.875115 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8731658 C_bot  0.15 k_c 0.0\n",
      "989 Train Loss 9.207185\n",
      "Loss  3.8731658 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8716486 C_bot  0.15 k_c 0.0\n",
      "990 Train Loss 9.203786\n",
      "Loss  3.8716486 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8698292 C_bot  0.15 k_c 0.0\n",
      "991 Train Loss 9.20022\n",
      "Loss  3.8698292 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8682978 C_bot  0.15 k_c 0.0\n",
      "992 Train Loss 9.19679\n",
      "Loss  3.8682978 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8663945 C_bot  0.15 k_c 0.0\n",
      "993 Train Loss 9.19318\n",
      "Loss  3.8663945 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8648987 C_bot  0.15 k_c 0.0\n",
      "994 Train Loss 9.189751\n",
      "Loss  3.8648987 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8631384 C_bot  0.15 k_c 0.0\n",
      "995 Train Loss 9.186343\n",
      "Loss  3.8631384 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8616247 C_bot  0.15 k_c 0.0\n",
      "996 Train Loss 9.182836\n",
      "Loss  3.8616247 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8598104 C_bot  0.15 k_c 0.0\n",
      "997 Train Loss 9.179469\n",
      "Loss  3.8598104 C_bot  0.15 k_c 0.0\n",
      "Loss  3.858752 C_bot  0.15 k_c 0.0\n",
      "998 Train Loss 9.17631\n",
      "Loss  3.858752 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8571756 C_bot  0.15 k_c 0.0\n",
      "999 Train Loss 9.173338\n",
      "Loss  3.8571756 C_bot  0.15 k_c 0.0\n",
      "Loss  3.856873 C_bot  0.15 k_c 0.0\n",
      "1000 Train Loss 9.170742\n",
      "Loss  3.856873 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8561316 C_bot  0.15 k_c 0.0\n",
      "1001 Train Loss 9.168878\n",
      "Loss  3.8561316 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8580241 C_bot  0.15 k_c 0.0\n",
      "1002 Train Loss 9.168128\n",
      "Loss  3.8580241 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8602843 C_bot  0.15 k_c 0.0\n",
      "1003 Train Loss 9.169753\n",
      "Loss  3.8602843 C_bot  0.15 k_c 0.0\n",
      "Loss  3.869192 C_bot  0.15 k_c 0.0\n",
      "1004 Train Loss 9.1753845\n",
      "Loss  3.869192 C_bot  0.15 k_c 0.0\n",
      "Loss  3.882345 C_bot  0.15 k_c 0.0\n",
      "1005 Train Loss 9.188798\n",
      "Loss  3.882345 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9142752 C_bot  0.15 k_c 0.0\n",
      "1006 Train Loss 9.216266\n",
      "Loss  3.9142752 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9680755 C_bot  0.15 k_c 0.0\n",
      "1007 Train Loss 9.272032\n",
      "Loss  3.9680755 C_bot  0.15 k_c 0.0\n",
      "Loss  4.082747 C_bot  0.15 k_c 0.0\n",
      "1008 Train Loss 9.379996\n",
      "Loss  4.082747 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2915516 C_bot  0.15 k_c 0.0\n",
      "1009 Train Loss 9.594128\n",
      "Loss  4.2915516 C_bot  0.15 k_c 0.0\n",
      "Loss  4.714131 C_bot  0.15 k_c 0.0\n",
      "1010 Train Loss 10.005705\n",
      "Loss  4.714131 C_bot  0.15 k_c 0.0\n",
      "Loss  5.5351243 C_bot  0.15 k_c 0.0\n",
      "1011 Train Loss 10.838932\n",
      "Loss  5.5351243 C_bot  0.15 k_c 0.0\n",
      "Loss  7.1425223 C_bot  0.15 k_c 0.0\n",
      "1012 Train Loss 12.427113\n",
      "Loss  7.1425223 C_bot  0.15 k_c 0.0\n",
      "Loss  10.420827 C_bot  0.15 k_c 0.0\n",
      "1013 Train Loss 15.732758\n",
      "Loss  10.420827 C_bot  0.15 k_c 0.0\n",
      "Loss  16.481459 C_bot  0.15 k_c 0.0\n",
      "1014 Train Loss 21.75849\n",
      "Loss  16.481459 C_bot  0.15 k_c 0.0\n",
      "Loss  29.08401 C_bot  0.15 k_c 0.0\n",
      "1015 Train Loss 34.424454\n",
      "Loss  29.08401 C_bot  0.15 k_c 0.0\n",
      "Loss  48.649746 C_bot  0.15 k_c 0.0\n",
      "1016 Train Loss 53.923122\n",
      "Loss  48.649746 C_bot  0.15 k_c 0.0\n",
      "Loss  85.43041 C_bot  0.15 k_c 0.0\n",
      "1017 Train Loss 90.856064\n",
      "Loss  85.43041 C_bot  0.15 k_c 0.0\n",
      "Loss  114.61166 C_bot  0.15 k_c 0.0\n",
      "1018 Train Loss 119.884544\n",
      "Loss  114.61166 C_bot  0.15 k_c 0.0\n",
      "Loss  137.6337 C_bot  0.15 k_c 0.0\n",
      "1019 Train Loss 143.19862\n",
      "Loss  137.6337 C_bot  0.15 k_c 0.0\n",
      "Loss  91.14726 C_bot  0.15 k_c 0.0\n",
      "1020 Train Loss 96.418724\n",
      "Loss  91.14726 C_bot  0.15 k_c 0.0\n",
      "Loss  23.73156 C_bot  0.15 k_c 0.0\n",
      "1021 Train Loss 29.248415\n",
      "Loss  23.73156 C_bot  0.15 k_c 0.0\n",
      "Loss  7.1335864 C_bot  0.15 k_c 0.0\n",
      "1022 Train Loss 12.663709\n",
      "Loss  7.1335864 C_bot  0.15 k_c 0.0\n",
      "Loss  44.89523 C_bot  0.15 k_c 0.0\n",
      "1023 Train Loss 50.319378\n",
      "Loss  44.89523 C_bot  0.15 k_c 0.0\n",
      "Loss  51.849617 C_bot  0.15 k_c 0.0\n",
      "1024 Train Loss 57.724262\n",
      "Loss  51.849617 C_bot  0.15 k_c 0.0\n",
      "Loss  14.904225 C_bot  0.15 k_c 0.0\n",
      "1025 Train Loss 20.536594\n",
      "Loss  14.904225 C_bot  0.15 k_c 0.0\n",
      "Loss  10.387787 C_bot  0.15 k_c 0.0\n",
      "1026 Train Loss 16.105696\n",
      "Loss  10.387787 C_bot  0.15 k_c 0.0\n",
      "Loss  36.02609 C_bot  0.15 k_c 0.0\n",
      "1027 Train Loss 42.081146\n",
      "Loss  36.02609 C_bot  0.15 k_c 0.0\n",
      "Loss  27.467342 C_bot  0.15 k_c 0.0\n",
      "1028 Train Loss 33.20916\n",
      "Loss  27.467342 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2993054 C_bot  0.15 k_c 0.0\n",
      "1029 Train Loss 11.239434\n",
      "Loss  5.2993054 C_bot  0.15 k_c 0.0\n",
      "Loss  15.867654 C_bot  0.15 k_c 0.0\n",
      "1030 Train Loss 21.920162\n",
      "Loss  15.867654 C_bot  0.15 k_c 0.0\n",
      "Loss  26.840038 C_bot  0.15 k_c 0.0\n",
      "1031 Train Loss 32.639328\n",
      "Loss  26.840038 C_bot  0.15 k_c 0.0\n",
      "Loss  11.747907 C_bot  0.15 k_c 0.0\n",
      "1032 Train Loss 17.777882\n",
      "Loss  11.747907 C_bot  0.15 k_c 0.0\n",
      "Loss  6.068759 C_bot  0.15 k_c 0.0\n",
      "1033 Train Loss 12.042317\n",
      "Loss  6.068759 C_bot  0.15 k_c 0.0\n",
      "Loss  18.331541 C_bot  0.15 k_c 0.0\n",
      "1034 Train Loss 24.164604\n",
      "Loss  18.331541 C_bot  0.15 k_c 0.0\n",
      "Loss  16.506542 C_bot  0.15 k_c 0.0\n",
      "1035 Train Loss 22.541565\n",
      "Loss  16.506542 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4500933 C_bot  0.15 k_c 0.0\n",
      "1036 Train Loss 11.34703\n",
      "Loss  5.4500933 C_bot  0.15 k_c 0.0\n",
      "Loss  9.346507 C_bot  0.15 k_c 0.0\n",
      "1037 Train Loss 15.200347\n",
      "Loss  9.346507 C_bot  0.15 k_c 0.0\n",
      "Loss  15.532977 C_bot  0.15 k_c 0.0\n",
      "1038 Train Loss 21.514738\n",
      "Loss  15.532977 C_bot  0.15 k_c 0.0\n",
      "Loss  8.9436035 C_bot  0.15 k_c 0.0\n",
      "1039 Train Loss 14.774365\n",
      "Loss  8.9436035 C_bot  0.15 k_c 0.0\n",
      "Loss  5.03901 C_bot  0.15 k_c 0.0\n",
      "1040 Train Loss 10.883876\n",
      "Loss  5.03901 C_bot  0.15 k_c 0.0\n",
      "Loss  10.638015 C_bot  0.15 k_c 0.0\n",
      "1041 Train Loss 16.525936\n",
      "Loss  10.638015 C_bot  0.15 k_c 0.0\n",
      "Loss  10.896995 C_bot  0.15 k_c 0.0\n",
      "1042 Train Loss 16.670033\n",
      "Loss  10.896995 C_bot  0.15 k_c 0.0\n",
      "Loss  5.327542 C_bot  0.15 k_c 0.0\n",
      "1043 Train Loss 11.136506\n",
      "Loss  5.327542 C_bot  0.15 k_c 0.0\n",
      "Loss  6.1486554 C_bot  0.15 k_c 0.0\n",
      "1044 Train Loss 11.944979\n",
      "Loss  6.1486554 C_bot  0.15 k_c 0.0\n",
      "Loss  9.718475 C_bot  0.15 k_c 0.0\n",
      "1045 Train Loss 15.452121\n",
      "Loss  9.718475 C_bot  0.15 k_c 0.0\n",
      "Loss  7.317693 C_bot  0.15 k_c 0.0\n",
      "1046 Train Loss 13.0872755\n",
      "Loss  7.317693 C_bot  0.15 k_c 0.0\n",
      "Loss  4.630431 C_bot  0.15 k_c 0.0\n",
      "1047 Train Loss 10.366326\n",
      "Loss  4.630431 C_bot  0.15 k_c 0.0\n",
      "Loss  6.87955 C_bot  0.15 k_c 0.0\n",
      "1048 Train Loss 12.590678\n",
      "Loss  6.87955 C_bot  0.15 k_c 0.0\n",
      "Loss  7.9136424 C_bot  0.15 k_c 0.0\n",
      "1049 Train Loss 13.649279\n",
      "Loss  7.9136424 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3646984 C_bot  0.15 k_c 0.0\n",
      "1050 Train Loss 11.063598\n",
      "Loss  5.3646984 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8617496 C_bot  0.15 k_c 0.0\n",
      "1051 Train Loss 10.554328\n",
      "Loss  4.8617496 C_bot  0.15 k_c 0.0\n",
      "Loss  6.738379 C_bot  0.15 k_c 0.0\n",
      "1052 Train Loss 12.438823\n",
      "Loss  6.738379 C_bot  0.15 k_c 0.0\n",
      "Loss  6.348051 C_bot  0.15 k_c 0.0\n",
      "1053 Train Loss 12.015739\n",
      "Loss  6.348051 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6358824 C_bot  0.15 k_c 0.0\n",
      "1054 Train Loss 10.304048\n",
      "Loss  4.6358824 C_bot  0.15 k_c 0.0\n",
      "Loss  5.131624 C_bot  0.15 k_c 0.0\n",
      "1055 Train Loss 10.791758\n",
      "Loss  5.131624 C_bot  0.15 k_c 0.0\n",
      "Loss  6.162578 C_bot  0.15 k_c 0.0\n",
      "1056 Train Loss 11.797292\n",
      "Loss  6.162578 C_bot  0.15 k_c 0.0\n",
      "Loss  5.292063 C_bot  0.15 k_c 0.0\n",
      "1057 Train Loss 10.9295845\n",
      "Loss  5.292063 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4854856 C_bot  0.15 k_c 0.0\n",
      "1058 Train Loss 10.105225\n",
      "Loss  4.4854856 C_bot  0.15 k_c 0.0\n",
      "Loss  5.209383 C_bot  0.15 k_c 0.0\n",
      "1059 Train Loss 10.81269\n",
      "Loss  5.209383 C_bot  0.15 k_c 0.0\n",
      "Loss  5.54548 C_bot  0.15 k_c 0.0\n",
      "1060 Train Loss 11.1519575\n",
      "Loss  5.54548 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7491355 C_bot  0.15 k_c 0.0\n",
      "1061 Train Loss 10.33397\n",
      "Loss  4.7491355 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4893227 C_bot  0.15 k_c 0.0\n",
      "1062 Train Loss 10.066816\n",
      "Loss  4.4893227 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0568943 C_bot  0.15 k_c 0.0\n",
      "1063 Train Loss 10.633833\n",
      "Loss  5.0568943 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0542903 C_bot  0.15 k_c 0.0\n",
      "1064 Train Loss 10.609541\n",
      "Loss  5.0542903 C_bot  0.15 k_c 0.0\n",
      "Loss  4.474645 C_bot  0.15 k_c 0.0\n",
      "1065 Train Loss 10.029503\n",
      "Loss  4.474645 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4822655 C_bot  0.15 k_c 0.0\n",
      "1066 Train Loss 10.028433\n",
      "Loss  4.4822655 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8591604 C_bot  0.15 k_c 0.0\n",
      "1067 Train Loss 10.387363\n",
      "Loss  4.8591604 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6963086 C_bot  0.15 k_c 0.0\n",
      "1068 Train Loss 10.226931\n",
      "Loss  4.6963086 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3398423 C_bot  0.15 k_c 0.0\n",
      "1069 Train Loss 9.854479\n",
      "Loss  4.3398423 C_bot  0.15 k_c 0.0\n",
      "Loss  4.431434 C_bot  0.15 k_c 0.0\n",
      "1070 Train Loss 9.93554\n",
      "Loss  4.431434 C_bot  0.15 k_c 0.0\n",
      "Loss  4.633529 C_bot  0.15 k_c 0.0\n",
      "1071 Train Loss 10.138563\n",
      "Loss  4.633529 C_bot  0.15 k_c 0.0\n",
      "Loss  4.479824 C_bot  0.15 k_c 0.0\n",
      "1072 Train Loss 9.966984\n",
      "Loss  4.479824 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2502103 C_bot  0.15 k_c 0.0\n",
      "1073 Train Loss 9.734924\n",
      "Loss  4.2502103 C_bot  0.15 k_c 0.0\n",
      "Loss  4.333459 C_bot  0.15 k_c 0.0\n",
      "1074 Train Loss 9.813905\n",
      "Loss  4.333459 C_bot  0.15 k_c 0.0\n",
      "Loss  4.456527 C_bot  0.15 k_c 0.0\n",
      "1075 Train Loss 9.922058\n",
      "Loss  4.456527 C_bot  0.15 k_c 0.0\n",
      "Loss  4.313584 C_bot  0.15 k_c 0.0\n",
      "1076 Train Loss 9.780628\n",
      "Loss  4.313584 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1760364 C_bot  0.15 k_c 0.0\n",
      "1077 Train Loss 9.632172\n",
      "Loss  4.1760364 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2408066 C_bot  0.15 k_c 0.0\n",
      "1078 Train Loss 9.687264\n",
      "Loss  4.2408066 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3005695 C_bot  0.15 k_c 0.0\n",
      "1079 Train Loss 9.746568\n",
      "Loss  4.3005695 C_bot  0.15 k_c 0.0\n",
      "Loss  4.212832 C_bot  0.15 k_c 0.0\n",
      "1080 Train Loss 9.644278\n",
      "Loss  4.212832 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1109066 C_bot  0.15 k_c 0.0\n",
      "1081 Train Loss 9.537898\n",
      "Loss  4.1109066 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1439924 C_bot  0.15 k_c 0.0\n",
      "1082 Train Loss 9.565435\n",
      "Loss  4.1439924 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1955953 C_bot  0.15 k_c 0.0\n",
      "1083 Train Loss 9.60433\n",
      "Loss  4.1955953 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1306252 C_bot  0.15 k_c 0.0\n",
      "1084 Train Loss 9.537863\n",
      "Loss  4.1306252 C_bot  0.15 k_c 0.0\n",
      "Loss  4.060735 C_bot  0.15 k_c 0.0\n",
      "1085 Train Loss 9.458136\n",
      "Loss  4.060735 C_bot  0.15 k_c 0.0\n",
      "Loss  4.073526 C_bot  0.15 k_c 0.0\n",
      "1086 Train Loss 9.4630995\n",
      "Loss  4.073526 C_bot  0.15 k_c 0.0\n",
      "Loss  4.10237 C_bot  0.15 k_c 0.0\n",
      "1087 Train Loss 9.489358\n",
      "Loss  4.10237 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0766926 C_bot  0.15 k_c 0.0\n",
      "1088 Train Loss 9.452455\n",
      "Loss  4.0766926 C_bot  0.15 k_c 0.0\n",
      "Loss  4.019596 C_bot  0.15 k_c 0.0\n",
      "1089 Train Loss 9.391512\n",
      "Loss  4.019596 C_bot  0.15 k_c 0.0\n",
      "Loss  4.014758 C_bot  0.15 k_c 0.0\n",
      "1090 Train Loss 9.380346\n",
      "Loss  4.014758 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0411963 C_bot  0.15 k_c 0.0\n",
      "1091 Train Loss 9.397544\n",
      "Loss  4.0411963 C_bot  0.15 k_c 0.0\n",
      "Loss  4.027025 C_bot  0.15 k_c 0.0\n",
      "1092 Train Loss 9.380749\n",
      "Loss  4.027025 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9915314 C_bot  0.15 k_c 0.0\n",
      "1093 Train Loss 9.336457\n",
      "Loss  3.9915314 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9752645 C_bot  0.15 k_c 0.0\n",
      "1094 Train Loss 9.314957\n",
      "Loss  3.9752645 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9859385 C_bot  0.15 k_c 0.0\n",
      "1095 Train Loss 9.32189\n",
      "Loss  3.9859385 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9899983 C_bot  0.15 k_c 0.0\n",
      "1096 Train Loss 9.318036\n",
      "Loss  3.9899983 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9633622 C_bot  0.15 k_c 0.0\n",
      "1097 Train Loss 9.2892\n",
      "Loss  3.9633622 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9437032 C_bot  0.15 k_c 0.0\n",
      "1098 Train Loss 9.263914\n",
      "Loss  3.9437032 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9448268 C_bot  0.15 k_c 0.0\n",
      "1099 Train Loss 9.260126\n",
      "Loss  3.9448268 C_bot  0.15 k_c 0.0\n",
      "Loss  3.947113 C_bot  0.15 k_c 0.0\n",
      "1100 Train Loss 9.260361\n",
      "Loss  3.947113 C_bot  0.15 k_c 0.0\n",
      "Loss  3.938043 C_bot  0.15 k_c 0.0\n",
      "1101 Train Loss 9.245131\n",
      "Loss  3.938043 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9174483 C_bot  0.15 k_c 0.0\n",
      "1102 Train Loss 9.222325\n",
      "Loss  3.9174483 C_bot  0.15 k_c 0.0\n",
      "Loss  3.909098 C_bot  0.15 k_c 0.0\n",
      "1103 Train Loss 9.210217\n",
      "Loss  3.909098 C_bot  0.15 k_c 0.0\n",
      "Loss  3.911535 C_bot  0.15 k_c 0.0\n",
      "1104 Train Loss 9.208096\n",
      "Loss  3.911535 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9063048 C_bot  0.15 k_c 0.0\n",
      "1105 Train Loss 9.201314\n",
      "Loss  3.9063048 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8950222 C_bot  0.15 k_c 0.0\n",
      "1106 Train Loss 9.185177\n",
      "Loss  3.8950222 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8822434 C_bot  0.15 k_c 0.0\n",
      "1107 Train Loss 9.170029\n",
      "Loss  3.8822434 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8778193 C_bot  0.15 k_c 0.0\n",
      "1108 Train Loss 9.162806\n",
      "Loss  3.8778193 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8777165 C_bot  0.15 k_c 0.0\n",
      "1109 Train Loss 9.158523\n",
      "Loss  3.8777165 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8700202 C_bot  0.15 k_c 0.0\n",
      "1110 Train Loss 9.149204\n",
      "Loss  3.8700202 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8607438 C_bot  0.15 k_c 0.0\n",
      "1111 Train Loss 9.135808\n",
      "Loss  3.8607438 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8528879 C_bot  0.15 k_c 0.0\n",
      "1112 Train Loss 9.125272\n",
      "Loss  3.8528879 C_bot  0.15 k_c 0.0\n",
      "Loss  3.849351 C_bot  0.15 k_c 0.0\n",
      "1113 Train Loss 9.119141\n",
      "Loss  3.849351 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8475592 C_bot  0.15 k_c 0.0\n",
      "1114 Train Loss 9.113319\n",
      "Loss  3.8475592 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8403194 C_bot  0.15 k_c 0.0\n",
      "1115 Train Loss 9.104095\n",
      "Loss  3.8403194 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8334024 C_bot  0.15 k_c 0.0\n",
      "1116 Train Loss 9.093374\n",
      "Loss  3.8334024 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8278503 C_bot  0.15 k_c 0.0\n",
      "1117 Train Loss 9.084912\n",
      "Loss  3.8278503 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8245966 C_bot  0.15 k_c 0.0\n",
      "1118 Train Loss 9.079027\n",
      "Loss  3.8245966 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8221495 C_bot  0.15 k_c 0.0\n",
      "1119 Train Loss 9.072741\n",
      "Loss  3.8221495 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8161001 C_bot  0.15 k_c 0.0\n",
      "1120 Train Loss 9.06445\n",
      "Loss  3.8161001 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8108046 C_bot  0.15 k_c 0.0\n",
      "1121 Train Loss 9.055578\n",
      "Loss  3.8108046 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8062494 C_bot  0.15 k_c 0.0\n",
      "1122 Train Loss 9.048084\n",
      "Loss  3.8062494 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8030405 C_bot  0.15 k_c 0.0\n",
      "1123 Train Loss 9.042197\n",
      "Loss  3.8030405 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8004766 C_bot  0.15 k_c 0.0\n",
      "1124 Train Loss 9.03604\n",
      "Loss  3.8004766 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7956843 C_bot  0.15 k_c 0.0\n",
      "1125 Train Loss 9.028889\n",
      "Loss  3.7956843 C_bot  0.15 k_c 0.0\n",
      "Loss  3.791411 C_bot  0.15 k_c 0.0\n",
      "1126 Train Loss 9.02125\n",
      "Loss  3.791411 C_bot  0.15 k_c 0.0\n",
      "Loss  3.787568 C_bot  0.15 k_c 0.0\n",
      "1127 Train Loss 9.0145855\n",
      "Loss  3.787568 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7845051 C_bot  0.15 k_c 0.0\n",
      "1128 Train Loss 9.008875\n",
      "Loss  3.7845051 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7819717 C_bot  0.15 k_c 0.0\n",
      "1129 Train Loss 9.003065\n",
      "Loss  3.7819717 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7779908 C_bot  0.15 k_c 0.0\n",
      "1130 Train Loss 8.996768\n",
      "Loss  3.7779908 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7746618 C_bot  0.15 k_c 0.0\n",
      "1131 Train Loss 8.990326\n",
      "Loss  3.7746618 C_bot  0.15 k_c 0.0\n",
      "Loss  3.770922 C_bot  0.15 k_c 0.0\n",
      "1132 Train Loss 8.983999\n",
      "Loss  3.770922 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7679856 C_bot  0.15 k_c 0.0\n",
      "1133 Train Loss 8.978501\n",
      "Loss  3.7679856 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7655594 C_bot  0.15 k_c 0.0\n",
      "1134 Train Loss 8.973131\n",
      "Loss  3.7655594 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7622035 C_bot  0.15 k_c 0.0\n",
      "1135 Train Loss 8.967533\n",
      "Loss  3.7622035 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7592936 C_bot  0.15 k_c 0.0\n",
      "1136 Train Loss 8.961728\n",
      "Loss  3.7592936 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7559896 C_bot  0.15 k_c 0.0\n",
      "1137 Train Loss 8.956042\n",
      "Loss  3.7559896 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7532656 C_bot  0.15 k_c 0.0\n",
      "1138 Train Loss 8.950813\n",
      "Loss  3.7532656 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7507815 C_bot  0.15 k_c 0.0\n",
      "1139 Train Loss 8.945666\n",
      "Loss  3.7507815 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7478516 C_bot  0.15 k_c 0.0\n",
      "1140 Train Loss 8.940541\n",
      "Loss  3.7478516 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7454064 C_bot  0.15 k_c 0.0\n",
      "1141 Train Loss 8.935389\n",
      "Loss  3.7454064 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7422426 C_bot  0.15 k_c 0.0\n",
      "1142 Train Loss 8.930014\n",
      "Loss  3.7422426 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7396002 C_bot  0.15 k_c 0.0\n",
      "1143 Train Loss 8.924907\n",
      "Loss  3.7396002 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7370083 C_bot  0.15 k_c 0.0\n",
      "1144 Train Loss 8.919892\n",
      "Loss  3.7370083 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7345114 C_bot  0.15 k_c 0.0\n",
      "1145 Train Loss 8.9152\n",
      "Loss  3.7345114 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7321584 C_bot  0.15 k_c 0.0\n",
      "1146 Train Loss 8.910306\n",
      "Loss  3.7321584 C_bot  0.15 k_c 0.0\n",
      "Loss  3.729277 C_bot  0.15 k_c 0.0\n",
      "1147 Train Loss 8.905298\n",
      "Loss  3.729277 C_bot  0.15 k_c 0.0\n",
      "Loss  3.726983 C_bot  0.15 k_c 0.0\n",
      "1148 Train Loss 8.900564\n",
      "Loss  3.726983 C_bot  0.15 k_c 0.0\n",
      "Loss  3.724398 C_bot  0.15 k_c 0.0\n",
      "1149 Train Loss 8.895735\n",
      "Loss  3.724398 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7218726 C_bot  0.15 k_c 0.0\n",
      "1150 Train Loss 8.890977\n",
      "Loss  3.7218726 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7197173 C_bot  0.15 k_c 0.0\n",
      "1151 Train Loss 8.886446\n",
      "Loss  3.7197173 C_bot  0.15 k_c 0.0\n",
      "Loss  3.717063 C_bot  0.15 k_c 0.0\n",
      "1152 Train Loss 8.88169\n",
      "Loss  3.717063 C_bot  0.15 k_c 0.0\n",
      "Loss  3.714723 C_bot  0.15 k_c 0.0\n",
      "1153 Train Loss 8.876973\n",
      "Loss  3.714723 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7123773 C_bot  0.15 k_c 0.0\n",
      "1154 Train Loss 8.872505\n",
      "Loss  3.7123773 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7099369 C_bot  0.15 k_c 0.0\n",
      "1155 Train Loss 8.867809\n",
      "Loss  3.7099369 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7077224 C_bot  0.15 k_c 0.0\n",
      "1156 Train Loss 8.863368\n",
      "Loss  3.7077224 C_bot  0.15 k_c 0.0\n",
      "Loss  3.705453 C_bot  0.15 k_c 0.0\n",
      "1157 Train Loss 8.858979\n",
      "Loss  3.705453 C_bot  0.15 k_c 0.0\n",
      "Loss  3.703175 C_bot  0.15 k_c 0.0\n",
      "1158 Train Loss 8.854415\n",
      "Loss  3.703175 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7009032 C_bot  0.15 k_c 0.0\n",
      "1159 Train Loss 8.850074\n",
      "Loss  3.7009032 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6987135 C_bot  0.15 k_c 0.0\n",
      "1160 Train Loss 8.845635\n",
      "Loss  3.6987135 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6964788 C_bot  0.15 k_c 0.0\n",
      "1161 Train Loss 8.841296\n",
      "Loss  3.6964788 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6941824 C_bot  0.15 k_c 0.0\n",
      "1162 Train Loss 8.83685\n",
      "Loss  3.6941824 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6919494 C_bot  0.15 k_c 0.0\n",
      "1163 Train Loss 8.832451\n",
      "Loss  3.6919494 C_bot  0.15 k_c 0.0\n",
      "Loss  3.689623 C_bot  0.15 k_c 0.0\n",
      "1164 Train Loss 8.82807\n",
      "Loss  3.689623 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6876085 C_bot  0.15 k_c 0.0\n",
      "1165 Train Loss 8.823862\n",
      "Loss  3.6876085 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6853333 C_bot  0.15 k_c 0.0\n",
      "1166 Train Loss 8.819565\n",
      "Loss  3.6853333 C_bot  0.15 k_c 0.0\n",
      "Loss  3.683105 C_bot  0.15 k_c 0.0\n",
      "1167 Train Loss 8.815184\n",
      "Loss  3.683105 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6809916 C_bot  0.15 k_c 0.0\n",
      "1168 Train Loss 8.81102\n",
      "Loss  3.6809916 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6786942 C_bot  0.15 k_c 0.0\n",
      "1169 Train Loss 8.806651\n",
      "Loss  3.6786942 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6767058 C_bot  0.15 k_c 0.0\n",
      "1170 Train Loss 8.802571\n",
      "Loss  3.6767058 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6745093 C_bot  0.15 k_c 0.0\n",
      "1171 Train Loss 8.798371\n",
      "Loss  3.6745093 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6724212 C_bot  0.15 k_c 0.0\n",
      "1172 Train Loss 8.794185\n",
      "Loss  3.6724212 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6702342 C_bot  0.15 k_c 0.0\n",
      "1173 Train Loss 8.790023\n",
      "Loss  3.6702342 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6681023 C_bot  0.15 k_c 0.0\n",
      "1174 Train Loss 8.785824\n",
      "Loss  3.6681023 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6659846 C_bot  0.15 k_c 0.0\n",
      "1175 Train Loss 8.781722\n",
      "Loss  3.6659846 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6638749 C_bot  0.15 k_c 0.0\n",
      "1176 Train Loss 8.777602\n",
      "Loss  3.6638749 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6617827 C_bot  0.15 k_c 0.0\n",
      "1177 Train Loss 8.773505\n",
      "Loss  3.6617827 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6595705 C_bot  0.15 k_c 0.0\n",
      "1178 Train Loss 8.769335\n",
      "Loss  3.6595705 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6574047 C_bot  0.15 k_c 0.0\n",
      "1179 Train Loss 8.765155\n",
      "Loss  3.6574047 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6552455 C_bot  0.15 k_c 0.0\n",
      "1180 Train Loss 8.761071\n",
      "Loss  3.6552455 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6531668 C_bot  0.15 k_c 0.0\n",
      "1181 Train Loss 8.7569895\n",
      "Loss  3.6531668 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6510742 C_bot  0.15 k_c 0.0\n",
      "1182 Train Loss 8.752981\n",
      "Loss  3.6510742 C_bot  0.15 k_c 0.0\n",
      "Loss  3.649059 C_bot  0.15 k_c 0.0\n",
      "1183 Train Loss 8.748997\n",
      "Loss  3.649059 C_bot  0.15 k_c 0.0\n",
      "Loss  3.646943 C_bot  0.15 k_c 0.0\n",
      "1184 Train Loss 8.744956\n",
      "Loss  3.646943 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6446736 C_bot  0.15 k_c 0.0\n",
      "1185 Train Loss 8.740761\n",
      "Loss  3.6446736 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6427393 C_bot  0.15 k_c 0.0\n",
      "1186 Train Loss 8.736887\n",
      "Loss  3.6427393 C_bot  0.15 k_c 0.0\n",
      "Loss  3.640532 C_bot  0.15 k_c 0.0\n",
      "1187 Train Loss 8.732793\n",
      "Loss  3.640532 C_bot  0.15 k_c 0.0\n",
      "Loss  3.638535 C_bot  0.15 k_c 0.0\n",
      "1188 Train Loss 8.728849\n",
      "Loss  3.638535 C_bot  0.15 k_c 0.0\n",
      "Loss  3.636364 C_bot  0.15 k_c 0.0\n",
      "1189 Train Loss 8.724818\n",
      "Loss  3.636364 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6343358 C_bot  0.15 k_c 0.0\n",
      "1190 Train Loss 8.72085\n",
      "Loss  3.6343358 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6322157 C_bot  0.15 k_c 0.0\n",
      "1191 Train Loss 8.716881\n",
      "Loss  3.6322157 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6302001 C_bot  0.15 k_c 0.0\n",
      "1192 Train Loss 8.712945\n",
      "Loss  3.6302001 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6280131 C_bot  0.15 k_c 0.0\n",
      "1193 Train Loss 8.708907\n",
      "Loss  3.6280131 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6259458 C_bot  0.15 k_c 0.0\n",
      "1194 Train Loss 8.704949\n",
      "Loss  3.6259458 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6238234 C_bot  0.15 k_c 0.0\n",
      "1195 Train Loss 8.700968\n",
      "Loss  3.6238234 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6217988 C_bot  0.15 k_c 0.0\n",
      "1196 Train Loss 8.697084\n",
      "Loss  3.6217988 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6196554 C_bot  0.15 k_c 0.0\n",
      "1197 Train Loss 8.693074\n",
      "Loss  3.6196554 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6175752 C_bot  0.15 k_c 0.0\n",
      "1198 Train Loss 8.689164\n",
      "Loss  3.6175752 C_bot  0.15 k_c 0.0\n",
      "Loss  3.615606 C_bot  0.15 k_c 0.0\n",
      "1199 Train Loss 8.685323\n",
      "Loss  3.615606 C_bot  0.15 k_c 0.0\n",
      "Loss  3.613436 C_bot  0.15 k_c 0.0\n",
      "1200 Train Loss 8.681347\n",
      "Loss  3.613436 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6114593 C_bot  0.15 k_c 0.0\n",
      "1201 Train Loss 8.677502\n",
      "Loss  3.6114593 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6093953 C_bot  0.15 k_c 0.0\n",
      "1202 Train Loss 8.673643\n",
      "Loss  3.6093953 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6073472 C_bot  0.15 k_c 0.0\n",
      "1203 Train Loss 8.669741\n",
      "Loss  3.6073472 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6051435 C_bot  0.15 k_c 0.0\n",
      "1204 Train Loss 8.665748\n",
      "Loss  3.6051435 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6030972 C_bot  0.15 k_c 0.0\n",
      "1205 Train Loss 8.661865\n",
      "Loss  3.6030972 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6011117 C_bot  0.15 k_c 0.0\n",
      "1206 Train Loss 8.658091\n",
      "Loss  3.6011117 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5990372 C_bot  0.15 k_c 0.0\n",
      "1207 Train Loss 8.654202\n",
      "Loss  3.5990372 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5968678 C_bot  0.15 k_c 0.0\n",
      "1208 Train Loss 8.65024\n",
      "Loss  3.5968678 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5947971 C_bot  0.15 k_c 0.0\n",
      "1209 Train Loss 8.6463785\n",
      "Loss  3.5947971 C_bot  0.15 k_c 0.0\n",
      "Loss  3.592743 C_bot  0.15 k_c 0.0\n",
      "1210 Train Loss 8.642532\n",
      "Loss  3.592743 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5907698 C_bot  0.15 k_c 0.0\n",
      "1211 Train Loss 8.638785\n",
      "Loss  3.5907698 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5887039 C_bot  0.15 k_c 0.0\n",
      "1212 Train Loss 8.63493\n",
      "Loss  3.5887039 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5865846 C_bot  0.15 k_c 0.0\n",
      "1213 Train Loss 8.631052\n",
      "Loss  3.5865846 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5844483 C_bot  0.15 k_c 0.0\n",
      "1214 Train Loss 8.62713\n",
      "Loss  3.5844483 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5823233 C_bot  0.15 k_c 0.0\n",
      "1215 Train Loss 8.623261\n",
      "Loss  3.5823233 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5804963 C_bot  0.15 k_c 0.0\n",
      "1216 Train Loss 8.619652\n",
      "Loss  3.5804963 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5782185 C_bot  0.15 k_c 0.0\n",
      "1217 Train Loss 8.615645\n",
      "Loss  3.5782185 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5761278 C_bot  0.15 k_c 0.0\n",
      "1218 Train Loss 8.611774\n",
      "Loss  3.5761278 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5741107 C_bot  0.15 k_c 0.0\n",
      "1219 Train Loss 8.608046\n",
      "Loss  3.5741107 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5721288 C_bot  0.15 k_c 0.0\n",
      "1220 Train Loss 8.604281\n",
      "Loss  3.5721288 C_bot  0.15 k_c 0.0\n",
      "Loss  3.569933 C_bot  0.15 k_c 0.0\n",
      "1221 Train Loss 8.600393\n",
      "Loss  3.569933 C_bot  0.15 k_c 0.0\n",
      "Loss  3.568002 C_bot  0.15 k_c 0.0\n",
      "1222 Train Loss 8.596676\n",
      "Loss  3.568002 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5658977 C_bot  0.15 k_c 0.0\n",
      "1223 Train Loss 8.592902\n",
      "Loss  3.5658977 C_bot  0.15 k_c 0.0\n",
      "Loss  3.563951 C_bot  0.15 k_c 0.0\n",
      "1224 Train Loss 8.589156\n",
      "Loss  3.563951 C_bot  0.15 k_c 0.0\n",
      "Loss  3.561706 C_bot  0.15 k_c 0.0\n",
      "1225 Train Loss 8.585278\n",
      "Loss  3.561706 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5598285 C_bot  0.15 k_c 0.0\n",
      "1226 Train Loss 8.581575\n",
      "Loss  3.5598285 C_bot  0.15 k_c 0.0\n",
      "Loss  3.55767 C_bot  0.15 k_c 0.0\n",
      "1227 Train Loss 8.577831\n",
      "Loss  3.55767 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5557005 C_bot  0.15 k_c 0.0\n",
      "1228 Train Loss 8.573995\n",
      "Loss  3.5557005 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5535035 C_bot  0.15 k_c 0.0\n",
      "1229 Train Loss 8.570283\n",
      "Loss  3.5535035 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5519037 C_bot  0.15 k_c 0.0\n",
      "1230 Train Loss 8.566742\n",
      "Loss  3.5519037 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5497503 C_bot  0.15 k_c 0.0\n",
      "1231 Train Loss 8.563186\n",
      "Loss  3.5497503 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5483863 C_bot  0.15 k_c 0.0\n",
      "1232 Train Loss 8.559757\n",
      "Loss  3.5483863 C_bot  0.15 k_c 0.0\n",
      "Loss  3.546416 C_bot  0.15 k_c 0.0\n",
      "1233 Train Loss 8.556561\n",
      "Loss  3.546416 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5457637 C_bot  0.15 k_c 0.0\n",
      "1234 Train Loss 8.55363\n",
      "Loss  3.5457637 C_bot  0.15 k_c 0.0\n",
      "Loss  3.54467 C_bot  0.15 k_c 0.0\n",
      "1235 Train Loss 8.55161\n",
      "Loss  3.54467 C_bot  0.15 k_c 0.0\n",
      "Loss  3.546307 C_bot  0.15 k_c 0.0\n",
      "1236 Train Loss 8.550592\n",
      "Loss  3.546307 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5478005 C_bot  0.15 k_c 0.0\n",
      "1237 Train Loss 8.551683\n",
      "Loss  3.5478005 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5561087 C_bot  0.15 k_c 0.0\n",
      "1238 Train Loss 8.556663\n",
      "Loss  3.5561087 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5671217 C_bot  0.15 k_c 0.0\n",
      "1239 Train Loss 8.568211\n",
      "Loss  3.5671217 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5951803 C_bot  0.15 k_c 0.0\n",
      "1240 Train Loss 8.591721\n",
      "Loss  3.5951803 C_bot  0.15 k_c 0.0\n",
      "Loss  3.639158 C_bot  0.15 k_c 0.0\n",
      "1241 Train Loss 8.637953\n",
      "Loss  3.639158 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7337608 C_bot  0.15 k_c 0.0\n",
      "1242 Train Loss 8.725774\n",
      "Loss  3.7337608 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8956583 C_bot  0.15 k_c 0.0\n",
      "1243 Train Loss 8.893177\n",
      "Loss  3.8956583 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2220397 C_bot  0.15 k_c 0.0\n",
      "1244 Train Loss 9.208635\n",
      "Loss  4.2220397 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8191266 C_bot  0.15 k_c 0.0\n",
      "1245 Train Loss 9.817603\n",
      "Loss  4.8191266 C_bot  0.15 k_c 0.0\n",
      "Loss  5.9916573 C_bot  0.15 k_c 0.0\n",
      "1246 Train Loss 10.971521\n",
      "Loss  5.9916573 C_bot  0.15 k_c 0.0\n",
      "Loss  8.24622 C_bot  0.15 k_c 0.0\n",
      "1247 Train Loss 13.251097\n",
      "Loss  8.24622 C_bot  0.15 k_c 0.0\n",
      "Loss  12.59848 C_bot  0.15 k_c 0.0\n",
      "1248 Train Loss 17.57063\n",
      "Loss  12.59848 C_bot  0.15 k_c 0.0\n",
      "Loss  21.278032 C_bot  0.15 k_c 0.0\n",
      "1249 Train Loss 26.30437\n",
      "Loss  21.278032 C_bot  0.15 k_c 0.0\n",
      "Loss  37.37889 C_bot  0.15 k_c 0.0\n",
      "1250 Train Loss 42.347107\n",
      "Loss  37.37889 C_bot  0.15 k_c 0.0\n",
      "Loss  69.75842 C_bot  0.15 k_c 0.0\n",
      "1251 Train Loss 74.85269\n",
      "Loss  69.75842 C_bot  0.15 k_c 0.0\n",
      "Loss  122.18286 C_bot  0.15 k_c 0.0\n",
      "1252 Train Loss 127.169014\n",
      "Loss  122.18286 C_bot  0.15 k_c 0.0\n",
      "Loss  216.82239 C_bot  0.15 k_c 0.0\n",
      "1253 Train Loss 222.11882\n",
      "Loss  216.82239 C_bot  0.15 k_c 0.0\n",
      "Loss  307.5342 C_bot  0.15 k_c 0.0\n",
      "1254 Train Loss 312.5721\n",
      "Loss  307.5342 C_bot  0.15 k_c 0.0\n",
      "Loss  381.46094 C_bot  0.15 k_c 0.0\n",
      "1255 Train Loss 387.11398\n",
      "Loss  381.46094 C_bot  0.15 k_c 0.0\n",
      "Loss  291.03775 C_bot  0.15 k_c 0.0\n",
      "1256 Train Loss 296.09732\n",
      "Loss  291.03775 C_bot  0.15 k_c 0.0\n",
      "Loss  110.5173 C_bot  0.15 k_c 0.0\n",
      "1257 Train Loss 116.15832\n",
      "Loss  110.5173 C_bot  0.15 k_c 0.0\n",
      "Loss  5.620429 C_bot  0.15 k_c 0.0\n",
      "1258 Train Loss 11.0817795\n",
      "Loss  5.620429 C_bot  0.15 k_c 0.0\n",
      "Loss  69.38449 C_bot  0.15 k_c 0.0\n",
      "1259 Train Loss 74.8119\n",
      "Loss  69.38449 C_bot  0.15 k_c 0.0\n",
      "Loss  171.36832 C_bot  0.15 k_c 0.0\n",
      "1260 Train Loss 177.68109\n",
      "Loss  171.36832 C_bot  0.15 k_c 0.0\n",
      "Loss  142.37563 C_bot  0.15 k_c 0.0\n",
      "1261 Train Loss 147.95024\n",
      "Loss  142.37563 C_bot  0.15 k_c 0.0\n",
      "Loss  39.124123 C_bot  0.15 k_c 0.0\n",
      "1262 Train Loss 45.341362\n",
      "Loss  39.124123 C_bot  0.15 k_c 0.0\n",
      "Loss  9.694444 C_bot  0.15 k_c 0.0\n",
      "1263 Train Loss 15.819659\n",
      "Loss  9.694444 C_bot  0.15 k_c 0.0\n",
      "Loss  74.015816 C_bot  0.15 k_c 0.0\n",
      "1264 Train Loss 79.831436\n",
      "Loss  74.015816 C_bot  0.15 k_c 0.0\n",
      "Loss  106.15748 C_bot  0.15 k_c 0.0\n",
      "1265 Train Loss 112.69587\n",
      "Loss  106.15748 C_bot  0.15 k_c 0.0\n",
      "Loss  48.010082 C_bot  0.15 k_c 0.0\n",
      "1266 Train Loss 53.90267\n",
      "Loss  48.010082 C_bot  0.15 k_c 0.0\n",
      "Loss  5.723126 C_bot  0.15 k_c 0.0\n",
      "1267 Train Loss 11.83005\n",
      "Loss  5.723126 C_bot  0.15 k_c 0.0\n",
      "Loss  37.287758 C_bot  0.15 k_c 0.0\n",
      "1268 Train Loss 43.606186\n",
      "Loss  37.287758 C_bot  0.15 k_c 0.0\n",
      "Loss  66.620926 C_bot  0.15 k_c 0.0\n",
      "1269 Train Loss 72.479095\n",
      "Loss  66.620926 C_bot  0.15 k_c 0.0\n",
      "Loss  38.133167 C_bot  0.15 k_c 0.0\n",
      "1270 Train Loss 44.41724\n",
      "Loss  38.133167 C_bot  0.15 k_c 0.0\n",
      "Loss  5.668015 C_bot  0.15 k_c 0.0\n",
      "1271 Train Loss 11.6904745\n",
      "Loss  5.668015 C_bot  0.15 k_c 0.0\n",
      "Loss  22.767462 C_bot  0.15 k_c 0.0\n",
      "1272 Train Loss 28.663872\n",
      "Loss  22.767462 C_bot  0.15 k_c 0.0\n",
      "Loss  45.14433 C_bot  0.15 k_c 0.0\n",
      "1273 Train Loss 51.329292\n",
      "Loss  45.14433 C_bot  0.15 k_c 0.0\n",
      "Loss  26.770004 C_bot  0.15 k_c 0.0\n",
      "1274 Train Loss 32.58449\n",
      "Loss  26.770004 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0383067 C_bot  0.15 k_c 0.0\n",
      "1275 Train Loss 10.931978\n",
      "Loss  5.0383067 C_bot  0.15 k_c 0.0\n",
      "Loss  16.196655 C_bot  0.15 k_c 0.0\n",
      "1276 Train Loss 22.114603\n",
      "Loss  16.196655 C_bot  0.15 k_c 0.0\n",
      "Loss  30.063656 C_bot  0.15 k_c 0.0\n",
      "1277 Train Loss 35.76164\n",
      "Loss  30.063656 C_bot  0.15 k_c 0.0\n",
      "Loss  17.92068 C_bot  0.15 k_c 0.0\n",
      "1278 Train Loss 23.75156\n",
      "Loss  17.92068 C_bot  0.15 k_c 0.0\n",
      "Loss  4.602561 C_bot  0.15 k_c 0.0\n",
      "1279 Train Loss 10.319355\n",
      "Loss  4.602561 C_bot  0.15 k_c 0.0\n",
      "Loss  13.059094 C_bot  0.15 k_c 0.0\n",
      "1280 Train Loss 18.718552\n",
      "Loss  13.059094 C_bot  0.15 k_c 0.0\n",
      "Loss  21.490358 C_bot  0.15 k_c 0.0\n",
      "1281 Train Loss 27.243015\n",
      "Loss  21.490358 C_bot  0.15 k_c 0.0\n",
      "Loss  12.227172 C_bot  0.15 k_c 0.0\n",
      "1282 Train Loss 17.863754\n",
      "Loss  12.227172 C_bot  0.15 k_c 0.0\n",
      "Loss  4.410971 C_bot  0.15 k_c 0.0\n",
      "1283 Train Loss 10.0644655\n",
      "Loss  4.410971 C_bot  0.15 k_c 0.0\n",
      "Loss  10.884503 C_bot  0.15 k_c 0.0\n",
      "1284 Train Loss 16.5554\n",
      "Loss  10.884503 C_bot  0.15 k_c 0.0\n",
      "Loss  15.487313 C_bot  0.15 k_c 0.0\n",
      "1285 Train Loss 21.082462\n",
      "Loss  15.487313 C_bot  0.15 k_c 0.0\n",
      "Loss  8.859219 C_bot  0.15 k_c 0.0\n",
      "1286 Train Loss 14.483593\n",
      "Loss  8.859219 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4764643 C_bot  0.15 k_c 0.0\n",
      "1287 Train Loss 10.057936\n",
      "Loss  4.4764643 C_bot  0.15 k_c 0.0\n",
      "Loss  9.213433 C_bot  0.15 k_c 0.0\n",
      "1288 Train Loss 14.755227\n",
      "Loss  9.213433 C_bot  0.15 k_c 0.0\n",
      "Loss  11.654397 C_bot  0.15 k_c 0.0\n",
      "1289 Train Loss 17.213856\n",
      "Loss  11.654397 C_bot  0.15 k_c 0.0\n",
      "Loss  6.900081 C_bot  0.15 k_c 0.0\n",
      "1290 Train Loss 12.400917\n",
      "Loss  6.900081 C_bot  0.15 k_c 0.0\n",
      "Loss  4.554894 C_bot  0.15 k_c 0.0\n",
      "1291 Train Loss 10.0428915\n",
      "Loss  4.554894 C_bot  0.15 k_c 0.0\n",
      "Loss  7.9309497 C_bot  0.15 k_c 0.0\n",
      "1292 Train Loss 13.41913\n",
      "Loss  7.9309497 C_bot  0.15 k_c 0.0\n",
      "Loss  9.060366 C_bot  0.15 k_c 0.0\n",
      "1293 Train Loss 14.501473\n",
      "Loss  9.060366 C_bot  0.15 k_c 0.0\n",
      "Loss  5.7427816 C_bot  0.15 k_c 0.0\n",
      "1294 Train Loss 11.19203\n",
      "Loss  5.7427816 C_bot  0.15 k_c 0.0\n",
      "Loss  4.562971 C_bot  0.15 k_c 0.0\n",
      "1295 Train Loss 9.991012\n",
      "Loss  4.562971 C_bot  0.15 k_c 0.0\n",
      "Loss  6.9172797 C_bot  0.15 k_c 0.0\n",
      "1296 Train Loss 12.315998\n",
      "Loss  6.9172797 C_bot  0.15 k_c 0.0\n",
      "Loss  7.385806 C_bot  0.15 k_c 0.0\n",
      "1297 Train Loss 12.80049\n",
      "Loss  7.385806 C_bot  0.15 k_c 0.0\n",
      "Loss  5.148689 C_bot  0.15 k_c 0.0\n",
      "1298 Train Loss 10.524756\n",
      "Loss  5.148689 C_bot  0.15 k_c 0.0\n",
      "Loss  4.559176 C_bot  0.15 k_c 0.0\n",
      "1299 Train Loss 9.925647\n",
      "Loss  4.559176 C_bot  0.15 k_c 0.0\n",
      "Loss  6.122479 C_bot  0.15 k_c 0.0\n",
      "1300 Train Loss 11.494002\n",
      "Loss  6.122479 C_bot  0.15 k_c 0.0\n",
      "Loss  6.309673 C_bot  0.15 k_c 0.0\n",
      "1301 Train Loss 11.641745\n",
      "Loss  6.309673 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7923574 C_bot  0.15 k_c 0.0\n",
      "1302 Train Loss 10.132566\n",
      "Loss  4.7923574 C_bot  0.15 k_c 0.0\n",
      "Loss  4.510562 C_bot  0.15 k_c 0.0\n",
      "1303 Train Loss 9.837065\n",
      "Loss  4.510562 C_bot  0.15 k_c 0.0\n",
      "Loss  5.577501 C_bot  0.15 k_c 0.0\n",
      "1304 Train Loss 10.879186\n",
      "Loss  5.577501 C_bot  0.15 k_c 0.0\n",
      "Loss  5.6121 C_bot  0.15 k_c 0.0\n",
      "1305 Train Loss 10.929032\n",
      "Loss  5.6121 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6305027 C_bot  0.15 k_c 0.0\n",
      "1306 Train Loss 9.9203825\n",
      "Loss  4.6305027 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4659696 C_bot  0.15 k_c 0.0\n",
      "1307 Train Loss 9.749817\n",
      "Loss  4.4659696 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1468463 C_bot  0.15 k_c 0.0\n",
      "1308 Train Loss 10.436998\n",
      "Loss  5.1468463 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1932845 C_bot  0.15 k_c 0.0\n",
      "1309 Train Loss 10.454716\n",
      "Loss  5.1932845 C_bot  0.15 k_c 0.0\n",
      "Loss  4.522303 C_bot  0.15 k_c 0.0\n",
      "1310 Train Loss 9.790069\n",
      "Loss  4.522303 C_bot  0.15 k_c 0.0\n",
      "Loss  4.405438 C_bot  0.15 k_c 0.0\n",
      "1311 Train Loss 9.662474\n",
      "Loss  4.405438 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8673143 C_bot  0.15 k_c 0.0\n",
      "1312 Train Loss 10.104996\n",
      "Loss  4.8673143 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8895698 C_bot  0.15 k_c 0.0\n",
      "1313 Train Loss 10.134727\n",
      "Loss  4.8895698 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4720774 C_bot  0.15 k_c 0.0\n",
      "1314 Train Loss 9.696243\n",
      "Loss  4.4720774 C_bot  0.15 k_c 0.0\n",
      "Loss  4.36504 C_bot  0.15 k_c 0.0\n",
      "1315 Train Loss 9.582996\n",
      "Loss  4.36504 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6486745 C_bot  0.15 k_c 0.0\n",
      "1316 Train Loss 9.866748\n",
      "Loss  4.6486745 C_bot  0.15 k_c 0.0\n",
      "Loss  4.713323 C_bot  0.15 k_c 0.0\n",
      "1317 Train Loss 9.911846\n",
      "Loss  4.713323 C_bot  0.15 k_c 0.0\n",
      "Loss  4.427017 C_bot  0.15 k_c 0.0\n",
      "1318 Train Loss 9.627721\n",
      "Loss  4.427017 C_bot  0.15 k_c 0.0\n",
      "Loss  4.326298 C_bot  0.15 k_c 0.0\n",
      "1319 Train Loss 9.517691\n",
      "Loss  4.326298 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5112925 C_bot  0.15 k_c 0.0\n",
      "1320 Train Loss 9.690808\n",
      "Loss  4.5112925 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5685844 C_bot  0.15 k_c 0.0\n",
      "1321 Train Loss 9.750208\n",
      "Loss  4.5685844 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4058785 C_bot  0.15 k_c 0.0\n",
      "1322 Train Loss 9.573045\n",
      "Loss  4.4058785 C_bot  0.15 k_c 0.0\n",
      "Loss  4.305448 C_bot  0.15 k_c 0.0\n",
      "1323 Train Loss 9.468246\n",
      "Loss  4.305448 C_bot  0.15 k_c 0.0\n",
      "Loss  4.404712 C_bot  0.15 k_c 0.0\n",
      "1324 Train Loss 9.563948\n",
      "Loss  4.404712 C_bot  0.15 k_c 0.0\n",
      "Loss  4.479605 C_bot  0.15 k_c 0.0\n",
      "1325 Train Loss 9.625982\n",
      "Loss  4.479605 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3789883 C_bot  0.15 k_c 0.0\n",
      "1326 Train Loss 9.524866\n",
      "Loss  4.3789883 C_bot  0.15 k_c 0.0\n",
      "Loss  4.294563 C_bot  0.15 k_c 0.0\n",
      "1327 Train Loss 9.431635\n",
      "Loss  4.294563 C_bot  0.15 k_c 0.0\n",
      "Loss  4.343809 C_bot  0.15 k_c 0.0\n",
      "1328 Train Loss 9.473472\n",
      "Loss  4.343809 C_bot  0.15 k_c 0.0\n",
      "Loss  4.401036 C_bot  0.15 k_c 0.0\n",
      "1329 Train Loss 9.529542\n",
      "Loss  4.401036 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3619533 C_bot  0.15 k_c 0.0\n",
      "1330 Train Loss 9.48018\n",
      "Loss  4.3619533 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2878075 C_bot  0.15 k_c 0.0\n",
      "1331 Train Loss 9.403361\n",
      "Loss  4.2878075 C_bot  0.15 k_c 0.0\n",
      "Loss  4.297217 C_bot  0.15 k_c 0.0\n",
      "1332 Train Loss 9.407953\n",
      "Loss  4.297217 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3479266 C_bot  0.15 k_c 0.0\n",
      "1333 Train Loss 9.450409\n",
      "Loss  4.3479266 C_bot  0.15 k_c 0.0\n",
      "Loss  4.333862 C_bot  0.15 k_c 0.0\n",
      "1334 Train Loss 9.435021\n",
      "Loss  4.333862 C_bot  0.15 k_c 0.0\n",
      "Loss  4.28565 C_bot  0.15 k_c 0.0\n",
      "1335 Train Loss 9.37883\n",
      "Loss  4.28565 C_bot  0.15 k_c 0.0\n",
      "Loss  4.273171 C_bot  0.15 k_c 0.0\n",
      "1336 Train Loss 9.361637\n",
      "Loss  4.273171 C_bot  0.15 k_c 0.0\n",
      "Loss  4.300776 C_bot  0.15 k_c 0.0\n",
      "1337 Train Loss 9.386129\n",
      "Loss  4.300776 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3121133 C_bot  0.15 k_c 0.0\n",
      "1338 Train Loss 9.389476\n",
      "Loss  4.3121133 C_bot  0.15 k_c 0.0\n",
      "Loss  4.279199 C_bot  0.15 k_c 0.0\n",
      "1339 Train Loss 9.35446\n",
      "Loss  4.279199 C_bot  0.15 k_c 0.0\n",
      "Loss  4.258754 C_bot  0.15 k_c 0.0\n",
      "1340 Train Loss 9.328293\n",
      "Loss  4.258754 C_bot  0.15 k_c 0.0\n",
      "Loss  4.271507 C_bot  0.15 k_c 0.0\n",
      "1341 Train Loss 9.3356495\n",
      "Loss  4.271507 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2828336 C_bot  0.15 k_c 0.0\n",
      "1342 Train Loss 9.344878\n",
      "Loss  4.2828336 C_bot  0.15 k_c 0.0\n",
      "Loss  4.272646 C_bot  0.15 k_c 0.0\n",
      "1343 Train Loss 9.327899\n",
      "Loss  4.272646 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2504406 C_bot  0.15 k_c 0.0\n",
      "1344 Train Loss 9.30296\n",
      "Loss  4.2504406 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2484565 C_bot  0.15 k_c 0.0\n",
      "1345 Train Loss 9.296834\n",
      "Loss  4.2484565 C_bot  0.15 k_c 0.0\n",
      "Loss  4.26048 C_bot  0.15 k_c 0.0\n",
      "1346 Train Loss 9.303259\n",
      "Loss  4.26048 C_bot  0.15 k_c 0.0\n",
      "Loss  4.257735 C_bot  0.15 k_c 0.0\n",
      "1347 Train Loss 9.298489\n",
      "Loss  4.257735 C_bot  0.15 k_c 0.0\n",
      "Loss  4.245395 C_bot  0.15 k_c 0.0\n",
      "1348 Train Loss 9.280378\n",
      "Loss  4.245395 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2357855 C_bot  0.15 k_c 0.0\n",
      "1349 Train Loss 9.267405\n",
      "Loss  4.2357855 C_bot  0.15 k_c 0.0\n",
      "Loss  4.238786 C_bot  0.15 k_c 0.0\n",
      "1350 Train Loss 9.26715\n",
      "Loss  4.238786 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2440953 C_bot  0.15 k_c 0.0\n",
      "1351 Train Loss 9.267142\n",
      "Loss  4.2440953 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2363796 C_bot  0.15 k_c 0.0\n",
      "1352 Train Loss 9.257335\n",
      "Loss  4.2363796 C_bot  0.15 k_c 0.0\n",
      "Loss  4.227595 C_bot  0.15 k_c 0.0\n",
      "1353 Train Loss 9.24385\n",
      "Loss  4.227595 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2243714 C_bot  0.15 k_c 0.0\n",
      "1354 Train Loss 9.237121\n",
      "Loss  4.2243714 C_bot  0.15 k_c 0.0\n",
      "Loss  4.225978 C_bot  0.15 k_c 0.0\n",
      "1355 Train Loss 9.236051\n",
      "Loss  4.225978 C_bot  0.15 k_c 0.0\n",
      "Loss  4.226822 C_bot  0.15 k_c 0.0\n",
      "1356 Train Loss 9.232116\n",
      "Loss  4.226822 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2190714 C_bot  0.15 k_c 0.0\n",
      "1357 Train Loss 9.222108\n",
      "Loss  4.2190714 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2136636 C_bot  0.15 k_c 0.0\n",
      "1358 Train Loss 9.212719\n",
      "Loss  4.2136636 C_bot  0.15 k_c 0.0\n",
      "Loss  4.212608 C_bot  0.15 k_c 0.0\n",
      "1359 Train Loss 9.208107\n",
      "Loss  4.212608 C_bot  0.15 k_c 0.0\n",
      "Loss  4.212339 C_bot  0.15 k_c 0.0\n",
      "1360 Train Loss 9.205331\n",
      "Loss  4.212339 C_bot  0.15 k_c 0.0\n",
      "Loss  4.210812 C_bot  0.15 k_c 0.0\n",
      "1361 Train Loss 9.1995\n",
      "Loss  4.210812 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2048798 C_bot  0.15 k_c 0.0\n",
      "1362 Train Loss 9.191165\n",
      "Loss  4.2048798 C_bot  0.15 k_c 0.0\n",
      "Loss  4.20112 C_bot  0.15 k_c 0.0\n",
      "1363 Train Loss 9.183936\n",
      "Loss  4.20112 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2003255 C_bot  0.15 k_c 0.0\n",
      "1364 Train Loss 9.179757\n",
      "Loss  4.2003255 C_bot  0.15 k_c 0.0\n",
      "Loss  4.198705 C_bot  0.15 k_c 0.0\n",
      "1365 Train Loss 9.175757\n",
      "Loss  4.198705 C_bot  0.15 k_c 0.0\n",
      "Loss  4.196537 C_bot  0.15 k_c 0.0\n",
      "1366 Train Loss 9.1697855\n",
      "Loss  4.196537 C_bot  0.15 k_c 0.0\n",
      "Loss  4.191769 C_bot  0.15 k_c 0.0\n",
      "1367 Train Loss 9.162609\n",
      "Loss  4.191769 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1888194 C_bot  0.15 k_c 0.0\n",
      "1368 Train Loss 9.156536\n",
      "Loss  4.1888194 C_bot  0.15 k_c 0.0\n",
      "Loss  4.187461 C_bot  0.15 k_c 0.0\n",
      "1369 Train Loss 9.152002\n",
      "Loss  4.187461 C_bot  0.15 k_c 0.0\n",
      "Loss  4.185476 C_bot  0.15 k_c 0.0\n",
      "1370 Train Loss 9.147659\n",
      "Loss  4.185476 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1832285 C_bot  0.15 k_c 0.0\n",
      "1371 Train Loss 9.141949\n",
      "Loss  4.1832285 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1793323 C_bot  0.15 k_c 0.0\n",
      "1372 Train Loss 9.135639\n",
      "Loss  4.1793323 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1766787 C_bot  0.15 k_c 0.0\n",
      "1373 Train Loss 9.130059\n",
      "Loss  4.1766787 C_bot  0.15 k_c 0.0\n",
      "Loss  4.174845 C_bot  0.15 k_c 0.0\n",
      "1374 Train Loss 9.1252575\n",
      "Loss  4.174845 C_bot  0.15 k_c 0.0\n",
      "Loss  4.17262 C_bot  0.15 k_c 0.0\n",
      "1375 Train Loss 9.120704\n",
      "Loss  4.17262 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1703734 C_bot  0.15 k_c 0.0\n",
      "1376 Train Loss 9.115296\n",
      "Loss  4.1703734 C_bot  0.15 k_c 0.0\n",
      "Loss  4.167004 C_bot  0.15 k_c 0.0\n",
      "1377 Train Loss 9.109591\n",
      "Loss  4.167004 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1643643 C_bot  0.15 k_c 0.0\n",
      "1378 Train Loss 9.104187\n",
      "Loss  4.1643643 C_bot  0.15 k_c 0.0\n",
      "Loss  4.162319 C_bot  0.15 k_c 0.0\n",
      "1379 Train Loss 9.099395\n",
      "Loss  4.162319 C_bot  0.15 k_c 0.0\n",
      "Loss  4.159898 C_bot  0.15 k_c 0.0\n",
      "1380 Train Loss 9.0946665\n",
      "Loss  4.159898 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1578007 C_bot  0.15 k_c 0.0\n",
      "1381 Train Loss 9.089634\n",
      "Loss  4.1578007 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1547327 C_bot  0.15 k_c 0.0\n",
      "1382 Train Loss 9.084291\n",
      "Loss  4.1547327 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1522665 C_bot  0.15 k_c 0.0\n",
      "1383 Train Loss 9.079147\n",
      "Loss  4.1522665 C_bot  0.15 k_c 0.0\n",
      "Loss  4.149872 C_bot  0.15 k_c 0.0\n",
      "1384 Train Loss 9.074186\n",
      "Loss  4.149872 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1475773 C_bot  0.15 k_c 0.0\n",
      "1385 Train Loss 9.069588\n",
      "Loss  4.1475773 C_bot  0.15 k_c 0.0\n",
      "Loss  4.145378 C_bot  0.15 k_c 0.0\n",
      "1386 Train Loss 9.064651\n",
      "Loss  4.145378 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1425343 C_bot  0.15 k_c 0.0\n",
      "1387 Train Loss 9.059599\n",
      "Loss  4.1425343 C_bot  0.15 k_c 0.0\n",
      "Loss  4.140144 C_bot  0.15 k_c 0.0\n",
      "1388 Train Loss 9.054632\n",
      "Loss  4.140144 C_bot  0.15 k_c 0.0\n",
      "Loss  4.137512 C_bot  0.15 k_c 0.0\n",
      "1389 Train Loss 9.049609\n",
      "Loss  4.137512 C_bot  0.15 k_c 0.0\n",
      "Loss  4.135138 C_bot  0.15 k_c 0.0\n",
      "1390 Train Loss 9.044953\n",
      "Loss  4.135138 C_bot  0.15 k_c 0.0\n",
      "Loss  4.133015 C_bot  0.15 k_c 0.0\n",
      "1391 Train Loss 9.040276\n",
      "Loss  4.133015 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1301484 C_bot  0.15 k_c 0.0\n",
      "1392 Train Loss 9.035257\n",
      "Loss  4.1301484 C_bot  0.15 k_c 0.0\n",
      "Loss  4.127943 C_bot  0.15 k_c 0.0\n",
      "1393 Train Loss 9.030554\n",
      "Loss  4.127943 C_bot  0.15 k_c 0.0\n",
      "Loss  4.125242 C_bot  0.15 k_c 0.0\n",
      "1394 Train Loss 9.025607\n",
      "Loss  4.125242 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1229777 C_bot  0.15 k_c 0.0\n",
      "1395 Train Loss 9.021059\n",
      "Loss  4.1229777 C_bot  0.15 k_c 0.0\n",
      "Loss  4.120579 C_bot  0.15 k_c 0.0\n",
      "1396 Train Loss 9.016268\n",
      "Loss  4.120579 C_bot  0.15 k_c 0.0\n",
      "Loss  4.118038 C_bot  0.15 k_c 0.0\n",
      "1397 Train Loss 9.011604\n",
      "Loss  4.118038 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1156697 C_bot  0.15 k_c 0.0\n",
      "1398 Train Loss 9.006837\n",
      "Loss  4.1156697 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1130977 C_bot  0.15 k_c 0.0\n",
      "1399 Train Loss 9.002133\n",
      "Loss  4.1130977 C_bot  0.15 k_c 0.0\n",
      "Loss  4.110635 C_bot  0.15 k_c 0.0\n",
      "1400 Train Loss 8.997413\n",
      "Loss  4.110635 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1082425 C_bot  0.15 k_c 0.0\n",
      "1401 Train Loss 8.992782\n",
      "Loss  4.1082425 C_bot  0.15 k_c 0.0\n",
      "Loss  4.105863 C_bot  0.15 k_c 0.0\n",
      "1402 Train Loss 8.988301\n",
      "Loss  4.105863 C_bot  0.15 k_c 0.0\n",
      "Loss  4.103531 C_bot  0.15 k_c 0.0\n",
      "1403 Train Loss 8.983676\n",
      "Loss  4.103531 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1009564 C_bot  0.15 k_c 0.0\n",
      "1404 Train Loss 8.97905\n",
      "Loss  4.1009564 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0986266 C_bot  0.15 k_c 0.0\n",
      "1405 Train Loss 8.97449\n",
      "Loss  4.0986266 C_bot  0.15 k_c 0.0\n",
      "Loss  4.096085 C_bot  0.15 k_c 0.0\n",
      "1406 Train Loss 8.969838\n",
      "Loss  4.096085 C_bot  0.15 k_c 0.0\n",
      "Loss  4.093641 C_bot  0.15 k_c 0.0\n",
      "1407 Train Loss 8.965294\n",
      "Loss  4.093641 C_bot  0.15 k_c 0.0\n",
      "Loss  4.091225 C_bot  0.15 k_c 0.0\n",
      "1408 Train Loss 8.960701\n",
      "Loss  4.091225 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0887003 C_bot  0.15 k_c 0.0\n",
      "1409 Train Loss 8.956165\n",
      "Loss  4.0887003 C_bot  0.15 k_c 0.0\n",
      "Loss  4.08637 C_bot  0.15 k_c 0.0\n",
      "1410 Train Loss 8.951665\n",
      "Loss  4.08637 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0838494 C_bot  0.15 k_c 0.0\n",
      "1411 Train Loss 8.947136\n",
      "Loss  4.0838494 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0814514 C_bot  0.15 k_c 0.0\n",
      "1412 Train Loss 8.942651\n",
      "Loss  4.0814514 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0790687 C_bot  0.15 k_c 0.0\n",
      "1413 Train Loss 8.9382105\n",
      "Loss  4.0790687 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0766234 C_bot  0.15 k_c 0.0\n",
      "1414 Train Loss 8.933772\n",
      "Loss  4.0766234 C_bot  0.15 k_c 0.0\n",
      "Loss  4.074284 C_bot  0.15 k_c 0.0\n",
      "1415 Train Loss 8.929347\n",
      "Loss  4.074284 C_bot  0.15 k_c 0.0\n",
      "Loss  4.07156 C_bot  0.15 k_c 0.0\n",
      "1416 Train Loss 8.924678\n",
      "Loss  4.07156 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0693407 C_bot  0.15 k_c 0.0\n",
      "1417 Train Loss 8.9204\n",
      "Loss  4.0693407 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0667887 C_bot  0.15 k_c 0.0\n",
      "1418 Train Loss 8.915894\n",
      "Loss  4.0667887 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0644364 C_bot  0.15 k_c 0.0\n",
      "1419 Train Loss 8.9115505\n",
      "Loss  4.0644364 C_bot  0.15 k_c 0.0\n",
      "Loss  4.06202 C_bot  0.15 k_c 0.0\n",
      "1420 Train Loss 8.907152\n",
      "Loss  4.06202 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0595894 C_bot  0.15 k_c 0.0\n",
      "1421 Train Loss 8.902799\n",
      "Loss  4.0595894 C_bot  0.15 k_c 0.0\n",
      "Loss  4.057098 C_bot  0.15 k_c 0.0\n",
      "1422 Train Loss 8.898314\n",
      "Loss  4.057098 C_bot  0.15 k_c 0.0\n",
      "Loss  4.054737 C_bot  0.15 k_c 0.0\n",
      "1423 Train Loss 8.894069\n",
      "Loss  4.054737 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0522733 C_bot  0.15 k_c 0.0\n",
      "1424 Train Loss 8.889638\n",
      "Loss  4.0522733 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0498157 C_bot  0.15 k_c 0.0\n",
      "1425 Train Loss 8.885294\n",
      "Loss  4.0498157 C_bot  0.15 k_c 0.0\n",
      "Loss  4.047382 C_bot  0.15 k_c 0.0\n",
      "1426 Train Loss 8.880945\n",
      "Loss  4.047382 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0450644 C_bot  0.15 k_c 0.0\n",
      "1427 Train Loss 8.876724\n",
      "Loss  4.0450644 C_bot  0.15 k_c 0.0\n",
      "Loss  4.042575 C_bot  0.15 k_c 0.0\n",
      "1428 Train Loss 8.872373\n",
      "Loss  4.042575 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0401435 C_bot  0.15 k_c 0.0\n",
      "1429 Train Loss 8.868032\n",
      "Loss  4.0401435 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0376616 C_bot  0.15 k_c 0.0\n",
      "1430 Train Loss 8.86372\n",
      "Loss  4.0376616 C_bot  0.15 k_c 0.0\n",
      "Loss  4.035261 C_bot  0.15 k_c 0.0\n",
      "1431 Train Loss 8.859427\n",
      "Loss  4.035261 C_bot  0.15 k_c 0.0\n",
      "Loss  4.032846 C_bot  0.15 k_c 0.0\n",
      "1432 Train Loss 8.855194\n",
      "Loss  4.032846 C_bot  0.15 k_c 0.0\n",
      "Loss  4.030444 C_bot  0.15 k_c 0.0\n",
      "1433 Train Loss 8.850935\n",
      "Loss  4.030444 C_bot  0.15 k_c 0.0\n",
      "Loss  4.027875 C_bot  0.15 k_c 0.0\n",
      "1434 Train Loss 8.846544\n",
      "Loss  4.027875 C_bot  0.15 k_c 0.0\n",
      "Loss  4.025607 C_bot  0.15 k_c 0.0\n",
      "1435 Train Loss 8.842463\n",
      "Loss  4.025607 C_bot  0.15 k_c 0.0\n",
      "Loss  4.023248 C_bot  0.15 k_c 0.0\n",
      "1436 Train Loss 8.838279\n",
      "Loss  4.023248 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0208564 C_bot  0.15 k_c 0.0\n",
      "1437 Train Loss 8.834106\n",
      "Loss  4.0208564 C_bot  0.15 k_c 0.0\n",
      "Loss  4.018277 C_bot  0.15 k_c 0.0\n",
      "1438 Train Loss 8.82971\n",
      "Loss  4.018277 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0158753 C_bot  0.15 k_c 0.0\n",
      "1439 Train Loss 8.825546\n",
      "Loss  4.0158753 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0134015 C_bot  0.15 k_c 0.0\n",
      "1440 Train Loss 8.821275\n",
      "Loss  4.0134015 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0111084 C_bot  0.15 k_c 0.0\n",
      "1441 Train Loss 8.817231\n",
      "Loss  4.0111084 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0086 C_bot  0.15 k_c 0.0\n",
      "1442 Train Loss 8.81295\n",
      "Loss  4.0086 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0062237 C_bot  0.15 k_c 0.0\n",
      "1443 Train Loss 8.808828\n",
      "Loss  4.0062237 C_bot  0.15 k_c 0.0\n",
      "Loss  4.00388 C_bot  0.15 k_c 0.0\n",
      "1444 Train Loss 8.804742\n",
      "Loss  4.00388 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0013447 C_bot  0.15 k_c 0.0\n",
      "1445 Train Loss 8.800466\n",
      "Loss  4.0013447 C_bot  0.15 k_c 0.0\n",
      "Loss  3.998827 C_bot  0.15 k_c 0.0\n",
      "1446 Train Loss 8.796231\n",
      "Loss  3.998827 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9965107 C_bot  0.15 k_c 0.0\n",
      "1447 Train Loss 8.792182\n",
      "Loss  3.9965107 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9940975 C_bot  0.15 k_c 0.0\n",
      "1448 Train Loss 8.788073\n",
      "Loss  3.9940975 C_bot  0.15 k_c 0.0\n",
      "Loss  3.991693 C_bot  0.15 k_c 0.0\n",
      "1449 Train Loss 8.783947\n",
      "Loss  3.991693 C_bot  0.15 k_c 0.0\n",
      "Loss  3.989241 C_bot  0.15 k_c 0.0\n",
      "1450 Train Loss 8.779815\n",
      "Loss  3.989241 C_bot  0.15 k_c 0.0\n",
      "Loss  3.98685 C_bot  0.15 k_c 0.0\n",
      "1451 Train Loss 8.775719\n",
      "Loss  3.98685 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9844222 C_bot  0.15 k_c 0.0\n",
      "1452 Train Loss 8.771624\n",
      "Loss  3.9844222 C_bot  0.15 k_c 0.0\n",
      "Loss  3.98197 C_bot  0.15 k_c 0.0\n",
      "1453 Train Loss 8.767484\n",
      "Loss  3.98197 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9794316 C_bot  0.15 k_c 0.0\n",
      "1454 Train Loss 8.763288\n",
      "Loss  3.9794316 C_bot  0.15 k_c 0.0\n",
      "Loss  3.977102 C_bot  0.15 k_c 0.0\n",
      "1455 Train Loss 8.75929\n",
      "Loss  3.977102 C_bot  0.15 k_c 0.0\n",
      "Loss  3.974586 C_bot  0.15 k_c 0.0\n",
      "1456 Train Loss 8.755127\n",
      "Loss  3.974586 C_bot  0.15 k_c 0.0\n",
      "Loss  3.972242 C_bot  0.15 k_c 0.0\n",
      "1457 Train Loss 8.751133\n",
      "Loss  3.972242 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9698954 C_bot  0.15 k_c 0.0\n",
      "1458 Train Loss 8.7471485\n",
      "Loss  3.9698954 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9675293 C_bot  0.15 k_c 0.0\n",
      "1459 Train Loss 8.743153\n",
      "Loss  3.9675293 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9650564 C_bot  0.15 k_c 0.0\n",
      "1460 Train Loss 8.739048\n",
      "Loss  3.9650564 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9626632 C_bot  0.15 k_c 0.0\n",
      "1461 Train Loss 8.735046\n",
      "Loss  3.9626632 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9602823 C_bot  0.15 k_c 0.0\n",
      "1462 Train Loss 8.73104\n",
      "Loss  3.9602823 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9576776 C_bot  0.15 k_c 0.0\n",
      "1463 Train Loss 8.726847\n",
      "Loss  3.9576776 C_bot  0.15 k_c 0.0\n",
      "Loss  3.955403 C_bot  0.15 k_c 0.0\n",
      "1464 Train Loss 8.722954\n",
      "Loss  3.955403 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9528935 C_bot  0.15 k_c 0.0\n",
      "1465 Train Loss 8.71887\n",
      "Loss  3.9528935 C_bot  0.15 k_c 0.0\n",
      "Loss  3.950592 C_bot  0.15 k_c 0.0\n",
      "1466 Train Loss 8.714964\n",
      "Loss  3.950592 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9480672 C_bot  0.15 k_c 0.0\n",
      "1467 Train Loss 8.7108755\n",
      "Loss  3.9480672 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9457707 C_bot  0.15 k_c 0.0\n",
      "1468 Train Loss 8.706994\n",
      "Loss  3.9457707 C_bot  0.15 k_c 0.0\n",
      "Loss  3.943224 C_bot  0.15 k_c 0.0\n",
      "1469 Train Loss 8.702886\n",
      "Loss  3.943224 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9408705 C_bot  0.15 k_c 0.0\n",
      "1470 Train Loss 8.698969\n",
      "Loss  3.9408705 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9384074 C_bot  0.15 k_c 0.0\n",
      "1471 Train Loss 8.694948\n",
      "Loss  3.9384074 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9360518 C_bot  0.15 k_c 0.0\n",
      "1472 Train Loss 8.691047\n",
      "Loss  3.9360518 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9335606 C_bot  0.15 k_c 0.0\n",
      "1473 Train Loss 8.687008\n",
      "Loss  3.9335606 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9311533 C_bot  0.15 k_c 0.0\n",
      "1474 Train Loss 8.683068\n",
      "Loss  3.9311533 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9287903 C_bot  0.15 k_c 0.0\n",
      "1475 Train Loss 8.679167\n",
      "Loss  3.9287903 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9263442 C_bot  0.15 k_c 0.0\n",
      "1476 Train Loss 8.675201\n",
      "Loss  3.9263442 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9239063 C_bot  0.15 k_c 0.0\n",
      "1477 Train Loss 8.671235\n",
      "Loss  3.9239063 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9215384 C_bot  0.15 k_c 0.0\n",
      "1478 Train Loss 8.66736\n",
      "Loss  3.9215384 C_bot  0.15 k_c 0.0\n",
      "Loss  3.919097 C_bot  0.15 k_c 0.0\n",
      "1479 Train Loss 8.663401\n",
      "Loss  3.919097 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9166026 C_bot  0.15 k_c 0.0\n",
      "1480 Train Loss 8.659411\n",
      "Loss  3.9166026 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9142053 C_bot  0.15 k_c 0.0\n",
      "1481 Train Loss 8.655504\n",
      "Loss  3.9142053 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9117997 C_bot  0.15 k_c 0.0\n",
      "1482 Train Loss 8.651619\n",
      "Loss  3.9117997 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9092798 C_bot  0.15 k_c 0.0\n",
      "1483 Train Loss 8.647594\n",
      "Loss  3.9092798 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9068909 C_bot  0.15 k_c 0.0\n",
      "1484 Train Loss 8.643743\n",
      "Loss  3.9068909 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9045928 C_bot  0.15 k_c 0.0\n",
      "1485 Train Loss 8.639944\n",
      "Loss  3.9045928 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9020507 C_bot  0.15 k_c 0.0\n",
      "1486 Train Loss 8.635954\n",
      "Loss  3.9020507 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8996532 C_bot  0.15 k_c 0.0\n",
      "1487 Train Loss 8.632062\n",
      "Loss  3.8996532 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8973083 C_bot  0.15 k_c 0.0\n",
      "1488 Train Loss 8.628284\n",
      "Loss  3.8973083 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8948536 C_bot  0.15 k_c 0.0\n",
      "1489 Train Loss 8.624336\n",
      "Loss  3.8948536 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8923905 C_bot  0.15 k_c 0.0\n",
      "1490 Train Loss 8.620463\n",
      "Loss  3.8923905 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8900578 C_bot  0.15 k_c 0.0\n",
      "1491 Train Loss 8.616632\n",
      "Loss  3.8900578 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8874948 C_bot  0.15 k_c 0.0\n",
      "1492 Train Loss 8.612685\n",
      "Loss  3.8874948 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8851857 C_bot  0.15 k_c 0.0\n",
      "1493 Train Loss 8.608868\n",
      "Loss  3.8851857 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8827493 C_bot  0.15 k_c 0.0\n",
      "1494 Train Loss 8.605084\n",
      "Loss  3.8827493 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8804002 C_bot  0.15 k_c 0.0\n",
      "1495 Train Loss 8.601199\n",
      "Loss  3.8804002 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8777583 C_bot  0.15 k_c 0.0\n",
      "1496 Train Loss 8.597266\n",
      "Loss  3.8777583 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8756347 C_bot  0.15 k_c 0.0\n",
      "1497 Train Loss 8.5935545\n",
      "Loss  3.8756347 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8730092 C_bot  0.15 k_c 0.0\n",
      "1498 Train Loss 8.58973\n",
      "Loss  3.8730092 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8712707 C_bot  0.15 k_c 0.0\n",
      "1499 Train Loss 8.586303\n",
      "Loss  3.8712707 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8688517 C_bot  0.15 k_c 0.0\n",
      "1500 Train Loss 8.582838\n",
      "Loss  3.8688517 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8677669 C_bot  0.15 k_c 0.0\n",
      "1501 Train Loss 8.579882\n",
      "Loss  3.8677669 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8659399 C_bot  0.15 k_c 0.0\n",
      "1502 Train Loss 8.577276\n",
      "Loss  3.8659399 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8668294 C_bot  0.15 k_c 0.0\n",
      "1503 Train Loss 8.575957\n",
      "Loss  3.8668294 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8678198 C_bot  0.15 k_c 0.0\n",
      "1504 Train Loss 8.576647\n",
      "Loss  3.8678198 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8750427 C_bot  0.15 k_c 0.0\n",
      "1505 Train Loss 8.581041\n",
      "Loss  3.8750427 C_bot  0.15 k_c 0.0\n",
      "Loss  3.88584 C_bot  0.15 k_c 0.0\n",
      "1506 Train Loss 8.59242\n",
      "Loss  3.88584 C_bot  0.15 k_c 0.0\n",
      "Loss  3.914386 C_bot  0.15 k_c 0.0\n",
      "1507 Train Loss 8.616975\n",
      "Loss  3.914386 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9620957 C_bot  0.15 k_c 0.0\n",
      "1508 Train Loss 8.666941\n",
      "Loss  3.9620957 C_bot  0.15 k_c 0.0\n",
      "Loss  4.067075 C_bot  0.15 k_c 0.0\n",
      "1509 Train Loss 8.765725\n",
      "Loss  4.067075 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2566204 C_bot  0.15 k_c 0.0\n",
      "1510 Train Loss 8.960811\n",
      "Loss  4.2566204 C_bot  0.15 k_c 0.0\n",
      "Loss  4.648482 C_bot  0.15 k_c 0.0\n",
      "1511 Train Loss 9.34227\n",
      "Loss  4.648482 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3984413 C_bot  0.15 k_c 0.0\n",
      "1512 Train Loss 10.104468\n",
      "Loss  5.3984413 C_bot  0.15 k_c 0.0\n",
      "Loss  6.9185734 C_bot  0.15 k_c 0.0\n",
      "1513 Train Loss 11.606201\n",
      "Loss  6.9185734 C_bot  0.15 k_c 0.0\n",
      "Loss  9.955497 C_bot  0.15 k_c 0.0\n",
      "1514 Train Loss 14.669821\n",
      "Loss  9.955497 C_bot  0.15 k_c 0.0\n",
      "Loss  16.034355 C_bot  0.15 k_c 0.0\n",
      "1515 Train Loss 20.715483\n",
      "Loss  16.034355 C_bot  0.15 k_c 0.0\n",
      "Loss  28.593689 C_bot  0.15 k_c 0.0\n",
      "1516 Train Loss 33.335716\n",
      "Loss  28.593689 C_bot  0.15 k_c 0.0\n",
      "Loss  52.934677 C_bot  0.15 k_c 0.0\n",
      "1517 Train Loss 57.617344\n",
      "Loss  52.934677 C_bot  0.15 k_c 0.0\n",
      "Loss  103.7438 C_bot  0.15 k_c 0.0\n",
      "1518 Train Loss 108.57999\n",
      "Loss  103.7438 C_bot  0.15 k_c 0.0\n",
      "Loss  190.92746 C_bot  0.15 k_c 0.0\n",
      "1519 Train Loss 195.65143\n",
      "Loss  190.92746 C_bot  0.15 k_c 0.0\n",
      "Loss  356.1786 C_bot  0.15 k_c 0.0\n",
      "1520 Train Loss 361.3278\n",
      "Loss  356.1786 C_bot  0.15 k_c 0.0\n",
      "Loss  530.7926 C_bot  0.15 k_c 0.0\n",
      "1521 Train Loss 535.6154\n",
      "Loss  530.7926 C_bot  0.15 k_c 0.0\n",
      "Loss  699.27734 C_bot  0.15 k_c 0.0\n",
      "1522 Train Loss 705.0405\n",
      "Loss  699.27734 C_bot  0.15 k_c 0.0\n",
      "Loss  553.96326 C_bot  0.15 k_c 0.0\n",
      "1523 Train Loss 558.77234\n",
      "Loss  553.96326 C_bot  0.15 k_c 0.0\n",
      "Loss  236.36833 C_bot  0.15 k_c 0.0\n",
      "1524 Train Loss 242.06754\n",
      "Loss  236.36833 C_bot  0.15 k_c 0.0\n",
      "Loss  10.632684 C_bot  0.15 k_c 0.0\n",
      "1525 Train Loss 15.874198\n",
      "Loss  10.632684 C_bot  0.15 k_c 0.0\n",
      "Loss  108.85257 C_bot  0.15 k_c 0.0\n",
      "1526 Train Loss 114.04089\n",
      "Loss  108.85257 C_bot  0.15 k_c 0.0\n",
      "Loss  311.86398 C_bot  0.15 k_c 0.0\n",
      "1527 Train Loss 318.3631\n",
      "Loss  311.86398 C_bot  0.15 k_c 0.0\n",
      "Loss  261.0506 C_bot  0.15 k_c 0.0\n",
      "1528 Train Loss 266.3422\n",
      "Loss  261.0506 C_bot  0.15 k_c 0.0\n",
      "Loss  67.42549 C_bot  0.15 k_c 0.0\n",
      "1529 Train Loss 73.67845\n",
      "Loss  67.42549 C_bot  0.15 k_c 0.0\n",
      "Loss  17.076694 C_bot  0.15 k_c 0.0\n",
      "1530 Train Loss 23.202404\n",
      "Loss  17.076694 C_bot  0.15 k_c 0.0\n",
      "Loss  143.59831 C_bot  0.15 k_c 0.0\n",
      "1531 Train Loss 149.2032\n",
      "Loss  143.59831 C_bot  0.15 k_c 0.0\n",
      "Loss  187.06184 C_bot  0.15 k_c 0.0\n",
      "1532 Train Loss 193.91139\n",
      "Loss  187.06184 C_bot  0.15 k_c 0.0\n",
      "Loss  59.814297 C_bot  0.15 k_c 0.0\n",
      "1533 Train Loss 65.63976\n",
      "Loss  59.814297 C_bot  0.15 k_c 0.0\n",
      "Loss  10.78503 C_bot  0.15 k_c 0.0\n",
      "1534 Train Loss 16.825338\n",
      "Loss  10.78503 C_bot  0.15 k_c 0.0\n",
      "Loss  94.66912 C_bot  0.15 k_c 0.0\n",
      "1535 Train Loss 101.28805\n",
      "Loss  94.66912 C_bot  0.15 k_c 0.0\n",
      "Loss  109.506004 C_bot  0.15 k_c 0.0\n",
      "1536 Train Loss 115.20702\n",
      "Loss  109.506004 C_bot  0.15 k_c 0.0\n",
      "Loss  29.328856 C_bot  0.15 k_c 0.0\n",
      "1537 Train Loss 35.585735\n",
      "Loss  29.328856 C_bot  0.15 k_c 0.0\n",
      "Loss  13.168935 C_bot  0.15 k_c 0.0\n",
      "1538 Train Loss 19.25182\n",
      "Loss  13.168935 C_bot  0.15 k_c 0.0\n",
      "Loss  70.27299 C_bot  0.15 k_c 0.0\n",
      "1539 Train Loss 75.890144\n",
      "Loss  70.27299 C_bot  0.15 k_c 0.0\n",
      "Loss  64.36338 C_bot  0.15 k_c 0.0\n",
      "1540 Train Loss 70.50412\n",
      "Loss  64.36338 C_bot  0.15 k_c 0.0\n",
      "Loss  10.042399 C_bot  0.15 k_c 0.0\n",
      "1541 Train Loss 15.714804\n",
      "Loss  10.042399 C_bot  0.15 k_c 0.0\n",
      "Loss  21.636154 C_bot  0.15 k_c 0.0\n",
      "1542 Train Loss 27.207266\n",
      "Loss  21.636154 C_bot  0.15 k_c 0.0\n",
      "Loss  55.838985 C_bot  0.15 k_c 0.0\n",
      "1543 Train Loss 61.732296\n",
      "Loss  55.838985 C_bot  0.15 k_c 0.0\n",
      "Loss  29.99623 C_bot  0.15 k_c 0.0\n",
      "1544 Train Loss 35.485977\n",
      "Loss  29.99623 C_bot  0.15 k_c 0.0\n",
      "Loss  4.384994 C_bot  0.15 k_c 0.0\n",
      "1545 Train Loss 9.9611025\n",
      "Loss  4.384994 C_bot  0.15 k_c 0.0\n",
      "Loss  26.967281 C_bot  0.15 k_c 0.0\n",
      "1546 Train Loss 32.64734\n",
      "Loss  26.967281 C_bot  0.15 k_c 0.0\n",
      "Loss  36.10475 C_bot  0.15 k_c 0.0\n",
      "1547 Train Loss 41.545998\n",
      "Loss  36.10475 C_bot  0.15 k_c 0.0\n",
      "Loss  11.406951 C_bot  0.15 k_c 0.0\n",
      "1548 Train Loss 16.970665\n",
      "Loss  11.406951 C_bot  0.15 k_c 0.0\n",
      "Loss  7.5478425 C_bot  0.15 k_c 0.0\n",
      "1549 Train Loss 13.063539\n",
      "Loss  7.5478425 C_bot  0.15 k_c 0.0\n",
      "Loss  25.630257 C_bot  0.15 k_c 0.0\n",
      "1550 Train Loss 31.032589\n",
      "Loss  25.630257 C_bot  0.15 k_c 0.0\n",
      "Loss  20.621586 C_bot  0.15 k_c 0.0\n",
      "1551 Train Loss 26.117691\n",
      "Loss  20.621586 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8923545 C_bot  0.15 k_c 0.0\n",
      "1552 Train Loss 10.285204\n",
      "Loss  4.8923545 C_bot  0.15 k_c 0.0\n",
      "Loss  11.840994 C_bot  0.15 k_c 0.0\n",
      "1553 Train Loss 17.189796\n",
      "Loss  11.840994 C_bot  0.15 k_c 0.0\n",
      "Loss  20.371435 C_bot  0.15 k_c 0.0\n",
      "1554 Train Loss 25.780357\n",
      "Loss  20.371435 C_bot  0.15 k_c 0.0\n",
      "Loss  9.981334 C_bot  0.15 k_c 0.0\n",
      "1555 Train Loss 15.286532\n",
      "Loss  9.981334 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7550244 C_bot  0.15 k_c 0.0\n",
      "1556 Train Loss 10.052248\n",
      "Loss  4.7550244 C_bot  0.15 k_c 0.0\n",
      "Loss  13.209724 C_bot  0.15 k_c 0.0\n",
      "1557 Train Loss 18.530792\n",
      "Loss  13.209724 C_bot  0.15 k_c 0.0\n",
      "Loss  13.246787 C_bot  0.15 k_c 0.0\n",
      "1558 Train Loss 18.476921\n",
      "Loss  13.246787 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1418667 C_bot  0.15 k_c 0.0\n",
      "1559 Train Loss 10.387459\n",
      "Loss  5.1418667 C_bot  0.15 k_c 0.0\n",
      "Loss  6.5029655 C_bot  0.15 k_c 0.0\n",
      "1560 Train Loss 11.732989\n",
      "Loss  6.5029655 C_bot  0.15 k_c 0.0\n",
      "Loss  11.621995 C_bot  0.15 k_c 0.0\n",
      "1561 Train Loss 16.782984\n",
      "Loss  11.621995 C_bot  0.15 k_c 0.0\n",
      "Loss  8.008333 C_bot  0.15 k_c 0.0\n",
      "1562 Train Loss 13.203563\n",
      "Loss  8.008333 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1921816 C_bot  0.15 k_c 0.0\n",
      "1563 Train Loss 9.343309\n",
      "Loss  4.1921816 C_bot  0.15 k_c 0.0\n",
      "Loss  7.622098 C_bot  0.15 k_c 0.0\n",
      "1564 Train Loss 12.735004\n",
      "Loss  7.622098 C_bot  0.15 k_c 0.0\n",
      "Loss  9.0054455 C_bot  0.15 k_c 0.0\n",
      "1565 Train Loss 14.159435\n",
      "Loss  9.0054455 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2261515 C_bot  0.15 k_c 0.0\n",
      "1566 Train Loss 10.321867\n",
      "Loss  5.2261515 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7184906 C_bot  0.15 k_c 0.0\n",
      "1567 Train Loss 9.806439\n",
      "Loss  4.7184906 C_bot  0.15 k_c 0.0\n",
      "Loss  7.45081 C_bot  0.15 k_c 0.0\n",
      "1568 Train Loss 12.565935\n",
      "Loss  7.45081 C_bot  0.15 k_c 0.0\n",
      "Loss  6.63597 C_bot  0.15 k_c 0.0\n",
      "1569 Train Loss 11.691892\n",
      "Loss  6.63597 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2431135 C_bot  0.15 k_c 0.0\n",
      "1570 Train Loss 9.313239\n",
      "Loss  4.2431135 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2463117 C_bot  0.15 k_c 0.0\n",
      "1571 Train Loss 10.317325\n",
      "Loss  5.2463117 C_bot  0.15 k_c 0.0\n",
      "Loss  6.594775 C_bot  0.15 k_c 0.0\n",
      "1572 Train Loss 11.619788\n",
      "Loss  6.594775 C_bot  0.15 k_c 0.0\n",
      "Loss  5.098968 C_bot  0.15 k_c 0.0\n",
      "1573 Train Loss 10.148038\n",
      "Loss  5.098968 C_bot  0.15 k_c 0.0\n",
      "Loss  4.177104 C_bot  0.15 k_c 0.0\n",
      "1574 Train Loss 9.204902\n",
      "Loss  4.177104 C_bot  0.15 k_c 0.0\n",
      "Loss  5.402311 C_bot  0.15 k_c 0.0\n",
      "1575 Train Loss 10.404644\n",
      "Loss  5.402311 C_bot  0.15 k_c 0.0\n",
      "Loss  5.6085 C_bot  0.15 k_c 0.0\n",
      "1576 Train Loss 10.632715\n",
      "Loss  5.6085 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3919425 C_bot  0.15 k_c 0.0\n",
      "1577 Train Loss 9.385012\n",
      "Loss  4.3919425 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3402495 C_bot  0.15 k_c 0.0\n",
      "1578 Train Loss 9.326013\n",
      "Loss  4.3402495 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1807914 C_bot  0.15 k_c 0.0\n",
      "1579 Train Loss 10.177251\n",
      "Loss  5.1807914 C_bot  0.15 k_c 0.0\n",
      "Loss  4.891787 C_bot  0.15 k_c 0.0\n",
      "1580 Train Loss 9.858109\n",
      "Loss  4.891787 C_bot  0.15 k_c 0.0\n",
      "Loss  4.129553 C_bot  0.15 k_c 0.0\n",
      "1581 Train Loss 9.099918\n",
      "Loss  4.129553 C_bot  0.15 k_c 0.0\n",
      "Loss  4.428904 C_bot  0.15 k_c 0.0\n",
      "1582 Train Loss 9.396677\n",
      "Loss  4.428904 C_bot  0.15 k_c 0.0\n",
      "Loss  4.879538 C_bot  0.15 k_c 0.0\n",
      "1583 Train Loss 9.825397\n",
      "Loss  4.879538 C_bot  0.15 k_c 0.0\n",
      "Loss  4.431091 C_bot  0.15 k_c 0.0\n",
      "1584 Train Loss 9.3850975\n",
      "Loss  4.431091 C_bot  0.15 k_c 0.0\n",
      "Loss  4.086315 C_bot  0.15 k_c 0.0\n",
      "1585 Train Loss 9.028482\n",
      "Loss  4.086315 C_bot  0.15 k_c 0.0\n",
      "Loss  4.43804 C_bot  0.15 k_c 0.0\n",
      "1586 Train Loss 9.368679\n",
      "Loss  4.43804 C_bot  0.15 k_c 0.0\n",
      "Loss  4.572919 C_bot  0.15 k_c 0.0\n",
      "1587 Train Loss 9.510135\n",
      "Loss  4.572919 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2085924 C_bot  0.15 k_c 0.0\n",
      "1588 Train Loss 9.130724\n",
      "Loss  4.2085924 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0979056 C_bot  0.15 k_c 0.0\n",
      "1589 Train Loss 9.016685\n",
      "Loss  4.0979056 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3599033 C_bot  0.15 k_c 0.0\n",
      "1590 Train Loss 9.280056\n",
      "Loss  4.3599033 C_bot  0.15 k_c 0.0\n",
      "Loss  4.361691 C_bot  0.15 k_c 0.0\n",
      "1591 Train Loss 9.268002\n",
      "Loss  4.361691 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0942044 C_bot  0.15 k_c 0.0\n",
      "1592 Train Loss 9.001764\n",
      "Loss  4.0942044 C_bot  0.15 k_c 0.0\n",
      "Loss  4.098704 C_bot  0.15 k_c 0.0\n",
      "1593 Train Loss 9.001727\n",
      "Loss  4.098704 C_bot  0.15 k_c 0.0\n",
      "Loss  4.277265 C_bot  0.15 k_c 0.0\n",
      "1594 Train Loss 9.170193\n",
      "Loss  4.277265 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2064557 C_bot  0.15 k_c 0.0\n",
      "1595 Train Loss 9.101742\n",
      "Loss  4.2064557 C_bot  0.15 k_c 0.0\n",
      "Loss  4.045618 C_bot  0.15 k_c 0.0\n",
      "1596 Train Loss 8.9323\n",
      "Loss  4.045618 C_bot  0.15 k_c 0.0\n",
      "Loss  4.089652 C_bot  0.15 k_c 0.0\n",
      "1597 Train Loss 8.970923\n",
      "Loss  4.089652 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1848373 C_bot  0.15 k_c 0.0\n",
      "1598 Train Loss 9.066945\n",
      "Loss  4.1848373 C_bot  0.15 k_c 0.0\n",
      "Loss  4.117133 C_bot  0.15 k_c 0.0\n",
      "1599 Train Loss 8.989532\n",
      "Loss  4.117133 C_bot  0.15 k_c 0.0\n",
      "Loss  4.016099 C_bot  0.15 k_c 0.0\n",
      "1600 Train Loss 8.88714\n",
      "Loss  4.016099 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0607305 C_bot  0.15 k_c 0.0\n",
      "1601 Train Loss 8.929463\n",
      "Loss  4.0607305 C_bot  0.15 k_c 0.0\n",
      "Loss  4.118401 C_bot  0.15 k_c 0.0\n",
      "1602 Train Loss 8.978948\n",
      "Loss  4.118401 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0512075 C_bot  0.15 k_c 0.0\n",
      "1603 Train Loss 8.912526\n",
      "Loss  4.0512075 C_bot  0.15 k_c 0.0\n",
      "Loss  3.995345 C_bot  0.15 k_c 0.0\n",
      "1604 Train Loss 8.851129\n",
      "Loss  3.995345 C_bot  0.15 k_c 0.0\n",
      "Loss  4.033718 C_bot  0.15 k_c 0.0\n",
      "1605 Train Loss 8.8843155\n",
      "Loss  4.033718 C_bot  0.15 k_c 0.0\n",
      "Loss  4.056533 C_bot  0.15 k_c 0.0\n",
      "1606 Train Loss 8.907528\n",
      "Loss  4.056533 C_bot  0.15 k_c 0.0\n",
      "Loss  4.011936 C_bot  0.15 k_c 0.0\n",
      "1607 Train Loss 8.855834\n",
      "Loss  4.011936 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9750276 C_bot  0.15 k_c 0.0\n",
      "1608 Train Loss 8.81689\n",
      "Loss  3.9750276 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9983907 C_bot  0.15 k_c 0.0\n",
      "1609 Train Loss 8.838562\n",
      "Loss  3.9983907 C_bot  0.15 k_c 0.0\n",
      "Loss  4.014221 C_bot  0.15 k_c 0.0\n",
      "1610 Train Loss 8.848004\n",
      "Loss  4.014221 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9756165 C_bot  0.15 k_c 0.0\n",
      "1611 Train Loss 8.809237\n",
      "Loss  3.9756165 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9522328 C_bot  0.15 k_c 0.0\n",
      "1612 Train Loss 8.781855\n",
      "Loss  3.9522328 C_bot  0.15 k_c 0.0\n",
      "Loss  3.968211 C_bot  0.15 k_c 0.0\n",
      "1613 Train Loss 8.793611\n",
      "Loss  3.968211 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9709723 C_bot  0.15 k_c 0.0\n",
      "1614 Train Loss 8.796114\n",
      "Loss  3.9709723 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9475188 C_bot  0.15 k_c 0.0\n",
      "1615 Train Loss 8.767431\n",
      "Loss  3.9475188 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9279723 C_bot  0.15 k_c 0.0\n",
      "1616 Train Loss 8.745871\n",
      "Loss  3.9279723 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9337885 C_bot  0.15 k_c 0.0\n",
      "1617 Train Loss 8.74992\n",
      "Loss  3.9337885 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9376323 C_bot  0.15 k_c 0.0\n",
      "1618 Train Loss 8.748963\n",
      "Loss  3.9376323 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9165893 C_bot  0.15 k_c 0.0\n",
      "1619 Train Loss 8.727136\n",
      "Loss  3.9165893 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9014246 C_bot  0.15 k_c 0.0\n",
      "1620 Train Loss 8.708609\n",
      "Loss  3.9014246 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9033122 C_bot  0.15 k_c 0.0\n",
      "1621 Train Loss 8.707248\n",
      "Loss  3.9033122 C_bot  0.15 k_c 0.0\n",
      "Loss  3.901202 C_bot  0.15 k_c 0.0\n",
      "1622 Train Loss 8.704243\n",
      "Loss  3.901202 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8886993 C_bot  0.15 k_c 0.0\n",
      "1623 Train Loss 8.687739\n",
      "Loss  3.8886993 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8737783 C_bot  0.15 k_c 0.0\n",
      "1624 Train Loss 8.671118\n",
      "Loss  3.8737783 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8704126 C_bot  0.15 k_c 0.0\n",
      "1625 Train Loss 8.665803\n",
      "Loss  3.8704126 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8698115 C_bot  0.15 k_c 0.0\n",
      "1626 Train Loss 8.661686\n",
      "Loss  3.8698115 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8578281 C_bot  0.15 k_c 0.0\n",
      "1627 Train Loss 8.6486635\n",
      "Loss  3.8578281 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8460362 C_bot  0.15 k_c 0.0\n",
      "1628 Train Loss 8.6339035\n",
      "Loss  3.8460362 C_bot  0.15 k_c 0.0\n",
      "Loss  3.840587 C_bot  0.15 k_c 0.0\n",
      "1629 Train Loss 8.626044\n",
      "Loss  3.840587 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8367498 C_bot  0.15 k_c 0.0\n",
      "1630 Train Loss 8.62084\n",
      "Loss  3.8367498 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8297963 C_bot  0.15 k_c 0.0\n",
      "1631 Train Loss 8.610678\n",
      "Loss  3.8297963 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8181808 C_bot  0.15 k_c 0.0\n",
      "1632 Train Loss 8.597591\n",
      "Loss  3.8181808 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8111913 C_bot  0.15 k_c 0.0\n",
      "1633 Train Loss 8.588442\n",
      "Loss  3.8111913 C_bot  0.15 k_c 0.0\n",
      "Loss  3.807814 C_bot  0.15 k_c 0.0\n",
      "1634 Train Loss 8.582413\n",
      "Loss  3.807814 C_bot  0.15 k_c 0.0\n",
      "Loss  3.800828 C_bot  0.15 k_c 0.0\n",
      "1635 Train Loss 8.574155\n",
      "Loss  3.800828 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7926738 C_bot  0.15 k_c 0.0\n",
      "1636 Train Loss 8.563266\n",
      "Loss  3.7926738 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7850301 C_bot  0.15 k_c 0.0\n",
      "1637 Train Loss 8.55376\n",
      "Loss  3.7850301 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7800465 C_bot  0.15 k_c 0.0\n",
      "1638 Train Loss 8.547016\n",
      "Loss  3.7800465 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7758296 C_bot  0.15 k_c 0.0\n",
      "1639 Train Loss 8.540134\n",
      "Loss  3.7758296 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7681677 C_bot  0.15 k_c 0.0\n",
      "1640 Train Loss 8.531036\n",
      "Loss  3.7681677 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7615187 C_bot  0.15 k_c 0.0\n",
      "1641 Train Loss 8.522022\n",
      "Loss  3.7615187 C_bot  0.15 k_c 0.0\n",
      "Loss  3.756328 C_bot  0.15 k_c 0.0\n",
      "1642 Train Loss 8.514736\n",
      "Loss  3.756328 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7515407 C_bot  0.15 k_c 0.0\n",
      "1643 Train Loss 8.508345\n",
      "Loss  3.7515407 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7467365 C_bot  0.15 k_c 0.0\n",
      "1644 Train Loss 8.501018\n",
      "Loss  3.7467365 C_bot  0.15 k_c 0.0\n",
      "Loss  3.740024 C_bot  0.15 k_c 0.0\n",
      "1645 Train Loss 8.492712\n",
      "Loss  3.740024 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7346053 C_bot  0.15 k_c 0.0\n",
      "1646 Train Loss 8.485195\n",
      "Loss  3.7346053 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7304714 C_bot  0.15 k_c 0.0\n",
      "1647 Train Loss 8.478886\n",
      "Loss  3.7304714 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7254589 C_bot  0.15 k_c 0.0\n",
      "1648 Train Loss 8.472288\n",
      "Loss  3.7254589 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7206228 C_bot  0.15 k_c 0.0\n",
      "1649 Train Loss 8.465079\n",
      "Loss  3.7206228 C_bot  0.15 k_c 0.0\n",
      "Loss  3.714978 C_bot  0.15 k_c 0.0\n",
      "1650 Train Loss 8.457706\n",
      "Loss  3.714978 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7103038 C_bot  0.15 k_c 0.0\n",
      "1651 Train Loss 8.451077\n",
      "Loss  3.7103038 C_bot  0.15 k_c 0.0\n",
      "Loss  3.70625 C_bot  0.15 k_c 0.0\n",
      "1652 Train Loss 8.444848\n",
      "Loss  3.70625 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7011898 C_bot  0.15 k_c 0.0\n",
      "1653 Train Loss 8.438174\n",
      "Loss  3.7011898 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6964722 C_bot  0.15 k_c 0.0\n",
      "1654 Train Loss 8.4312315\n",
      "Loss  3.6964722 C_bot  0.15 k_c 0.0\n",
      "Loss  3.691272 C_bot  0.15 k_c 0.0\n",
      "1655 Train Loss 8.424241\n",
      "Loss  3.691272 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6865668 C_bot  0.15 k_c 0.0\n",
      "1656 Train Loss 8.417666\n",
      "Loss  3.6865668 C_bot  0.15 k_c 0.0\n",
      "Loss  3.682382 C_bot  0.15 k_c 0.0\n",
      "1657 Train Loss 8.411359\n",
      "Loss  3.682382 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6773615 C_bot  0.15 k_c 0.0\n",
      "1658 Train Loss 8.404685\n",
      "Loss  3.6773615 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6726623 C_bot  0.15 k_c 0.0\n",
      "1659 Train Loss 8.397866\n",
      "Loss  3.6726623 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6677077 C_bot  0.15 k_c 0.0\n",
      "1660 Train Loss 8.391086\n",
      "Loss  3.6677077 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6629748 C_bot  0.15 k_c 0.0\n",
      "1661 Train Loss 8.384514\n",
      "Loss  3.6629748 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6586087 C_bot  0.15 k_c 0.0\n",
      "1662 Train Loss 8.378075\n",
      "Loss  3.6586087 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6535277 C_bot  0.15 k_c 0.0\n",
      "1663 Train Loss 8.3713045\n",
      "Loss  3.6535277 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6487634 C_bot  0.15 k_c 0.0\n",
      "1664 Train Loss 8.3644905\n",
      "Loss  3.6487634 C_bot  0.15 k_c 0.0\n",
      "Loss  3.643732 C_bot  0.15 k_c 0.0\n",
      "1665 Train Loss 8.357628\n",
      "Loss  3.643732 C_bot  0.15 k_c 0.0\n",
      "Loss  3.638919 C_bot  0.15 k_c 0.0\n",
      "1666 Train Loss 8.350982\n",
      "Loss  3.638919 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6343088 C_bot  0.15 k_c 0.0\n",
      "1667 Train Loss 8.344347\n",
      "Loss  3.6343088 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6291249 C_bot  0.15 k_c 0.0\n",
      "1668 Train Loss 8.3374405\n",
      "Loss  3.6291249 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6242912 C_bot  0.15 k_c 0.0\n",
      "1669 Train Loss 8.330584\n",
      "Loss  3.6242912 C_bot  0.15 k_c 0.0\n",
      "Loss  3.619221 C_bot  0.15 k_c 0.0\n",
      "1670 Train Loss 8.323679\n",
      "Loss  3.619221 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6143281 C_bot  0.15 k_c 0.0\n",
      "1671 Train Loss 8.316918\n",
      "Loss  3.6143281 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6095724 C_bot  0.15 k_c 0.0\n",
      "1672 Train Loss 8.310167\n",
      "Loss  3.6095724 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6044774 C_bot  0.15 k_c 0.0\n",
      "1673 Train Loss 8.303296\n",
      "Loss  3.6044774 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5994704 C_bot  0.15 k_c 0.0\n",
      "1674 Train Loss 8.296273\n",
      "Loss  3.5994704 C_bot  0.15 k_c 0.0\n",
      "Loss  3.594293 C_bot  0.15 k_c 0.0\n",
      "1675 Train Loss 8.289246\n",
      "Loss  3.594293 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5892682 C_bot  0.15 k_c 0.0\n",
      "1676 Train Loss 8.282308\n",
      "Loss  3.5892682 C_bot  0.15 k_c 0.0\n",
      "Loss  3.584401 C_bot  0.15 k_c 0.0\n",
      "1677 Train Loss 8.275456\n",
      "Loss  3.584401 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5792892 C_bot  0.15 k_c 0.0\n",
      "1678 Train Loss 8.268504\n",
      "Loss  3.5792892 C_bot  0.15 k_c 0.0\n",
      "Loss  3.574242 C_bot  0.15 k_c 0.0\n",
      "1679 Train Loss 8.261424\n",
      "Loss  3.574242 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5691133 C_bot  0.15 k_c 0.0\n",
      "1680 Train Loss 8.254404\n",
      "Loss  3.5691133 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5640564 C_bot  0.15 k_c 0.0\n",
      "1681 Train Loss 8.247367\n",
      "Loss  3.5640564 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5590591 C_bot  0.15 k_c 0.0\n",
      "1682 Train Loss 8.240368\n",
      "Loss  3.5590591 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5539787 C_bot  0.15 k_c 0.0\n",
      "1683 Train Loss 8.23336\n",
      "Loss  3.5539787 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5490782 C_bot  0.15 k_c 0.0\n",
      "1684 Train Loss 8.22639\n",
      "Loss  3.5490782 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5438144 C_bot  0.15 k_c 0.0\n",
      "1685 Train Loss 8.219174\n",
      "Loss  3.5438144 C_bot  0.15 k_c 0.0\n",
      "Loss  3.538975 C_bot  0.15 k_c 0.0\n",
      "1686 Train Loss 8.212274\n",
      "Loss  3.538975 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5340087 C_bot  0.15 k_c 0.0\n",
      "1687 Train Loss 8.205269\n",
      "Loss  3.5340087 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5288243 C_bot  0.15 k_c 0.0\n",
      "1688 Train Loss 8.198053\n",
      "Loss  3.5288243 C_bot  0.15 k_c 0.0\n",
      "Loss  3.523919 C_bot  0.15 k_c 0.0\n",
      "1689 Train Loss 8.19103\n",
      "Loss  3.523919 C_bot  0.15 k_c 0.0\n",
      "Loss  3.518931 C_bot  0.15 k_c 0.0\n",
      "1690 Train Loss 8.183999\n",
      "Loss  3.518931 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5139873 C_bot  0.15 k_c 0.0\n",
      "1691 Train Loss 8.176912\n",
      "Loss  3.5139873 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5089977 C_bot  0.15 k_c 0.0\n",
      "1692 Train Loss 8.169819\n",
      "Loss  3.5089977 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5040882 C_bot  0.15 k_c 0.0\n",
      "1693 Train Loss 8.162783\n",
      "Loss  3.5040882 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4991658 C_bot  0.15 k_c 0.0\n",
      "1694 Train Loss 8.155687\n",
      "Loss  3.4991658 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4943008 C_bot  0.15 k_c 0.0\n",
      "1695 Train Loss 8.148704\n",
      "Loss  3.4943008 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4894872 C_bot  0.15 k_c 0.0\n",
      "1696 Train Loss 8.141691\n",
      "Loss  3.4894872 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4846141 C_bot  0.15 k_c 0.0\n",
      "1697 Train Loss 8.134679\n",
      "Loss  3.4846141 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4798257 C_bot  0.15 k_c 0.0\n",
      "1698 Train Loss 8.12771\n",
      "Loss  3.4798257 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4750435 C_bot  0.15 k_c 0.0\n",
      "1699 Train Loss 8.120762\n",
      "Loss  3.4750435 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4702196 C_bot  0.15 k_c 0.0\n",
      "1700 Train Loss 8.113802\n",
      "Loss  3.4702196 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4655402 C_bot  0.15 k_c 0.0\n",
      "1701 Train Loss 8.10696\n",
      "Loss  3.4655402 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4607625 C_bot  0.15 k_c 0.0\n",
      "1702 Train Loss 8.100093\n",
      "Loss  3.4607625 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4561026 C_bot  0.15 k_c 0.0\n",
      "1703 Train Loss 8.093323\n",
      "Loss  3.4561026 C_bot  0.15 k_c 0.0\n",
      "Loss  3.451556 C_bot  0.15 k_c 0.0\n",
      "1704 Train Loss 8.086738\n",
      "Loss  3.451556 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4469285 C_bot  0.15 k_c 0.0\n",
      "1705 Train Loss 8.080091\n",
      "Loss  3.4469285 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4423804 C_bot  0.15 k_c 0.0\n",
      "1706 Train Loss 8.073572\n",
      "Loss  3.4423804 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4379423 C_bot  0.15 k_c 0.0\n",
      "1707 Train Loss 8.067231\n",
      "Loss  3.4379423 C_bot  0.15 k_c 0.0\n",
      "Loss  3.433454 C_bot  0.15 k_c 0.0\n",
      "1708 Train Loss 8.060861\n",
      "Loss  3.433454 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4290044 C_bot  0.15 k_c 0.0\n",
      "1709 Train Loss 8.054627\n",
      "Loss  3.4290044 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4247131 C_bot  0.15 k_c 0.0\n",
      "1710 Train Loss 8.04857\n",
      "Loss  3.4247131 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4203296 C_bot  0.15 k_c 0.0\n",
      "1711 Train Loss 8.04251\n",
      "Loss  3.4203296 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4161787 C_bot  0.15 k_c 0.0\n",
      "1712 Train Loss 8.036726\n",
      "Loss  3.4161787 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4120045 C_bot  0.15 k_c 0.0\n",
      "1713 Train Loss 8.030972\n",
      "Loss  3.4120045 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4078104 C_bot  0.15 k_c 0.0\n",
      "1714 Train Loss 8.0252695\n",
      "Loss  3.4078104 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4037304 C_bot  0.15 k_c 0.0\n",
      "1715 Train Loss 8.019701\n",
      "Loss  3.4037304 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3997371 C_bot  0.15 k_c 0.0\n",
      "1716 Train Loss 8.0143\n",
      "Loss  3.3997371 C_bot  0.15 k_c 0.0\n",
      "Loss  3.395843 C_bot  0.15 k_c 0.0\n",
      "1717 Train Loss 8.009007\n",
      "Loss  3.395843 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3919423 C_bot  0.15 k_c 0.0\n",
      "1718 Train Loss 8.003768\n",
      "Loss  3.3919423 C_bot  0.15 k_c 0.0\n",
      "Loss  3.388172 C_bot  0.15 k_c 0.0\n",
      "1719 Train Loss 7.998679\n",
      "Loss  3.388172 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3843284 C_bot  0.15 k_c 0.0\n",
      "1720 Train Loss 7.993546\n",
      "Loss  3.3843284 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3807368 C_bot  0.15 k_c 0.0\n",
      "1721 Train Loss 7.9886975\n",
      "Loss  3.3807368 C_bot  0.15 k_c 0.0\n",
      "Loss  3.377146 C_bot  0.15 k_c 0.0\n",
      "1722 Train Loss 7.9838543\n",
      "Loss  3.377146 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3735194 C_bot  0.15 k_c 0.0\n",
      "1723 Train Loss 7.9790134\n",
      "Loss  3.3735194 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3699734 C_bot  0.15 k_c 0.0\n",
      "1724 Train Loss 7.9742465\n",
      "Loss  3.3699734 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3666117 C_bot  0.15 k_c 0.0\n",
      "1725 Train Loss 7.969695\n",
      "Loss  3.3666117 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3631546 C_bot  0.15 k_c 0.0\n",
      "1726 Train Loss 7.965046\n",
      "Loss  3.3631546 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3598619 C_bot  0.15 k_c 0.0\n",
      "1727 Train Loss 7.960575\n",
      "Loss  3.3598619 C_bot  0.15 k_c 0.0\n",
      "Loss  3.356466 C_bot  0.15 k_c 0.0\n",
      "1728 Train Loss 7.95601\n",
      "Loss  3.356466 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3532915 C_bot  0.15 k_c 0.0\n",
      "1729 Train Loss 7.951668\n",
      "Loss  3.3532915 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3500626 C_bot  0.15 k_c 0.0\n",
      "1730 Train Loss 7.9472833\n",
      "Loss  3.3500626 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3468857 C_bot  0.15 k_c 0.0\n",
      "1731 Train Loss 7.9429483\n",
      "Loss  3.3468857 C_bot  0.15 k_c 0.0\n",
      "Loss  3.343845 C_bot  0.15 k_c 0.0\n",
      "1732 Train Loss 7.9387603\n",
      "Loss  3.343845 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3407714 C_bot  0.15 k_c 0.0\n",
      "1733 Train Loss 7.9345384\n",
      "Loss  3.3407714 C_bot  0.15 k_c 0.0\n",
      "Loss  3.337767 C_bot  0.15 k_c 0.0\n",
      "1734 Train Loss 7.930393\n",
      "Loss  3.337767 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3347113 C_bot  0.15 k_c 0.0\n",
      "1735 Train Loss 7.926197\n",
      "Loss  3.3347113 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3318546 C_bot  0.15 k_c 0.0\n",
      "1736 Train Loss 7.922205\n",
      "Loss  3.3318546 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3288467 C_bot  0.15 k_c 0.0\n",
      "1737 Train Loss 7.9180627\n",
      "Loss  3.3288467 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3259745 C_bot  0.15 k_c 0.0\n",
      "1738 Train Loss 7.9140596\n",
      "Loss  3.3259745 C_bot  0.15 k_c 0.0\n",
      "Loss  3.323057 C_bot  0.15 k_c 0.0\n",
      "1739 Train Loss 7.9100122\n",
      "Loss  3.323057 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3201888 C_bot  0.15 k_c 0.0\n",
      "1740 Train Loss 7.9060173\n",
      "Loss  3.3201888 C_bot  0.15 k_c 0.0\n",
      "Loss  3.317403 C_bot  0.15 k_c 0.0\n",
      "1741 Train Loss 7.9021053\n",
      "Loss  3.317403 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3145669 C_bot  0.15 k_c 0.0\n",
      "1742 Train Loss 7.898145\n",
      "Loss  3.3145669 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3119261 C_bot  0.15 k_c 0.0\n",
      "1743 Train Loss 7.8943825\n",
      "Loss  3.3119261 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3092246 C_bot  0.15 k_c 0.0\n",
      "1744 Train Loss 7.8905582\n",
      "Loss  3.3092246 C_bot  0.15 k_c 0.0\n",
      "Loss  3.306363 C_bot  0.15 k_c 0.0\n",
      "1745 Train Loss 7.88658\n",
      "Loss  3.306363 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3036978 C_bot  0.15 k_c 0.0\n",
      "1746 Train Loss 7.882791\n",
      "Loss  3.3036978 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3010745 C_bot  0.15 k_c 0.0\n",
      "1747 Train Loss 7.8790545\n",
      "Loss  3.3010745 C_bot  0.15 k_c 0.0\n",
      "Loss  3.298454 C_bot  0.15 k_c 0.0\n",
      "1748 Train Loss 7.8753133\n",
      "Loss  3.298454 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2958283 C_bot  0.15 k_c 0.0\n",
      "1749 Train Loss 7.871574\n",
      "Loss  3.2958283 C_bot  0.15 k_c 0.0\n",
      "Loss  3.293244 C_bot  0.15 k_c 0.0\n",
      "1750 Train Loss 7.867875\n",
      "Loss  3.293244 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2906518 C_bot  0.15 k_c 0.0\n",
      "1751 Train Loss 7.864167\n",
      "Loss  3.2906518 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2880223 C_bot  0.15 k_c 0.0\n",
      "1752 Train Loss 7.8604317\n",
      "Loss  3.2880223 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2854903 C_bot  0.15 k_c 0.0\n",
      "1753 Train Loss 7.8567805\n",
      "Loss  3.2854903 C_bot  0.15 k_c 0.0\n",
      "Loss  3.28296 C_bot  0.15 k_c 0.0\n",
      "1754 Train Loss 7.8531537\n",
      "Loss  3.28296 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2805715 C_bot  0.15 k_c 0.0\n",
      "1755 Train Loss 7.849644\n",
      "Loss  3.2805715 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2779956 C_bot  0.15 k_c 0.0\n",
      "1756 Train Loss 7.8459787\n",
      "Loss  3.2779956 C_bot  0.15 k_c 0.0\n",
      "Loss  3.275576 C_bot  0.15 k_c 0.0\n",
      "1757 Train Loss 7.842441\n",
      "Loss  3.275576 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2730281 C_bot  0.15 k_c 0.0\n",
      "1758 Train Loss 7.8388095\n",
      "Loss  3.2730281 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2706656 C_bot  0.15 k_c 0.0\n",
      "1759 Train Loss 7.835334\n",
      "Loss  3.2706656 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2681837 C_bot  0.15 k_c 0.0\n",
      "1760 Train Loss 7.8317733\n",
      "Loss  3.2681837 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2657719 C_bot  0.15 k_c 0.0\n",
      "1761 Train Loss 7.8282547\n",
      "Loss  3.2657719 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2633264 C_bot  0.15 k_c 0.0\n",
      "1762 Train Loss 7.8247385\n",
      "Loss  3.2633264 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2609925 C_bot  0.15 k_c 0.0\n",
      "1763 Train Loss 7.821302\n",
      "Loss  3.2609925 C_bot  0.15 k_c 0.0\n",
      "Loss  3.258513 C_bot  0.15 k_c 0.0\n",
      "1764 Train Loss 7.8177605\n",
      "Loss  3.258513 C_bot  0.15 k_c 0.0\n",
      "Loss  3.25628 C_bot  0.15 k_c 0.0\n",
      "1765 Train Loss 7.8144317\n",
      "Loss  3.25628 C_bot  0.15 k_c 0.0\n",
      "Loss  3.253708 C_bot  0.15 k_c 0.0\n",
      "1766 Train Loss 7.8108068\n",
      "Loss  3.253708 C_bot  0.15 k_c 0.0\n",
      "Loss  3.251449 C_bot  0.15 k_c 0.0\n",
      "1767 Train Loss 7.8074584\n",
      "Loss  3.251449 C_bot  0.15 k_c 0.0\n",
      "Loss  3.249117 C_bot  0.15 k_c 0.0\n",
      "1768 Train Loss 7.804083\n",
      "Loss  3.249117 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2468114 C_bot  0.15 k_c 0.0\n",
      "1769 Train Loss 7.800695\n",
      "Loss  3.2468114 C_bot  0.15 k_c 0.0\n",
      "Loss  3.244401 C_bot  0.15 k_c 0.0\n",
      "1770 Train Loss 7.797253\n",
      "Loss  3.244401 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2421343 C_bot  0.15 k_c 0.0\n",
      "1771 Train Loss 7.7939067\n",
      "Loss  3.2421343 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2397192 C_bot  0.15 k_c 0.0\n",
      "1772 Train Loss 7.790477\n",
      "Loss  3.2397192 C_bot  0.15 k_c 0.0\n",
      "Loss  3.237468 C_bot  0.15 k_c 0.0\n",
      "1773 Train Loss 7.787145\n",
      "Loss  3.237468 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2352178 C_bot  0.15 k_c 0.0\n",
      "1774 Train Loss 7.7839\n",
      "Loss  3.2352178 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2328029 C_bot  0.15 k_c 0.0\n",
      "1775 Train Loss 7.7804\n",
      "Loss  3.2328029 C_bot  0.15 k_c 0.0\n",
      "Loss  3.23049 C_bot  0.15 k_c 0.0\n",
      "1776 Train Loss 7.777118\n",
      "Loss  3.23049 C_bot  0.15 k_c 0.0\n",
      "Loss  3.228318 C_bot  0.15 k_c 0.0\n",
      "1777 Train Loss 7.773847\n",
      "Loss  3.228318 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2259037 C_bot  0.15 k_c 0.0\n",
      "1778 Train Loss 7.770497\n",
      "Loss  3.2259037 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2236989 C_bot  0.15 k_c 0.0\n",
      "1779 Train Loss 7.7671733\n",
      "Loss  3.2236989 C_bot  0.15 k_c 0.0\n",
      "Loss  3.221294 C_bot  0.15 k_c 0.0\n",
      "1780 Train Loss 7.763876\n",
      "Loss  3.221294 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2192473 C_bot  0.15 k_c 0.0\n",
      "1781 Train Loss 7.760676\n",
      "Loss  3.2192473 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2169836 C_bot  0.15 k_c 0.0\n",
      "1782 Train Loss 7.7575793\n",
      "Loss  3.2169836 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2149403 C_bot  0.15 k_c 0.0\n",
      "1783 Train Loss 7.754326\n",
      "Loss  3.2149403 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2125554 C_bot  0.15 k_c 0.0\n",
      "1784 Train Loss 7.751198\n",
      "Loss  3.2125554 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2109847 C_bot  0.15 k_c 0.0\n",
      "1785 Train Loss 7.7483196\n",
      "Loss  3.2109847 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2087355 C_bot  0.15 k_c 0.0\n",
      "1786 Train Loss 7.745467\n",
      "Loss  3.2087355 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2075806 C_bot  0.15 k_c 0.0\n",
      "1787 Train Loss 7.7428417\n",
      "Loss  3.2075806 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2058446 C_bot  0.15 k_c 0.0\n",
      "1788 Train Loss 7.7407303\n",
      "Loss  3.2058446 C_bot  0.15 k_c 0.0\n",
      "Loss  3.206113 C_bot  0.15 k_c 0.0\n",
      "1789 Train Loss 7.739254\n",
      "Loss  3.206113 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2057586 C_bot  0.15 k_c 0.0\n",
      "1790 Train Loss 7.7388983\n",
      "Loss  3.2057586 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2098975 C_bot  0.15 k_c 0.0\n",
      "1791 Train Loss 7.740825\n",
      "Loss  3.2098975 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2144766 C_bot  0.15 k_c 0.0\n",
      "1792 Train Loss 7.746041\n",
      "Loss  3.2144766 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2289374 C_bot  0.15 k_c 0.0\n",
      "1793 Train Loss 7.7574763\n",
      "Loss  3.2289374 C_bot  0.15 k_c 0.0\n",
      "Loss  3.249207 C_bot  0.15 k_c 0.0\n",
      "1794 Train Loss 7.779504\n",
      "Loss  3.249207 C_bot  0.15 k_c 0.0\n",
      "Loss  3.294819 C_bot  0.15 k_c 0.0\n",
      "1795 Train Loss 7.8206577\n",
      "Loss  3.294819 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3666813 C_bot  0.15 k_c 0.0\n",
      "1796 Train Loss 7.8962936\n",
      "Loss  3.3666813 C_bot  0.15 k_c 0.0\n",
      "Loss  3.510471 C_bot  0.15 k_c 0.0\n",
      "1797 Train Loss 8.033077\n",
      "Loss  3.510471 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7543168 C_bot  0.15 k_c 0.0\n",
      "1798 Train Loss 8.284418\n",
      "Loss  3.7543168 C_bot  0.15 k_c 0.0\n",
      "Loss  4.221167 C_bot  0.15 k_c 0.0\n",
      "1799 Train Loss 8.739713\n",
      "Loss  4.221167 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0566063 C_bot  0.15 k_c 0.0\n",
      "1800 Train Loss 9.589773\n",
      "Loss  5.0566063 C_bot  0.15 k_c 0.0\n",
      "Loss  6.6218944 C_bot  0.15 k_c 0.0\n",
      "1801 Train Loss 11.135392\n",
      "Loss  6.6218944 C_bot  0.15 k_c 0.0\n",
      "Loss  9.544089 C_bot  0.15 k_c 0.0\n",
      "1802 Train Loss 14.086582\n",
      "Loss  9.544089 C_bot  0.15 k_c 0.0\n",
      "Loss  14.909511 C_bot  0.15 k_c 0.0\n",
      "1803 Train Loss 19.418066\n",
      "Loss  14.909511 C_bot  0.15 k_c 0.0\n",
      "Loss  25.246874 C_bot  0.15 k_c 0.0\n",
      "1804 Train Loss 29.815815\n",
      "Loss  25.246874 C_bot  0.15 k_c 0.0\n",
      "Loss  43.345497 C_bot  0.15 k_c 0.0\n",
      "1805 Train Loss 47.855408\n",
      "Loss  43.345497 C_bot  0.15 k_c 0.0\n",
      "Loss  78.37629 C_bot  0.15 k_c 0.0\n",
      "1806 Train Loss 83.022514\n",
      "Loss  78.37629 C_bot  0.15 k_c 0.0\n",
      "Loss  131.38782 C_bot  0.15 k_c 0.0\n",
      "1807 Train Loss 135.9231\n",
      "Loss  131.38782 C_bot  0.15 k_c 0.0\n",
      "Loss  223.88124 C_bot  0.15 k_c 0.0\n",
      "1808 Train Loss 228.7408\n",
      "Loss  223.88124 C_bot  0.15 k_c 0.0\n",
      "Loss  307.62805 C_bot  0.15 k_c 0.0\n",
      "1809 Train Loss 312.21585\n",
      "Loss  307.62805 C_bot  0.15 k_c 0.0\n",
      "Loss  380.67752 C_bot  0.15 k_c 0.0\n",
      "1810 Train Loss 385.8992\n",
      "Loss  380.67752 C_bot  0.15 k_c 0.0\n",
      "Loss  307.05048 C_bot  0.15 k_c 0.0\n",
      "1811 Train Loss 311.64804\n",
      "Loss  307.05048 C_bot  0.15 k_c 0.0\n",
      "Loss  147.43893 C_bot  0.15 k_c 0.0\n",
      "1812 Train Loss 152.70181\n",
      "Loss  147.43893 C_bot  0.15 k_c 0.0\n",
      "Loss  17.720655 C_bot  0.15 k_c 0.0\n",
      "1813 Train Loss 22.653938\n",
      "Loss  17.720655 C_bot  0.15 k_c 0.0\n",
      "Loss  31.0066 C_bot  0.15 k_c 0.0\n",
      "1814 Train Loss 36.018692\n",
      "Loss  31.0066 C_bot  0.15 k_c 0.0\n",
      "Loss  132.03151 C_bot  0.15 k_c 0.0\n",
      "1815 Train Loss 137.747\n",
      "Loss  132.03151 C_bot  0.15 k_c 0.0\n",
      "Loss  169.33563 C_bot  0.15 k_c 0.0\n",
      "1816 Train Loss 174.35579\n",
      "Loss  169.33563 C_bot  0.15 k_c 0.0\n",
      "Loss  105.72761 C_bot  0.15 k_c 0.0\n",
      "1817 Train Loss 111.52384\n",
      "Loss  105.72761 C_bot  0.15 k_c 0.0\n",
      "Loss  18.54201 C_bot  0.15 k_c 0.0\n",
      "1818 Train Loss 23.845125\n",
      "Loss  18.54201 C_bot  0.15 k_c 0.0\n",
      "Loss  17.275906 C_bot  0.15 k_c 0.0\n",
      "1819 Train Loss 22.608175\n",
      "Loss  17.275906 C_bot  0.15 k_c 0.0\n",
      "Loss  77.98414 C_bot  0.15 k_c 0.0\n",
      "1820 Train Loss 83.77112\n",
      "Loss  77.98414 C_bot  0.15 k_c 0.0\n",
      "Loss  97.669464 C_bot  0.15 k_c 0.0\n",
      "1821 Train Loss 102.89834\n",
      "Loss  97.669464 C_bot  0.15 k_c 0.0\n",
      "Loss  53.257893 C_bot  0.15 k_c 0.0\n",
      "1822 Train Loss 58.983994\n",
      "Loss  53.257893 C_bot  0.15 k_c 0.0\n",
      "Loss  7.541798 C_bot  0.15 k_c 0.0\n",
      "1823 Train Loss 12.968492\n",
      "Loss  7.541798 C_bot  0.15 k_c 0.0\n",
      "Loss  19.421822 C_bot  0.15 k_c 0.0\n",
      "1824 Train Loss 24.781229\n",
      "Loss  19.421822 C_bot  0.15 k_c 0.0\n",
      "Loss  56.179073 C_bot  0.15 k_c 0.0\n",
      "1825 Train Loss 61.837986\n",
      "Loss  56.179073 C_bot  0.15 k_c 0.0\n",
      "Loss  54.895294 C_bot  0.15 k_c 0.0\n",
      "1826 Train Loss 60.142517\n",
      "Loss  54.895294 C_bot  0.15 k_c 0.0\n",
      "Loss  21.184198 C_bot  0.15 k_c 0.0\n",
      "1827 Train Loss 26.637995\n",
      "Loss  21.184198 C_bot  0.15 k_c 0.0\n",
      "Loss  4.362624 C_bot  0.15 k_c 0.0\n",
      "1828 Train Loss 9.673462\n",
      "Loss  4.362624 C_bot  0.15 k_c 0.0\n",
      "Loss  22.237865 C_bot  0.15 k_c 0.0\n",
      "1829 Train Loss 27.436644\n",
      "Loss  22.237865 C_bot  0.15 k_c 0.0\n",
      "Loss  39.042377 C_bot  0.15 k_c 0.0\n",
      "1830 Train Loss 44.407528\n",
      "Loss  39.042377 C_bot  0.15 k_c 0.0\n",
      "Loss  26.520586 C_bot  0.15 k_c 0.0\n",
      "1831 Train Loss 31.686434\n",
      "Loss  26.520586 C_bot  0.15 k_c 0.0\n",
      "Loss  6.607125 C_bot  0.15 k_c 0.0\n",
      "1832 Train Loss 11.843685\n",
      "Loss  6.607125 C_bot  0.15 k_c 0.0\n",
      "Loss  7.5751233 C_bot  0.15 k_c 0.0\n",
      "1833 Train Loss 12.80683\n",
      "Loss  7.5751233 C_bot  0.15 k_c 0.0\n",
      "Loss  21.837572 C_bot  0.15 k_c 0.0\n",
      "1834 Train Loss 26.999763\n",
      "Loss  21.837572 C_bot  0.15 k_c 0.0\n",
      "Loss  23.921125 C_bot  0.15 k_c 0.0\n",
      "1835 Train Loss 29.168957\n",
      "Loss  23.921125 C_bot  0.15 k_c 0.0\n",
      "Loss  11.043226 C_bot  0.15 k_c 0.0\n",
      "1836 Train Loss 16.193651\n",
      "Loss  11.043226 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0055137 C_bot  0.15 k_c 0.0\n",
      "1837 Train Loss 9.156753\n",
      "Loss  4.0055137 C_bot  0.15 k_c 0.0\n",
      "Loss  10.7605505 C_bot  0.15 k_c 0.0\n",
      "1838 Train Loss 15.91978\n",
      "Loss  10.7605505 C_bot  0.15 k_c 0.0\n",
      "Loss  17.188393 C_bot  0.15 k_c 0.0\n",
      "1839 Train Loss 22.278263\n",
      "Loss  17.188393 C_bot  0.15 k_c 0.0\n",
      "Loss  12.635886 C_bot  0.15 k_c 0.0\n",
      "1840 Train Loss 17.758106\n",
      "Loss  12.635886 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9408994 C_bot  0.15 k_c 0.0\n",
      "1841 Train Loss 10.00708\n",
      "Loss  4.9408994 C_bot  0.15 k_c 0.0\n",
      "Loss  5.460811 C_bot  0.15 k_c 0.0\n",
      "1842 Train Loss 10.508867\n",
      "Loss  5.460811 C_bot  0.15 k_c 0.0\n",
      "Loss  11.014299 C_bot  0.15 k_c 0.0\n",
      "1843 Train Loss 16.086412\n",
      "Loss  11.014299 C_bot  0.15 k_c 0.0\n",
      "Loss  11.4811945 C_bot  0.15 k_c 0.0\n",
      "1844 Train Loss 16.488089\n",
      "Loss  11.4811945 C_bot  0.15 k_c 0.0\n",
      "Loss  6.4535713 C_bot  0.15 k_c 0.0\n",
      "1845 Train Loss 11.486067\n",
      "Loss  6.4535713 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9307408 C_bot  0.15 k_c 0.0\n",
      "1846 Train Loss 8.93248\n",
      "Loss  3.9307408 C_bot  0.15 k_c 0.0\n",
      "Loss  6.7024612 C_bot  0.15 k_c 0.0\n",
      "1847 Train Loss 11.671447\n",
      "Loss  6.7024612 C_bot  0.15 k_c 0.0\n",
      "Loss  9.101459 C_bot  0.15 k_c 0.0\n",
      "1848 Train Loss 14.103719\n",
      "Loss  9.101459 C_bot  0.15 k_c 0.0\n",
      "Loss  7.1865363 C_bot  0.15 k_c 0.0\n",
      "1849 Train Loss 12.125642\n",
      "Loss  7.1865363 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2105317 C_bot  0.15 k_c 0.0\n",
      "1850 Train Loss 9.164029\n",
      "Loss  4.2105317 C_bot  0.15 k_c 0.0\n",
      "Loss  4.4900417 C_bot  0.15 k_c 0.0\n",
      "1851 Train Loss 9.434983\n",
      "Loss  4.4900417 C_bot  0.15 k_c 0.0\n",
      "Loss  6.677464 C_bot  0.15 k_c 0.0\n",
      "1852 Train Loss 11.582718\n",
      "Loss  6.677464 C_bot  0.15 k_c 0.0\n",
      "Loss  6.831402 C_bot  0.15 k_c 0.0\n",
      "1853 Train Loss 11.770251\n",
      "Loss  6.831402 C_bot  0.15 k_c 0.0\n",
      "Loss  4.899308 C_bot  0.15 k_c 0.0\n",
      "1854 Train Loss 9.792735\n",
      "Loss  4.899308 C_bot  0.15 k_c 0.0\n",
      "Loss  3.864809 C_bot  0.15 k_c 0.0\n",
      "1855 Train Loss 8.76041\n",
      "Loss  3.864809 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8852615 C_bot  0.15 k_c 0.0\n",
      "1856 Train Loss 9.786059\n",
      "Loss  4.8852615 C_bot  0.15 k_c 0.0\n",
      "Loss  5.898754 C_bot  0.15 k_c 0.0\n",
      "1857 Train Loss 10.759858\n",
      "Loss  5.898754 C_bot  0.15 k_c 0.0\n",
      "Loss  5.206837 C_bot  0.15 k_c 0.0\n",
      "1858 Train Loss 10.089374\n",
      "Loss  5.206837 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0312753 C_bot  0.15 k_c 0.0\n",
      "1859 Train Loss 8.883381\n",
      "Loss  4.0312753 C_bot  0.15 k_c 0.0\n",
      "Loss  4.006293 C_bot  0.15 k_c 0.0\n",
      "1860 Train Loss 8.848821\n",
      "Loss  4.006293 C_bot  0.15 k_c 0.0\n",
      "Loss  4.822268 C_bot  0.15 k_c 0.0\n",
      "1861 Train Loss 9.67163\n",
      "Loss  4.822268 C_bot  0.15 k_c 0.0\n",
      "Loss  5.049997 C_bot  0.15 k_c 0.0\n",
      "1862 Train Loss 9.867887\n",
      "Loss  5.049997 C_bot  0.15 k_c 0.0\n",
      "Loss  4.334441 C_bot  0.15 k_c 0.0\n",
      "1863 Train Loss 9.162955\n",
      "Loss  4.334441 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8138092 C_bot  0.15 k_c 0.0\n",
      "1864 Train Loss 8.626227\n",
      "Loss  3.8138092 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0956006 C_bot  0.15 k_c 0.0\n",
      "1865 Train Loss 8.8968315\n",
      "Loss  4.0956006 C_bot  0.15 k_c 0.0\n",
      "Loss  4.544474 C_bot  0.15 k_c 0.0\n",
      "1866 Train Loss 9.352508\n",
      "Loss  4.544474 C_bot  0.15 k_c 0.0\n",
      "Loss  4.447712 C_bot  0.15 k_c 0.0\n",
      "1867 Train Loss 9.233683\n",
      "Loss  4.447712 C_bot  0.15 k_c 0.0\n",
      "Loss  3.957804 C_bot  0.15 k_c 0.0\n",
      "1868 Train Loss 8.747578\n",
      "Loss  3.957804 C_bot  0.15 k_c 0.0\n",
      "Loss  3.79567 C_bot  0.15 k_c 0.0\n",
      "1869 Train Loss 8.5758915\n",
      "Loss  3.79567 C_bot  0.15 k_c 0.0\n",
      "Loss  4.070387 C_bot  0.15 k_c 0.0\n",
      "1870 Train Loss 8.838907\n",
      "Loss  4.070387 C_bot  0.15 k_c 0.0\n",
      "Loss  4.26467 C_bot  0.15 k_c 0.0\n",
      "1871 Train Loss 9.036586\n",
      "Loss  4.26467 C_bot  0.15 k_c 0.0\n",
      "Loss  4.102636 C_bot  0.15 k_c 0.0\n",
      "1872 Train Loss 8.858287\n",
      "Loss  4.102636 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8193252 C_bot  0.15 k_c 0.0\n",
      "1873 Train Loss 8.574683\n",
      "Loss  3.8193252 C_bot  0.15 k_c 0.0\n",
      "Loss  3.800381 C_bot  0.15 k_c 0.0\n",
      "1874 Train Loss 8.549538\n",
      "Loss  3.800381 C_bot  0.15 k_c 0.0\n",
      "Loss  3.990038 C_bot  0.15 k_c 0.0\n",
      "1875 Train Loss 8.728822\n",
      "Loss  3.990038 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0557623 C_bot  0.15 k_c 0.0\n",
      "1876 Train Loss 8.796193\n",
      "Loss  4.0557623 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9208567 C_bot  0.15 k_c 0.0\n",
      "1877 Train Loss 8.648533\n",
      "Loss  3.9208567 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7665873 C_bot  0.15 k_c 0.0\n",
      "1878 Train Loss 8.491856\n",
      "Loss  3.7665873 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7859929 C_bot  0.15 k_c 0.0\n",
      "1879 Train Loss 8.505725\n",
      "Loss  3.7859929 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9019165 C_bot  0.15 k_c 0.0\n",
      "1880 Train Loss 8.611442\n",
      "Loss  3.9019165 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9182587 C_bot  0.15 k_c 0.0\n",
      "1881 Train Loss 8.627529\n",
      "Loss  3.9182587 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8269708 C_bot  0.15 k_c 0.0\n",
      "1882 Train Loss 8.525158\n",
      "Loss  3.8269708 C_bot  0.15 k_c 0.0\n",
      "Loss  3.741885 C_bot  0.15 k_c 0.0\n",
      "1883 Train Loss 8.436852\n",
      "Loss  3.741885 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7616642 C_bot  0.15 k_c 0.0\n",
      "1884 Train Loss 8.451962\n",
      "Loss  3.7616642 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8299193 C_bot  0.15 k_c 0.0\n",
      "1885 Train Loss 8.511527\n",
      "Loss  3.8299193 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8314464 C_bot  0.15 k_c 0.0\n",
      "1886 Train Loss 8.512678\n",
      "Loss  3.8314464 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7751937 C_bot  0.15 k_c 0.0\n",
      "1887 Train Loss 8.447396\n",
      "Loss  3.7751937 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7242453 C_bot  0.15 k_c 0.0\n",
      "1888 Train Loss 8.393531\n",
      "Loss  3.7242453 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7353888 C_bot  0.15 k_c 0.0\n",
      "1889 Train Loss 8.400572\n",
      "Loss  3.7353888 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7756333 C_bot  0.15 k_c 0.0\n",
      "1890 Train Loss 8.433483\n",
      "Loss  3.7756333 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7759023 C_bot  0.15 k_c 0.0\n",
      "1891 Train Loss 8.433005\n",
      "Loss  3.7759023 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7435913 C_bot  0.15 k_c 0.0\n",
      "1892 Train Loss 8.393279\n",
      "Loss  3.7435913 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7101102 C_bot  0.15 k_c 0.0\n",
      "1893 Train Loss 8.357477\n",
      "Loss  3.7101102 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7127197 C_bot  0.15 k_c 0.0\n",
      "1894 Train Loss 8.356688\n",
      "Loss  3.7127197 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7354789 C_bot  0.15 k_c 0.0\n",
      "1895 Train Loss 8.374001\n",
      "Loss  3.7354789 C_bot  0.15 k_c 0.0\n",
      "Loss  3.737332 C_bot  0.15 k_c 0.0\n",
      "1896 Train Loss 8.375283\n",
      "Loss  3.737332 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7201471 C_bot  0.15 k_c 0.0\n",
      "1897 Train Loss 8.352328\n",
      "Loss  3.7201471 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6966136 C_bot  0.15 k_c 0.0\n",
      "1898 Train Loss 8.327148\n",
      "Loss  3.6966136 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6930075 C_bot  0.15 k_c 0.0\n",
      "1899 Train Loss 8.320424\n",
      "Loss  3.6930075 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7047226 C_bot  0.15 k_c 0.0\n",
      "1900 Train Loss 8.327914\n",
      "Loss  3.7047226 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7079604 C_bot  0.15 k_c 0.0\n",
      "1901 Train Loss 8.330236\n",
      "Loss  3.7079604 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7004898 C_bot  0.15 k_c 0.0\n",
      "1902 Train Loss 8.317909\n",
      "Loss  3.7004898 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6840398 C_bot  0.15 k_c 0.0\n",
      "1903 Train Loss 8.3001\n",
      "Loss  3.6840398 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6773283 C_bot  0.15 k_c 0.0\n",
      "1904 Train Loss 8.290334\n",
      "Loss  3.6773283 C_bot  0.15 k_c 0.0\n",
      "Loss  3.681101 C_bot  0.15 k_c 0.0\n",
      "1905 Train Loss 8.290886\n",
      "Loss  3.681101 C_bot  0.15 k_c 0.0\n",
      "Loss  3.683627 C_bot  0.15 k_c 0.0\n",
      "1906 Train Loss 8.292223\n",
      "Loss  3.683627 C_bot  0.15 k_c 0.0\n",
      "Loss  3.681749 C_bot  0.15 k_c 0.0\n",
      "1907 Train Loss 8.286263\n",
      "Loss  3.681749 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6709495 C_bot  0.15 k_c 0.0\n",
      "1908 Train Loss 8.274255\n",
      "Loss  3.6709495 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6637602 C_bot  0.15 k_c 0.0\n",
      "1909 Train Loss 8.263939\n",
      "Loss  3.6637602 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6624074 C_bot  0.15 k_c 0.0\n",
      "1910 Train Loss 8.259987\n",
      "Loss  3.6624074 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6631851 C_bot  0.15 k_c 0.0\n",
      "1911 Train Loss 8.259112\n",
      "Loss  3.6631851 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6637986 C_bot  0.15 k_c 0.0\n",
      "1912 Train Loss 8.256178\n",
      "Loss  3.6637986 C_bot  0.15 k_c 0.0\n",
      "Loss  3.657516 C_bot  0.15 k_c 0.0\n",
      "1913 Train Loss 8.248649\n",
      "Loss  3.657516 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6518261 C_bot  0.15 k_c 0.0\n",
      "1914 Train Loss 8.239854\n",
      "Loss  3.6518261 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6473393 C_bot  0.15 k_c 0.0\n",
      "1915 Train Loss 8.233326\n",
      "Loss  3.6473393 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6460383 C_bot  0.15 k_c 0.0\n",
      "1916 Train Loss 8.23009\n",
      "Loss  3.6460383 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6463735 C_bot  0.15 k_c 0.0\n",
      "1917 Train Loss 8.227485\n",
      "Loss  3.6463735 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6430209 C_bot  0.15 k_c 0.0\n",
      "1918 Train Loss 8.222815\n",
      "Loss  3.6430209 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6395173 C_bot  0.15 k_c 0.0\n",
      "1919 Train Loss 8.216327\n",
      "Loss  3.6395173 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6345346 C_bot  0.15 k_c 0.0\n",
      "1920 Train Loss 8.209657\n",
      "Loss  3.6345346 C_bot  0.15 k_c 0.0\n",
      "Loss  3.631641 C_bot  0.15 k_c 0.0\n",
      "1921 Train Loss 8.204526\n",
      "Loss  3.631641 C_bot  0.15 k_c 0.0\n",
      "Loss  3.630456 C_bot  0.15 k_c 0.0\n",
      "1922 Train Loss 8.200905\n",
      "Loss  3.630456 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6284642 C_bot  0.15 k_c 0.0\n",
      "1923 Train Loss 8.197388\n",
      "Loss  3.6284642 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6265638 C_bot  0.15 k_c 0.0\n",
      "1924 Train Loss 8.192752\n",
      "Loss  3.6265638 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6223998 C_bot  0.15 k_c 0.0\n",
      "1925 Train Loss 8.187118\n",
      "Loss  3.6223998 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6191678 C_bot  0.15 k_c 0.0\n",
      "1926 Train Loss 8.18153\n",
      "Loss  3.6191678 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6164482 C_bot  0.15 k_c 0.0\n",
      "1927 Train Loss 8.176855\n",
      "Loss  3.6164482 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6144283 C_bot  0.15 k_c 0.0\n",
      "1928 Train Loss 8.173087\n",
      "Loss  3.6144283 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6128938 C_bot  0.15 k_c 0.0\n",
      "1929 Train Loss 8.169159\n",
      "Loss  3.6128938 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6098914 C_bot  0.15 k_c 0.0\n",
      "1930 Train Loss 8.164688\n",
      "Loss  3.6098914 C_bot  0.15 k_c 0.0\n",
      "Loss  3.60736 C_bot  0.15 k_c 0.0\n",
      "1931 Train Loss 8.159786\n",
      "Loss  3.60736 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6040478 C_bot  0.15 k_c 0.0\n",
      "1932 Train Loss 8.154826\n",
      "Loss  3.6040478 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6015816 C_bot  0.15 k_c 0.0\n",
      "1933 Train Loss 8.150392\n",
      "Loss  3.6015816 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5993783 C_bot  0.15 k_c 0.0\n",
      "1934 Train Loss 8.146184\n",
      "Loss  3.5993783 C_bot  0.15 k_c 0.0\n",
      "Loss  3.597146 C_bot  0.15 k_c 0.0\n",
      "1935 Train Loss 8.142377\n",
      "Loss  3.597146 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5950565 C_bot  0.15 k_c 0.0\n",
      "1936 Train Loss 8.138109\n",
      "Loss  3.5950565 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5921576 C_bot  0.15 k_c 0.0\n",
      "1937 Train Loss 8.133724\n",
      "Loss  3.5921576 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5896776 C_bot  0.15 k_c 0.0\n",
      "1938 Train Loss 8.129202\n",
      "Loss  3.5896776 C_bot  0.15 k_c 0.0\n",
      "Loss  3.586944 C_bot  0.15 k_c 0.0\n",
      "1939 Train Loss 8.124786\n",
      "Loss  3.586944 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5846498 C_bot  0.15 k_c 0.0\n",
      "1940 Train Loss 8.120754\n",
      "Loss  3.5846498 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5825496 C_bot  0.15 k_c 0.0\n",
      "1941 Train Loss 8.116735\n",
      "Loss  3.5825496 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5800521 C_bot  0.15 k_c 0.0\n",
      "1942 Train Loss 8.112728\n",
      "Loss  3.5800521 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5777166 C_bot  0.15 k_c 0.0\n",
      "1943 Train Loss 8.108414\n",
      "Loss  3.5777166 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5750577 C_bot  0.15 k_c 0.0\n",
      "1944 Train Loss 8.104265\n",
      "Loss  3.5750577 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5727267 C_bot  0.15 k_c 0.0\n",
      "1945 Train Loss 8.100102\n",
      "Loss  3.5727267 C_bot  0.15 k_c 0.0\n",
      "Loss  3.570106 C_bot  0.15 k_c 0.0\n",
      "1946 Train Loss 8.095854\n",
      "Loss  3.570106 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5678396 C_bot  0.15 k_c 0.0\n",
      "1947 Train Loss 8.091979\n",
      "Loss  3.5678396 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5656722 C_bot  0.15 k_c 0.0\n",
      "1948 Train Loss 8.08804\n",
      "Loss  3.5656722 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5631652 C_bot  0.15 k_c 0.0\n",
      "1949 Train Loss 8.084069\n",
      "Loss  3.5631652 C_bot  0.15 k_c 0.0\n",
      "Loss  3.560963 C_bot  0.15 k_c 0.0\n",
      "1950 Train Loss 8.080069\n",
      "Loss  3.560963 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5583837 C_bot  0.15 k_c 0.0\n",
      "1951 Train Loss 8.076035\n",
      "Loss  3.5583837 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5560925 C_bot  0.15 k_c 0.0\n",
      "1952 Train Loss 8.072048\n",
      "Loss  3.5560925 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5536003 C_bot  0.15 k_c 0.0\n",
      "1953 Train Loss 8.068015\n",
      "Loss  3.5536003 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5514 C_bot  0.15 k_c 0.0\n",
      "1954 Train Loss 8.064279\n",
      "Loss  3.5514 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5490668 C_bot  0.15 k_c 0.0\n",
      "1955 Train Loss 8.060316\n",
      "Loss  3.5490668 C_bot  0.15 k_c 0.0\n",
      "Loss  3.546581 C_bot  0.15 k_c 0.0\n",
      "1956 Train Loss 8.056411\n",
      "Loss  3.546581 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5444992 C_bot  0.15 k_c 0.0\n",
      "1957 Train Loss 8.052682\n",
      "Loss  3.5444992 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5420942 C_bot  0.15 k_c 0.0\n",
      "1958 Train Loss 8.04888\n",
      "Loss  3.5420942 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5397453 C_bot  0.15 k_c 0.0\n",
      "1959 Train Loss 8.044945\n",
      "Loss  3.5397453 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5372283 C_bot  0.15 k_c 0.0\n",
      "1960 Train Loss 8.040985\n",
      "Loss  3.5372283 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5349827 C_bot  0.15 k_c 0.0\n",
      "1961 Train Loss 8.037257\n",
      "Loss  3.5349827 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5327291 C_bot  0.15 k_c 0.0\n",
      "1962 Train Loss 8.033502\n",
      "Loss  3.5327291 C_bot  0.15 k_c 0.0\n",
      "Loss  3.530422 C_bot  0.15 k_c 0.0\n",
      "1963 Train Loss 8.029803\n",
      "Loss  3.530422 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5282462 C_bot  0.15 k_c 0.0\n",
      "1964 Train Loss 8.026104\n",
      "Loss  3.5282462 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5258245 C_bot  0.15 k_c 0.0\n",
      "1965 Train Loss 8.022333\n",
      "Loss  3.5258245 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5236232 C_bot  0.15 k_c 0.0\n",
      "1966 Train Loss 8.018635\n",
      "Loss  3.5236232 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5212464 C_bot  0.15 k_c 0.0\n",
      "1967 Train Loss 8.014904\n",
      "Loss  3.5212464 C_bot  0.15 k_c 0.0\n",
      "Loss  3.519049 C_bot  0.15 k_c 0.0\n",
      "1968 Train Loss 8.011269\n",
      "Loss  3.519049 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5165904 C_bot  0.15 k_c 0.0\n",
      "1969 Train Loss 8.007426\n",
      "Loss  3.5165904 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5143275 C_bot  0.15 k_c 0.0\n",
      "1970 Train Loss 8.003793\n",
      "Loss  3.5143275 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5121331 C_bot  0.15 k_c 0.0\n",
      "1971 Train Loss 8.000191\n",
      "Loss  3.5121331 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5098383 C_bot  0.15 k_c 0.0\n",
      "1972 Train Loss 7.996577\n",
      "Loss  3.5098383 C_bot  0.15 k_c 0.0\n",
      "Loss  3.507753 C_bot  0.15 k_c 0.0\n",
      "1973 Train Loss 7.9930816\n",
      "Loss  3.507753 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5053182 C_bot  0.15 k_c 0.0\n",
      "1974 Train Loss 7.989352\n",
      "Loss  3.5053182 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5032363 C_bot  0.15 k_c 0.0\n",
      "1975 Train Loss 7.9858875\n",
      "Loss  3.5032363 C_bot  0.15 k_c 0.0\n",
      "Loss  3.500827 C_bot  0.15 k_c 0.0\n",
      "1976 Train Loss 7.982185\n",
      "Loss  3.500827 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4986758 C_bot  0.15 k_c 0.0\n",
      "1977 Train Loss 7.9786935\n",
      "Loss  3.4986758 C_bot  0.15 k_c 0.0\n",
      "Loss  3.496359 C_bot  0.15 k_c 0.0\n",
      "1978 Train Loss 7.975069\n",
      "Loss  3.496359 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4940264 C_bot  0.15 k_c 0.0\n",
      "1979 Train Loss 7.9714403\n",
      "Loss  3.4940264 C_bot  0.15 k_c 0.0\n",
      "Loss  3.491928 C_bot  0.15 k_c 0.0\n",
      "1980 Train Loss 7.9680257\n",
      "Loss  3.491928 C_bot  0.15 k_c 0.0\n",
      "Loss  3.489607 C_bot  0.15 k_c 0.0\n",
      "1981 Train Loss 7.9644446\n",
      "Loss  3.489607 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4874864 C_bot  0.15 k_c 0.0\n",
      "1982 Train Loss 7.961009\n",
      "Loss  3.4874864 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4852962 C_bot  0.15 k_c 0.0\n",
      "1983 Train Loss 7.9575825\n",
      "Loss  3.4852962 C_bot  0.15 k_c 0.0\n",
      "Loss  3.483048 C_bot  0.15 k_c 0.0\n",
      "1984 Train Loss 7.9540353\n",
      "Loss  3.483048 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4806242 C_bot  0.15 k_c 0.0\n",
      "1985 Train Loss 7.950385\n",
      "Loss  3.4806242 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4786315 C_bot  0.15 k_c 0.0\n",
      "1986 Train Loss 7.947119\n",
      "Loss  3.4786315 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4762883 C_bot  0.15 k_c 0.0\n",
      "1987 Train Loss 7.9435496\n",
      "Loss  3.4762883 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4741724 C_bot  0.15 k_c 0.0\n",
      "1988 Train Loss 7.9401913\n",
      "Loss  3.4741724 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4718966 C_bot  0.15 k_c 0.0\n",
      "1989 Train Loss 7.9366856\n",
      "Loss  3.4718966 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4697745 C_bot  0.15 k_c 0.0\n",
      "1990 Train Loss 7.933352\n",
      "Loss  3.4697745 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4676194 C_bot  0.15 k_c 0.0\n",
      "1991 Train Loss 7.9299664\n",
      "Loss  3.4676194 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4654486 C_bot  0.15 k_c 0.0\n",
      "1992 Train Loss 7.9266114\n",
      "Loss  3.4654486 C_bot  0.15 k_c 0.0\n",
      "Loss  3.463247 C_bot  0.15 k_c 0.0\n",
      "1993 Train Loss 7.9231844\n",
      "Loss  3.463247 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4609964 C_bot  0.15 k_c 0.0\n",
      "1994 Train Loss 7.9197683\n",
      "Loss  3.4609964 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4588964 C_bot  0.15 k_c 0.0\n",
      "1995 Train Loss 7.916455\n",
      "Loss  3.4588964 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4567282 C_bot  0.15 k_c 0.0\n",
      "1996 Train Loss 7.9131346\n",
      "Loss  3.4567282 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4544942 C_bot  0.15 k_c 0.0\n",
      "1997 Train Loss 7.9097023\n",
      "Loss  3.4544942 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4523556 C_bot  0.15 k_c 0.0\n",
      "1998 Train Loss 7.9064193\n",
      "Loss  3.4523556 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4502177 C_bot  0.15 k_c 0.0\n",
      "1999 Train Loss 7.903101\n",
      "Loss  3.4502177 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4480174 C_bot  0.15 k_c 0.0\n",
      "2000 Train Loss 7.8997645\n",
      "Loss  3.4480174 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4459765 C_bot  0.15 k_c 0.0\n",
      "2001 Train Loss 7.8965607\n",
      "Loss  3.4459765 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4438314 C_bot  0.15 k_c 0.0\n",
      "2002 Train Loss 7.893284\n",
      "Loss  3.4438314 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4415853 C_bot  0.15 k_c 0.0\n",
      "2003 Train Loss 7.8898964\n",
      "Loss  3.4415853 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4393957 C_bot  0.15 k_c 0.0\n",
      "2004 Train Loss 7.8865786\n",
      "Loss  3.4393957 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4373858 C_bot  0.15 k_c 0.0\n",
      "2005 Train Loss 7.88345\n",
      "Loss  3.4373858 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4351826 C_bot  0.15 k_c 0.0\n",
      "2006 Train Loss 7.8801193\n",
      "Loss  3.4351826 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4330328 C_bot  0.15 k_c 0.0\n",
      "2007 Train Loss 7.876871\n",
      "Loss  3.4330328 C_bot  0.15 k_c 0.0\n",
      "Loss  3.430857 C_bot  0.15 k_c 0.0\n",
      "2008 Train Loss 7.8735704\n",
      "Loss  3.430857 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4287486 C_bot  0.15 k_c 0.0\n",
      "2009 Train Loss 7.8703837\n",
      "Loss  3.4287486 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4265075 C_bot  0.15 k_c 0.0\n",
      "2010 Train Loss 7.8670197\n",
      "Loss  3.4265075 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4244232 C_bot  0.15 k_c 0.0\n",
      "2011 Train Loss 7.863877\n",
      "Loss  3.4244232 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4223166 C_bot  0.15 k_c 0.0\n",
      "2012 Train Loss 7.8606496\n",
      "Loss  3.4223166 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4200969 C_bot  0.15 k_c 0.0\n",
      "2013 Train Loss 7.8573904\n",
      "Loss  3.4200969 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4180813 C_bot  0.15 k_c 0.0\n",
      "2014 Train Loss 7.854256\n",
      "Loss  3.4180813 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4159245 C_bot  0.15 k_c 0.0\n",
      "2015 Train Loss 7.8510814\n",
      "Loss  3.4159245 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4139202 C_bot  0.15 k_c 0.0\n",
      "2016 Train Loss 7.8479576\n",
      "Loss  3.4139202 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4117458 C_bot  0.15 k_c 0.0\n",
      "2017 Train Loss 7.8447847\n",
      "Loss  3.4117458 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4096675 C_bot  0.15 k_c 0.0\n",
      "2018 Train Loss 7.8415866\n",
      "Loss  3.4096675 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4075165 C_bot  0.15 k_c 0.0\n",
      "2019 Train Loss 7.83846\n",
      "Loss  3.4075165 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4054148 C_bot  0.15 k_c 0.0\n",
      "2020 Train Loss 7.835234\n",
      "Loss  3.4054148 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4032054 C_bot  0.15 k_c 0.0\n",
      "2021 Train Loss 7.832075\n",
      "Loss  3.4032054 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4012856 C_bot  0.15 k_c 0.0\n",
      "2022 Train Loss 7.829021\n",
      "Loss  3.4012856 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3990932 C_bot  0.15 k_c 0.0\n",
      "2023 Train Loss 7.8259125\n",
      "Loss  3.3990932 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3970828 C_bot  0.15 k_c 0.0\n",
      "2024 Train Loss 7.8227468\n",
      "Loss  3.3970828 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3949523 C_bot  0.15 k_c 0.0\n",
      "2025 Train Loss 7.819748\n",
      "Loss  3.3949523 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3930333 C_bot  0.15 k_c 0.0\n",
      "2026 Train Loss 7.816635\n",
      "Loss  3.3930333 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3908873 C_bot  0.15 k_c 0.0\n",
      "2027 Train Loss 7.8136883\n",
      "Loss  3.3908873 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3892975 C_bot  0.15 k_c 0.0\n",
      "2028 Train Loss 7.810841\n",
      "Loss  3.3892975 C_bot  0.15 k_c 0.0\n",
      "Loss  3.387133 C_bot  0.15 k_c 0.0\n",
      "2029 Train Loss 7.807974\n",
      "Loss  3.387133 C_bot  0.15 k_c 0.0\n",
      "Loss  3.385692 C_bot  0.15 k_c 0.0\n",
      "2030 Train Loss 7.8051734\n",
      "Loss  3.385692 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3837674 C_bot  0.15 k_c 0.0\n",
      "2031 Train Loss 7.8026943\n",
      "Loss  3.3837674 C_bot  0.15 k_c 0.0\n",
      "Loss  3.383103 C_bot  0.15 k_c 0.0\n",
      "2032 Train Loss 7.8005066\n",
      "Loss  3.383103 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3817046 C_bot  0.15 k_c 0.0\n",
      "2033 Train Loss 7.798781\n",
      "Loss  3.3817046 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3825257 C_bot  0.15 k_c 0.0\n",
      "2034 Train Loss 7.797812\n",
      "Loss  3.3825257 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3827052 C_bot  0.15 k_c 0.0\n",
      "2035 Train Loss 7.798024\n",
      "Loss  3.3827052 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3872435 C_bot  0.15 k_c 0.0\n",
      "2036 Train Loss 7.800336\n",
      "Loss  3.3872435 C_bot  0.15 k_c 0.0\n",
      "Loss  3.392195 C_bot  0.15 k_c 0.0\n",
      "2037 Train Loss 7.8059063\n",
      "Loss  3.392195 C_bot  0.15 k_c 0.0\n",
      "Loss  3.406491 C_bot  0.15 k_c 0.0\n",
      "2038 Train Loss 7.8172474\n",
      "Loss  3.406491 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4256437 C_bot  0.15 k_c 0.0\n",
      "2039 Train Loss 7.8380084\n",
      "Loss  3.4256437 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4668672 C_bot  0.15 k_c 0.0\n",
      "2040 Train Loss 7.875038\n",
      "Loss  3.4668672 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5287764 C_bot  0.15 k_c 0.0\n",
      "2041 Train Loss 7.9402585\n",
      "Loss  3.5287764 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6487164 C_bot  0.15 k_c 0.0\n",
      "2042 Train Loss 8.053885\n",
      "Loss  3.6487164 C_bot  0.15 k_c 0.0\n",
      "Loss  3.842387 C_bot  0.15 k_c 0.0\n",
      "2043 Train Loss 8.253876\n",
      "Loss  3.842387 C_bot  0.15 k_c 0.0\n",
      "Loss  4.201934 C_bot  0.15 k_c 0.0\n",
      "2044 Train Loss 8.603451\n",
      "Loss  4.201934 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8149705 C_bot  0.15 k_c 0.0\n",
      "2045 Train Loss 9.228288\n",
      "Loss  4.8149705 C_bot  0.15 k_c 0.0\n",
      "Loss  5.9332166 C_bot  0.15 k_c 0.0\n",
      "2046 Train Loss 10.330242\n",
      "Loss  5.9332166 C_bot  0.15 k_c 0.0\n",
      "Loss  7.9220185 C_bot  0.15 k_c 0.0\n",
      "2047 Train Loss 12.341269\n",
      "Loss  7.9220185 C_bot  0.15 k_c 0.0\n",
      "Loss  11.515214 C_bot  0.15 k_c 0.0\n",
      "2048 Train Loss 15.907363\n",
      "Loss  11.515214 C_bot  0.15 k_c 0.0\n",
      "Loss  18.128202 C_bot  0.15 k_c 0.0\n",
      "2049 Train Loss 22.563663\n",
      "Loss  18.128202 C_bot  0.15 k_c 0.0\n",
      "Loss  29.874222 C_bot  0.15 k_c 0.0\n",
      "2050 Train Loss 34.26455\n",
      "Loss  29.874222 C_bot  0.15 k_c 0.0\n",
      "Loss  52.017235 C_bot  0.15 k_c 0.0\n",
      "2051 Train Loss 56.49748\n",
      "Loss  52.017235 C_bot  0.15 k_c 0.0\n",
      "Loss  89.35292 C_bot  0.15 k_c 0.0\n",
      "2052 Train Loss 93.75779\n",
      "Loss  89.35292 C_bot  0.15 k_c 0.0\n",
      "Loss  159.22878 C_bot  0.15 k_c 0.0\n",
      "2053 Train Loss 163.83704\n",
      "Loss  159.22878 C_bot  0.15 k_c 0.0\n",
      "Loss  258.7758 C_bot  0.15 k_c 0.0\n",
      "2054 Train Loss 263.24164\n",
      "Loss  258.7758 C_bot  0.15 k_c 0.0\n",
      "Loss  422.53494 C_bot  0.15 k_c 0.0\n",
      "2055 Train Loss 427.47217\n",
      "Loss  422.53494 C_bot  0.15 k_c 0.0\n",
      "Loss  544.46344 C_bot  0.15 k_c 0.0\n",
      "2056 Train Loss 549.0184\n",
      "Loss  544.46344 C_bot  0.15 k_c 0.0\n",
      "Loss  628.57404 C_bot  0.15 k_c 0.0\n",
      "2057 Train Loss 633.94476\n",
      "Loss  628.57404 C_bot  0.15 k_c 0.0\n",
      "Loss  441.4681 C_bot  0.15 k_c 0.0\n",
      "2058 Train Loss 445.9859\n",
      "Loss  441.4681 C_bot  0.15 k_c 0.0\n",
      "Loss  173.91463 C_bot  0.15 k_c 0.0\n",
      "2059 Train Loss 179.06694\n",
      "Loss  173.91463 C_bot  0.15 k_c 0.0\n",
      "Loss  7.7438374 C_bot  0.15 k_c 0.0\n",
      "2060 Train Loss 12.546846\n",
      "Loss  7.7438374 C_bot  0.15 k_c 0.0\n",
      "Loss  83.27448 C_bot  0.15 k_c 0.0\n",
      "2061 Train Loss 88.02325\n",
      "Loss  83.27448 C_bot  0.15 k_c 0.0\n",
      "Loss  246.21376 C_bot  0.15 k_c 0.0\n",
      "2062 Train Loss 251.87819\n",
      "Loss  246.21376 C_bot  0.15 k_c 0.0\n",
      "Loss  239.60208 C_bot  0.15 k_c 0.0\n",
      "2063 Train Loss 244.3951\n",
      "Loss  239.60208 C_bot  0.15 k_c 0.0\n",
      "Loss  97.91847 C_bot  0.15 k_c 0.0\n",
      "2064 Train Loss 103.45967\n",
      "Loss  97.91847 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0721774 C_bot  0.15 k_c 0.0\n",
      "2065 Train Loss 10.273419\n",
      "Loss  5.0721774 C_bot  0.15 k_c 0.0\n",
      "Loss  71.73921 C_bot  0.15 k_c 0.0\n",
      "2066 Train Loss 76.76064\n",
      "Loss  71.73921 C_bot  0.15 k_c 0.0\n",
      "Loss  157.84602 C_bot  0.15 k_c 0.0\n",
      "2067 Train Loss 163.6004\n",
      "Loss  157.84602 C_bot  0.15 k_c 0.0\n",
      "Loss  107.46757 C_bot  0.15 k_c 0.0\n",
      "2068 Train Loss 112.46868\n",
      "Loss  107.46757 C_bot  0.15 k_c 0.0\n",
      "Loss  17.16499 C_bot  0.15 k_c 0.0\n",
      "2069 Train Loss 22.508062\n",
      "Loss  17.16499 C_bot  0.15 k_c 0.0\n",
      "Loss  21.037731 C_bot  0.15 k_c 0.0\n",
      "2070 Train Loss 26.360878\n",
      "Loss  21.037731 C_bot  0.15 k_c 0.0\n",
      "Loss  83.43382 C_bot  0.15 k_c 0.0\n",
      "2071 Train Loss 88.39824\n",
      "Loss  83.43382 C_bot  0.15 k_c 0.0\n",
      "Loss  82.78433 C_bot  0.15 k_c 0.0\n",
      "2072 Train Loss 88.17568\n",
      "Loss  82.78433 C_bot  0.15 k_c 0.0\n",
      "Loss  20.716532 C_bot  0.15 k_c 0.0\n",
      "2073 Train Loss 25.692219\n",
      "Loss  20.716532 C_bot  0.15 k_c 0.0\n",
      "Loss  9.933468 C_bot  0.15 k_c 0.0\n",
      "2074 Train Loss 14.90344\n",
      "Loss  9.933468 C_bot  0.15 k_c 0.0\n",
      "Loss  51.68177 C_bot  0.15 k_c 0.0\n",
      "2075 Train Loss 56.841774\n",
      "Loss  51.68177 C_bot  0.15 k_c 0.0\n",
      "Loss  54.60407 C_bot  0.15 k_c 0.0\n",
      "2076 Train Loss 59.45332\n",
      "Loss  54.60407 C_bot  0.15 k_c 0.0\n",
      "Loss  16.213242 C_bot  0.15 k_c 0.0\n",
      "2077 Train Loss 21.20959\n",
      "Loss  16.213242 C_bot  0.15 k_c 0.0\n",
      "Loss  6.699785 C_bot  0.15 k_c 0.0\n",
      "2078 Train Loss 11.634182\n",
      "Loss  6.699785 C_bot  0.15 k_c 0.0\n",
      "Loss  33.359856 C_bot  0.15 k_c 0.0\n",
      "2079 Train Loss 38.193394\n",
      "Loss  33.359856 C_bot  0.15 k_c 0.0\n",
      "Loss  37.770824 C_bot  0.15 k_c 0.0\n",
      "2080 Train Loss 42.741604\n",
      "Loss  37.770824 C_bot  0.15 k_c 0.0\n",
      "Loss  11.950556 C_bot  0.15 k_c 0.0\n",
      "2081 Train Loss 16.78188\n",
      "Loss  11.950556 C_bot  0.15 k_c 0.0\n",
      "Loss  5.9997377 C_bot  0.15 k_c 0.0\n",
      "2082 Train Loss 10.828707\n",
      "Loss  5.9997377 C_bot  0.15 k_c 0.0\n",
      "Loss  23.884693 C_bot  0.15 k_c 0.0\n",
      "2083 Train Loss 28.768135\n",
      "Loss  23.884693 C_bot  0.15 k_c 0.0\n",
      "Loss  25.148596 C_bot  0.15 k_c 0.0\n",
      "2084 Train Loss 29.93451\n",
      "Loss  25.148596 C_bot  0.15 k_c 0.0\n",
      "Loss  8.439062 C_bot  0.15 k_c 0.0\n",
      "2085 Train Loss 13.2603245\n",
      "Loss  8.439062 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4630656 C_bot  0.15 k_c 0.0\n",
      "2086 Train Loss 10.261956\n",
      "Loss  5.4630656 C_bot  0.15 k_c 0.0\n",
      "Loss  17.2017 C_bot  0.15 k_c 0.0\n",
      "2087 Train Loss 21.953932\n",
      "Loss  17.2017 C_bot  0.15 k_c 0.0\n",
      "Loss  17.635792 C_bot  0.15 k_c 0.0\n",
      "2088 Train Loss 22.439407\n",
      "Loss  17.635792 C_bot  0.15 k_c 0.0\n",
      "Loss  6.408922 C_bot  0.15 k_c 0.0\n",
      "2089 Train Loss 11.144761\n",
      "Loss  6.408922 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1949897 C_bot  0.15 k_c 0.0\n",
      "2090 Train Loss 9.920543\n",
      "Loss  5.1949897 C_bot  0.15 k_c 0.0\n",
      "Loss  13.027392 C_bot  0.15 k_c 0.0\n",
      "2091 Train Loss 17.786854\n",
      "Loss  13.027392 C_bot  0.15 k_c 0.0\n",
      "Loss  12.4894705 C_bot  0.15 k_c 0.0\n",
      "2092 Train Loss 17.17782\n",
      "Loss  12.4894705 C_bot  0.15 k_c 0.0\n",
      "Loss  5.156734 C_bot  0.15 k_c 0.0\n",
      "2093 Train Loss 9.873293\n",
      "Loss  5.156734 C_bot  0.15 k_c 0.0\n",
      "Loss  4.863844 C_bot  0.15 k_c 0.0\n",
      "2094 Train Loss 9.57101\n",
      "Loss  4.863844 C_bot  0.15 k_c 0.0\n",
      "Loss  10.002786 C_bot  0.15 k_c 0.0\n",
      "2095 Train Loss 14.665598\n",
      "Loss  10.002786 C_bot  0.15 k_c 0.0\n",
      "Loss  9.367681 C_bot  0.15 k_c 0.0\n",
      "2096 Train Loss 14.07947\n",
      "Loss  9.367681 C_bot  0.15 k_c 0.0\n",
      "Loss  4.566207 C_bot  0.15 k_c 0.0\n",
      "2097 Train Loss 9.228587\n",
      "Loss  4.566207 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6013193 C_bot  0.15 k_c 0.0\n",
      "2098 Train Loss 9.255776\n",
      "Loss  4.6013193 C_bot  0.15 k_c 0.0\n",
      "Loss  7.946156 C_bot  0.15 k_c 0.0\n",
      "2099 Train Loss 12.630781\n",
      "Loss  7.946156 C_bot  0.15 k_c 0.0\n",
      "Loss  7.3851204 C_bot  0.15 k_c 0.0\n",
      "2100 Train Loss 12.013872\n",
      "Loss  7.3851204 C_bot  0.15 k_c 0.0\n",
      "Loss  4.204965 C_bot  0.15 k_c 0.0\n",
      "2101 Train Loss 8.854473\n",
      "Loss  4.204965 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2865868 C_bot  0.15 k_c 0.0\n",
      "2102 Train Loss 8.928887\n",
      "Loss  4.2865868 C_bot  0.15 k_c 0.0\n",
      "Loss  6.505395 C_bot  0.15 k_c 0.0\n",
      "2103 Train Loss 11.114497\n",
      "Loss  6.505395 C_bot  0.15 k_c 0.0\n",
      "Loss  6.104793 C_bot  0.15 k_c 0.0\n",
      "2104 Train Loss 10.741915\n",
      "Loss  6.104793 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0345197 C_bot  0.15 k_c 0.0\n",
      "2105 Train Loss 8.640336\n",
      "Loss  4.0345197 C_bot  0.15 k_c 0.0\n",
      "Loss  4.049252 C_bot  0.15 k_c 0.0\n",
      "2106 Train Loss 8.648382\n",
      "Loss  4.049252 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4864984 C_bot  0.15 k_c 0.0\n",
      "2107 Train Loss 10.099428\n",
      "Loss  5.4864984 C_bot  0.15 k_c 0.0\n",
      "Loss  5.3087826 C_bot  0.15 k_c 0.0\n",
      "2108 Train Loss 9.890036\n",
      "Loss  5.3087826 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9174016 C_bot  0.15 k_c 0.0\n",
      "2109 Train Loss 8.508524\n",
      "Loss  3.9174016 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8467662 C_bot  0.15 k_c 0.0\n",
      "2110 Train Loss 8.431533\n",
      "Loss  3.8467662 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8138194 C_bot  0.15 k_c 0.0\n",
      "2111 Train Loss 9.381218\n",
      "Loss  4.8138194 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7647514 C_bot  0.15 k_c 0.0\n",
      "2112 Train Loss 9.345229\n",
      "Loss  4.7647514 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8684423 C_bot  0.15 k_c 0.0\n",
      "2113 Train Loss 8.431703\n",
      "Loss  3.8684423 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7249 C_bot  0.15 k_c 0.0\n",
      "2114 Train Loss 8.285594\n",
      "Loss  3.7249 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3380036 C_bot  0.15 k_c 0.0\n",
      "2115 Train Loss 8.903958\n",
      "Loss  4.3380036 C_bot  0.15 k_c 0.0\n",
      "Loss  4.414022 C_bot  0.15 k_c 0.0\n",
      "2116 Train Loss 8.963694\n",
      "Loss  4.414022 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8218591 C_bot  0.15 k_c 0.0\n",
      "2117 Train Loss 8.377254\n",
      "Loss  3.8218591 C_bot  0.15 k_c 0.0\n",
      "Loss  3.641078 C_bot  0.15 k_c 0.0\n",
      "2118 Train Loss 8.19084\n",
      "Loss  3.641078 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0318456 C_bot  0.15 k_c 0.0\n",
      "2119 Train Loss 8.572804\n",
      "Loss  4.0318456 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1460886 C_bot  0.15 k_c 0.0\n",
      "2120 Train Loss 8.693409\n",
      "Loss  4.1460886 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7949507 C_bot  0.15 k_c 0.0\n",
      "2121 Train Loss 8.3307295\n",
      "Loss  3.7949507 C_bot  0.15 k_c 0.0\n",
      "Loss  3.598757 C_bot  0.15 k_c 0.0\n",
      "2122 Train Loss 8.13396\n",
      "Loss  3.598757 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8134775 C_bot  0.15 k_c 0.0\n",
      "2123 Train Loss 8.349585\n",
      "Loss  3.8134775 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9604878 C_bot  0.15 k_c 0.0\n",
      "2124 Train Loss 8.486279\n",
      "Loss  3.9604878 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7547429 C_bot  0.15 k_c 0.0\n",
      "2125 Train Loss 8.284909\n",
      "Loss  3.7547429 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5788085 C_bot  0.15 k_c 0.0\n",
      "2126 Train Loss 8.102954\n",
      "Loss  3.5788085 C_bot  0.15 k_c 0.0\n",
      "Loss  3.681961 C_bot  0.15 k_c 0.0\n",
      "2127 Train Loss 8.201498\n",
      "Loss  3.681961 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8059638 C_bot  0.15 k_c 0.0\n",
      "2128 Train Loss 8.329041\n",
      "Loss  3.8059638 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7226572 C_bot  0.15 k_c 0.0\n",
      "2129 Train Loss 8.236688\n",
      "Loss  3.7226572 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5725055 C_bot  0.15 k_c 0.0\n",
      "2130 Train Loss 8.087746\n",
      "Loss  3.5725055 C_bot  0.15 k_c 0.0\n",
      "Loss  3.594691 C_bot  0.15 k_c 0.0\n",
      "2131 Train Loss 8.108216\n",
      "Loss  3.594691 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6997712 C_bot  0.15 k_c 0.0\n",
      "2132 Train Loss 8.2067375\n",
      "Loss  3.6997712 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6744864 C_bot  0.15 k_c 0.0\n",
      "2133 Train Loss 8.18468\n",
      "Loss  3.6744864 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5733998 C_bot  0.15 k_c 0.0\n",
      "2134 Train Loss 8.077408\n",
      "Loss  3.5733998 C_bot  0.15 k_c 0.0\n",
      "Loss  3.549473 C_bot  0.15 k_c 0.0\n",
      "2135 Train Loss 8.051869\n",
      "Loss  3.549473 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6100001 C_bot  0.15 k_c 0.0\n",
      "2136 Train Loss 8.112984\n",
      "Loss  3.6100001 C_bot  0.15 k_c 0.0\n",
      "Loss  3.631616 C_bot  0.15 k_c 0.0\n",
      "2137 Train Loss 8.128219\n",
      "Loss  3.631616 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5658095 C_bot  0.15 k_c 0.0\n",
      "2138 Train Loss 8.063979\n",
      "Loss  3.5658095 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5245707 C_bot  0.15 k_c 0.0\n",
      "2139 Train Loss 8.019211\n",
      "Loss  3.5245707 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5525954 C_bot  0.15 k_c 0.0\n",
      "2140 Train Loss 8.043976\n",
      "Loss  3.5525954 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5784411 C_bot  0.15 k_c 0.0\n",
      "2141 Train Loss 8.070862\n",
      "Loss  3.5784411 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5558493 C_bot  0.15 k_c 0.0\n",
      "2142 Train Loss 8.043087\n",
      "Loss  3.5558493 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5123084 C_bot  0.15 k_c 0.0\n",
      "2143 Train Loss 7.9995008\n",
      "Loss  3.5123084 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5109577 C_bot  0.15 k_c 0.0\n",
      "2144 Train Loss 7.99639\n",
      "Loss  3.5109577 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5359569 C_bot  0.15 k_c 0.0\n",
      "2145 Train Loss 8.017716\n",
      "Loss  3.5359569 C_bot  0.15 k_c 0.0\n",
      "Loss  3.531472 C_bot  0.15 k_c 0.0\n",
      "2146 Train Loss 8.013963\n",
      "Loss  3.531472 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5049734 C_bot  0.15 k_c 0.0\n",
      "2147 Train Loss 7.983619\n",
      "Loss  3.5049734 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4878378 C_bot  0.15 k_c 0.0\n",
      "2148 Train Loss 7.9653516\n",
      "Loss  3.4878378 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4962478 C_bot  0.15 k_c 0.0\n",
      "2149 Train Loss 7.972824\n",
      "Loss  3.4962478 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5060494 C_bot  0.15 k_c 0.0\n",
      "2150 Train Loss 7.9790764\n",
      "Loss  3.5060494 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4909816 C_bot  0.15 k_c 0.0\n",
      "2151 Train Loss 7.9641275\n",
      "Loss  3.4909816 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4731817 C_bot  0.15 k_c 0.0\n",
      "2152 Train Loss 7.943469\n",
      "Loss  3.4731817 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4692676 C_bot  0.15 k_c 0.0\n",
      "2153 Train Loss 7.93783\n",
      "Loss  3.4692676 C_bot  0.15 k_c 0.0\n",
      "Loss  3.474438 C_bot  0.15 k_c 0.0\n",
      "2154 Train Loss 7.9423556\n",
      "Loss  3.474438 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4738953 C_bot  0.15 k_c 0.0\n",
      "2155 Train Loss 7.9386463\n",
      "Loss  3.4738953 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4592962 C_bot  0.15 k_c 0.0\n",
      "2156 Train Loss 7.923664\n",
      "Loss  3.4592962 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4489117 C_bot  0.15 k_c 0.0\n",
      "2157 Train Loss 7.9110794\n",
      "Loss  3.4489117 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4485123 C_bot  0.15 k_c 0.0\n",
      "2158 Train Loss 7.9086957\n",
      "Loss  3.4485123 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4487395 C_bot  0.15 k_c 0.0\n",
      "2159 Train Loss 7.908298\n",
      "Loss  3.4487395 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4438872 C_bot  0.15 k_c 0.0\n",
      "2160 Train Loss 7.900648\n",
      "Loss  3.4438872 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4320974 C_bot  0.15 k_c 0.0\n",
      "2161 Train Loss 7.88811\n",
      "Loss  3.4320974 C_bot  0.15 k_c 0.0\n",
      "Loss  3.42563 C_bot  0.15 k_c 0.0\n",
      "2162 Train Loss 7.8798437\n",
      "Loss  3.42563 C_bot  0.15 k_c 0.0\n",
      "Loss  3.424627 C_bot  0.15 k_c 0.0\n",
      "2163 Train Loss 7.87683\n",
      "Loss  3.424627 C_bot  0.15 k_c 0.0\n",
      "Loss  3.421445 C_bot  0.15 k_c 0.0\n",
      "2164 Train Loss 7.8729734\n",
      "Loss  3.421445 C_bot  0.15 k_c 0.0\n",
      "Loss  3.415264 C_bot  0.15 k_c 0.0\n",
      "2165 Train Loss 7.8643684\n",
      "Loss  3.415264 C_bot  0.15 k_c 0.0\n",
      "Loss  3.40611 C_bot  0.15 k_c 0.0\n",
      "2166 Train Loss 7.8542724\n",
      "Loss  3.40611 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4005282 C_bot  0.15 k_c 0.0\n",
      "2167 Train Loss 7.847148\n",
      "Loss  3.4005282 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3978968 C_bot  0.15 k_c 0.0\n",
      "2168 Train Loss 7.8426094\n",
      "Loss  3.3978968 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3928545 C_bot  0.15 k_c 0.0\n",
      "2169 Train Loss 7.836815\n",
      "Loss  3.3928545 C_bot  0.15 k_c 0.0\n",
      "Loss  3.386475 C_bot  0.15 k_c 0.0\n",
      "2170 Train Loss 7.828332\n",
      "Loss  3.386475 C_bot  0.15 k_c 0.0\n",
      "Loss  3.378415 C_bot  0.15 k_c 0.0\n",
      "2171 Train Loss 7.819252\n",
      "Loss  3.378415 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3727665 C_bot  0.15 k_c 0.0\n",
      "2172 Train Loss 7.8122315\n",
      "Loss  3.3727665 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3687272 C_bot  0.15 k_c 0.0\n",
      "2173 Train Loss 7.8064785\n",
      "Loss  3.3687272 C_bot  0.15 k_c 0.0\n",
      "Loss  3.362707 C_bot  0.15 k_c 0.0\n",
      "2174 Train Loss 7.799673\n",
      "Loss  3.362707 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3561792 C_bot  0.15 k_c 0.0\n",
      "2175 Train Loss 7.791342\n",
      "Loss  3.3561792 C_bot  0.15 k_c 0.0\n",
      "Loss  3.348645 C_bot  0.15 k_c 0.0\n",
      "2176 Train Loss 7.782819\n",
      "Loss  3.348645 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3424103 C_bot  0.15 k_c 0.0\n",
      "2177 Train Loss 7.775347\n",
      "Loss  3.3424103 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3372476 C_bot  0.15 k_c 0.0\n",
      "2178 Train Loss 7.7686887\n",
      "Loss  3.3372476 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3307471 C_bot  0.15 k_c 0.0\n",
      "2179 Train Loss 7.761404\n",
      "Loss  3.3307471 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3241093 C_bot  0.15 k_c 0.0\n",
      "2180 Train Loss 7.7532077\n",
      "Loss  3.3241093 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3166945 C_bot  0.15 k_c 0.0\n",
      "2181 Train Loss 7.744881\n",
      "Loss  3.3166945 C_bot  0.15 k_c 0.0\n",
      "Loss  3.310015 C_bot  0.15 k_c 0.0\n",
      "2182 Train Loss 7.737068\n",
      "Loss  3.310015 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3041327 C_bot  0.15 k_c 0.0\n",
      "2183 Train Loss 7.729904\n",
      "Loss  3.3041327 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2974882 C_bot  0.15 k_c 0.0\n",
      "2184 Train Loss 7.722485\n",
      "Loss  3.2974882 C_bot  0.15 k_c 0.0\n",
      "Loss  3.290893 C_bot  0.15 k_c 0.0\n",
      "2185 Train Loss 7.7145257\n",
      "Loss  3.290893 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2837193 C_bot  0.15 k_c 0.0\n",
      "2186 Train Loss 7.7065153\n",
      "Loss  3.2837193 C_bot  0.15 k_c 0.0\n",
      "Loss  3.277011 C_bot  0.15 k_c 0.0\n",
      "2187 Train Loss 7.69872\n",
      "Loss  3.277011 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2708006 C_bot  0.15 k_c 0.0\n",
      "2188 Train Loss 7.6913853\n",
      "Loss  3.2708006 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2642984 C_bot  0.15 k_c 0.0\n",
      "2189 Train Loss 7.684074\n",
      "Loss  3.2642984 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2580547 C_bot  0.15 k_c 0.0\n",
      "2190 Train Loss 7.67658\n",
      "Loss  3.2580547 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2512453 C_bot  0.15 k_c 0.0\n",
      "2191 Train Loss 7.668952\n",
      "Loss  3.2512453 C_bot  0.15 k_c 0.0\n",
      "Loss  3.244963 C_bot  0.15 k_c 0.0\n",
      "2192 Train Loss 7.6615696\n",
      "Loss  3.244963 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2389283 C_bot  0.15 k_c 0.0\n",
      "2193 Train Loss 7.6544967\n",
      "Loss  3.2389283 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2330818 C_bot  0.15 k_c 0.0\n",
      "2194 Train Loss 7.6477585\n",
      "Loss  3.2330818 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2274725 C_bot  0.15 k_c 0.0\n",
      "2195 Train Loss 7.64095\n",
      "Loss  3.2274725 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2214987 C_bot  0.15 k_c 0.0\n",
      "2196 Train Loss 7.634118\n",
      "Loss  3.2214987 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2158918 C_bot  0.15 k_c 0.0\n",
      "2197 Train Loss 7.627352\n",
      "Loss  3.2158918 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2103834 C_bot  0.15 k_c 0.0\n",
      "2198 Train Loss 7.6208324\n",
      "Loss  3.2103834 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2052228 C_bot  0.15 k_c 0.0\n",
      "2199 Train Loss 7.614659\n",
      "Loss  3.2052228 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2002423 C_bot  0.15 k_c 0.0\n",
      "2200 Train Loss 7.608499\n",
      "Loss  3.2002423 C_bot  0.15 k_c 0.0\n",
      "Loss  3.195179 C_bot  0.15 k_c 0.0\n",
      "2201 Train Loss 7.6024904\n",
      "Loss  3.195179 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1903467 C_bot  0.15 k_c 0.0\n",
      "2202 Train Loss 7.596445\n",
      "Loss  3.1903467 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1854498 C_bot  0.15 k_c 0.0\n",
      "2203 Train Loss 7.5905294\n",
      "Loss  3.1854498 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1808586 C_bot  0.15 k_c 0.0\n",
      "2204 Train Loss 7.5848064\n",
      "Loss  3.1808586 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1763456 C_bot  0.15 k_c 0.0\n",
      "2205 Train Loss 7.579138\n",
      "Loss  3.1763456 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1718638 C_bot  0.15 k_c 0.0\n",
      "2206 Train Loss 7.5736103\n",
      "Loss  3.1718638 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1677444 C_bot  0.15 k_c 0.0\n",
      "2207 Train Loss 7.5682564\n",
      "Loss  3.1677444 C_bot  0.15 k_c 0.0\n",
      "Loss  3.163346 C_bot  0.15 k_c 0.0\n",
      "2208 Train Loss 7.562811\n",
      "Loss  3.163346 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1591084 C_bot  0.15 k_c 0.0\n",
      "2209 Train Loss 7.557361\n",
      "Loss  3.1591084 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1549692 C_bot  0.15 k_c 0.0\n",
      "2210 Train Loss 7.5520964\n",
      "Loss  3.1549692 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1507769 C_bot  0.15 k_c 0.0\n",
      "2211 Train Loss 7.546765\n",
      "Loss  3.1507769 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1467614 C_bot  0.15 k_c 0.0\n",
      "2212 Train Loss 7.541543\n",
      "Loss  3.1467614 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1426938 C_bot  0.15 k_c 0.0\n",
      "2213 Train Loss 7.5363846\n",
      "Loss  3.1426938 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1386995 C_bot  0.15 k_c 0.0\n",
      "2214 Train Loss 7.531155\n",
      "Loss  3.1386995 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1346421 C_bot  0.15 k_c 0.0\n",
      "2215 Train Loss 7.5259953\n",
      "Loss  3.1346421 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1306548 C_bot  0.15 k_c 0.0\n",
      "2216 Train Loss 7.520807\n",
      "Loss  3.1306548 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1266346 C_bot  0.15 k_c 0.0\n",
      "2217 Train Loss 7.51563\n",
      "Loss  3.1266346 C_bot  0.15 k_c 0.0\n",
      "Loss  3.122619 C_bot  0.15 k_c 0.0\n",
      "2218 Train Loss 7.510469\n",
      "Loss  3.122619 C_bot  0.15 k_c 0.0\n",
      "Loss  3.118594 C_bot  0.15 k_c 0.0\n",
      "2219 Train Loss 7.5052376\n",
      "Loss  3.118594 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1146398 C_bot  0.15 k_c 0.0\n",
      "2220 Train Loss 7.5001717\n",
      "Loss  3.1146398 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1106706 C_bot  0.15 k_c 0.0\n",
      "2221 Train Loss 7.4949846\n",
      "Loss  3.1106706 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1066089 C_bot  0.15 k_c 0.0\n",
      "2222 Train Loss 7.4898043\n",
      "Loss  3.1066089 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1026518 C_bot  0.15 k_c 0.0\n",
      "2223 Train Loss 7.4846544\n",
      "Loss  3.1026518 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0986876 C_bot  0.15 k_c 0.0\n",
      "2224 Train Loss 7.479538\n",
      "Loss  3.0986876 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0946765 C_bot  0.15 k_c 0.0\n",
      "2225 Train Loss 7.4743733\n",
      "Loss  3.0946765 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0907347 C_bot  0.15 k_c 0.0\n",
      "2226 Train Loss 7.4692464\n",
      "Loss  3.0907347 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0867589 C_bot  0.15 k_c 0.0\n",
      "2227 Train Loss 7.464142\n",
      "Loss  3.0867589 C_bot  0.15 k_c 0.0\n",
      "Loss  3.082761 C_bot  0.15 k_c 0.0\n",
      "2228 Train Loss 7.4589486\n",
      "Loss  3.082761 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0788934 C_bot  0.15 k_c 0.0\n",
      "2229 Train Loss 7.4539547\n",
      "Loss  3.0788934 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0748315 C_bot  0.15 k_c 0.0\n",
      "2230 Train Loss 7.4487095\n",
      "Loss  3.0748315 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0709143 C_bot  0.15 k_c 0.0\n",
      "2231 Train Loss 7.443649\n",
      "Loss  3.0709143 C_bot  0.15 k_c 0.0\n",
      "Loss  3.066947 C_bot  0.15 k_c 0.0\n",
      "2232 Train Loss 7.4385223\n",
      "Loss  3.066947 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0629892 C_bot  0.15 k_c 0.0\n",
      "2233 Train Loss 7.433401\n",
      "Loss  3.0629892 C_bot  0.15 k_c 0.0\n",
      "Loss  3.059143 C_bot  0.15 k_c 0.0\n",
      "2234 Train Loss 7.4284167\n",
      "Loss  3.059143 C_bot  0.15 k_c 0.0\n",
      "Loss  3.055279 C_bot  0.15 k_c 0.0\n",
      "2235 Train Loss 7.423379\n",
      "Loss  3.055279 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0513346 C_bot  0.15 k_c 0.0\n",
      "2236 Train Loss 7.4183035\n",
      "Loss  3.0513346 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0475109 C_bot  0.15 k_c 0.0\n",
      "2237 Train Loss 7.4133067\n",
      "Loss  3.0475109 C_bot  0.15 k_c 0.0\n",
      "Loss  3.043571 C_bot  0.15 k_c 0.0\n",
      "2238 Train Loss 7.408234\n",
      "Loss  3.043571 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0396485 C_bot  0.15 k_c 0.0\n",
      "2239 Train Loss 7.4031477\n",
      "Loss  3.0396485 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0358953 C_bot  0.15 k_c 0.0\n",
      "2240 Train Loss 7.3982525\n",
      "Loss  3.0358953 C_bot  0.15 k_c 0.0\n",
      "Loss  3.032016 C_bot  0.15 k_c 0.0\n",
      "2241 Train Loss 7.393223\n",
      "Loss  3.032016 C_bot  0.15 k_c 0.0\n",
      "Loss  3.028189 C_bot  0.15 k_c 0.0\n",
      "2242 Train Loss 7.388243\n",
      "Loss  3.028189 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0243125 C_bot  0.15 k_c 0.0\n",
      "2243 Train Loss 7.3832283\n",
      "Loss  3.0243125 C_bot  0.15 k_c 0.0\n",
      "Loss  3.020493 C_bot  0.15 k_c 0.0\n",
      "2244 Train Loss 7.3782473\n",
      "Loss  3.020493 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0167637 C_bot  0.15 k_c 0.0\n",
      "2245 Train Loss 7.3733854\n",
      "Loss  3.0167637 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0130262 C_bot  0.15 k_c 0.0\n",
      "2246 Train Loss 7.368482\n",
      "Loss  3.0130262 C_bot  0.15 k_c 0.0\n",
      "Loss  3.009139 C_bot  0.15 k_c 0.0\n",
      "2247 Train Loss 7.3634615\n",
      "Loss  3.009139 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0054228 C_bot  0.15 k_c 0.0\n",
      "2248 Train Loss 7.3585815\n",
      "Loss  3.0054228 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0016463 C_bot  0.15 k_c 0.0\n",
      "2249 Train Loss 7.353664\n",
      "Loss  3.0016463 C_bot  0.15 k_c 0.0\n",
      "Loss  2.997875 C_bot  0.15 k_c 0.0\n",
      "2250 Train Loss 7.348732\n",
      "Loss  2.997875 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9940932 C_bot  0.15 k_c 0.0\n",
      "2251 Train Loss 7.3438005\n",
      "Loss  2.9940932 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9903603 C_bot  0.15 k_c 0.0\n",
      "2252 Train Loss 7.338908\n",
      "Loss  2.9903603 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9867268 C_bot  0.15 k_c 0.0\n",
      "2253 Train Loss 7.334114\n",
      "Loss  2.9867268 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9829164 C_bot  0.15 k_c 0.0\n",
      "2254 Train Loss 7.329144\n",
      "Loss  2.9829164 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9792023 C_bot  0.15 k_c 0.0\n",
      "2255 Train Loss 7.324259\n",
      "Loss  2.9792023 C_bot  0.15 k_c 0.0\n",
      "Loss  2.975625 C_bot  0.15 k_c 0.0\n",
      "2256 Train Loss 7.3195186\n",
      "Loss  2.975625 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9718342 C_bot  0.15 k_c 0.0\n",
      "2257 Train Loss 7.314549\n",
      "Loss  2.9718342 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9681206 C_bot  0.15 k_c 0.0\n",
      "2258 Train Loss 7.309664\n",
      "Loss  2.9681206 C_bot  0.15 k_c 0.0\n",
      "Loss  2.964559 C_bot  0.15 k_c 0.0\n",
      "2259 Train Loss 7.3049173\n",
      "Loss  2.964559 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9609864 C_bot  0.15 k_c 0.0\n",
      "2260 Train Loss 7.300164\n",
      "Loss  2.9609864 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9572582 C_bot  0.15 k_c 0.0\n",
      "2261 Train Loss 7.295245\n",
      "Loss  2.9572582 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9535456 C_bot  0.15 k_c 0.0\n",
      "2262 Train Loss 7.290342\n",
      "Loss  2.9535456 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9500859 C_bot  0.15 k_c 0.0\n",
      "2263 Train Loss 7.285689\n",
      "Loss  2.9500859 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9463396 C_bot  0.15 k_c 0.0\n",
      "2264 Train Loss 7.2807407\n",
      "Loss  2.9463396 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9428182 C_bot  0.15 k_c 0.0\n",
      "2265 Train Loss 7.2760253\n",
      "Loss  2.9428182 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9392917 C_bot  0.15 k_c 0.0\n",
      "2266 Train Loss 7.271291\n",
      "Loss  2.9392917 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9356415 C_bot  0.15 k_c 0.0\n",
      "2267 Train Loss 7.266446\n",
      "Loss  2.9356415 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9321764 C_bot  0.15 k_c 0.0\n",
      "2268 Train Loss 7.261771\n",
      "Loss  2.9321764 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9285984 C_bot  0.15 k_c 0.0\n",
      "2269 Train Loss 7.2570014\n",
      "Loss  2.9285984 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9251041 C_bot  0.15 k_c 0.0\n",
      "2270 Train Loss 7.2523007\n",
      "Loss  2.9251041 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9215698 C_bot  0.15 k_c 0.0\n",
      "2271 Train Loss 7.247583\n",
      "Loss  2.9215698 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9180868 C_bot  0.15 k_c 0.0\n",
      "2272 Train Loss 7.2429023\n",
      "Loss  2.9180868 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9145477 C_bot  0.15 k_c 0.0\n",
      "2273 Train Loss 7.2381935\n",
      "Loss  2.9145477 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9110854 C_bot  0.15 k_c 0.0\n",
      "2274 Train Loss 7.233548\n",
      "Loss  2.9110854 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9076807 C_bot  0.15 k_c 0.0\n",
      "2275 Train Loss 7.228997\n",
      "Loss  2.9076807 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9042323 C_bot  0.15 k_c 0.0\n",
      "2276 Train Loss 7.2243834\n",
      "Loss  2.9042323 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9008641 C_bot  0.15 k_c 0.0\n",
      "2277 Train Loss 7.219902\n",
      "Loss  2.9008641 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8974757 C_bot  0.15 k_c 0.0\n",
      "2278 Train Loss 7.215369\n",
      "Loss  2.8974757 C_bot  0.15 k_c 0.0\n",
      "Loss  2.89411 C_bot  0.15 k_c 0.0\n",
      "2279 Train Loss 7.210934\n",
      "Loss  2.89411 C_bot  0.15 k_c 0.0\n",
      "Loss  2.890774 C_bot  0.15 k_c 0.0\n",
      "2280 Train Loss 7.206476\n",
      "Loss  2.890774 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8872917 C_bot  0.15 k_c 0.0\n",
      "2281 Train Loss 7.201975\n",
      "Loss  2.8872917 C_bot  0.15 k_c 0.0\n",
      "Loss  2.884109 C_bot  0.15 k_c 0.0\n",
      "2282 Train Loss 7.197698\n",
      "Loss  2.884109 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8807993 C_bot  0.15 k_c 0.0\n",
      "2283 Train Loss 7.1934223\n",
      "Loss  2.8807993 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8775384 C_bot  0.15 k_c 0.0\n",
      "2284 Train Loss 7.189096\n",
      "Loss  2.8775384 C_bot  0.15 k_c 0.0\n",
      "Loss  2.87425 C_bot  0.15 k_c 0.0\n",
      "2285 Train Loss 7.184901\n",
      "Loss  2.87425 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8711798 C_bot  0.15 k_c 0.0\n",
      "2286 Train Loss 7.180785\n",
      "Loss  2.8711798 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8678946 C_bot  0.15 k_c 0.0\n",
      "2287 Train Loss 7.176664\n",
      "Loss  2.8678946 C_bot  0.15 k_c 0.0\n",
      "Loss  2.864972 C_bot  0.15 k_c 0.0\n",
      "2288 Train Loss 7.172702\n",
      "Loss  2.864972 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8616874 C_bot  0.15 k_c 0.0\n",
      "2289 Train Loss 7.168664\n",
      "Loss  2.8616874 C_bot  0.15 k_c 0.0\n",
      "Loss  2.85887 C_bot  0.15 k_c 0.0\n",
      "2290 Train Loss 7.16479\n",
      "Loss  2.85887 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8555954 C_bot  0.15 k_c 0.0\n",
      "2291 Train Loss 7.1608667\n",
      "Loss  2.8555954 C_bot  0.15 k_c 0.0\n",
      "Loss  2.853166 C_bot  0.15 k_c 0.0\n",
      "2292 Train Loss 7.1573277\n",
      "Loss  2.853166 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8501823 C_bot  0.15 k_c 0.0\n",
      "2293 Train Loss 7.1538377\n",
      "Loss  2.8501823 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8481007 C_bot  0.15 k_c 0.0\n",
      "2294 Train Loss 7.1505322\n",
      "Loss  2.8481007 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8457243 C_bot  0.15 k_c 0.0\n",
      "2295 Train Loss 7.1478596\n",
      "Loss  2.8457243 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8451636 C_bot  0.15 k_c 0.0\n",
      "2296 Train Loss 7.145862\n",
      "Loss  2.8451636 C_bot  0.15 k_c 0.0\n",
      "Loss  2.844123 C_bot  0.15 k_c 0.0\n",
      "2297 Train Loss 7.144857\n",
      "Loss  2.844123 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8469422 C_bot  0.15 k_c 0.0\n",
      "2298 Train Loss 7.145858\n",
      "Loss  2.8469422 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8506923 C_bot  0.15 k_c 0.0\n",
      "2299 Train Loss 7.1501956\n",
      "Loss  2.8506923 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8630786 C_bot  0.15 k_c 0.0\n",
      "2300 Train Loss 7.160084\n",
      "Loss  2.8630786 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8815494 C_bot  0.15 k_c 0.0\n",
      "2301 Train Loss 7.180099\n",
      "Loss  2.8815494 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9232008 C_bot  0.15 k_c 0.0\n",
      "2302 Train Loss 7.2180433\n",
      "Loss  2.9232008 C_bot  0.15 k_c 0.0\n",
      "Loss  2.990406 C_bot  0.15 k_c 0.0\n",
      "2303 Train Loss 7.288522\n",
      "Loss  2.990406 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1251388 C_bot  0.15 k_c 0.0\n",
      "2304 Train Loss 7.417369\n",
      "Loss  3.1251388 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3570745 C_bot  0.15 k_c 0.0\n",
      "2305 Train Loss 7.6557984\n",
      "Loss  3.3570745 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8023884 C_bot  0.15 k_c 0.0\n",
      "2306 Train Loss 8.091303\n",
      "Loss  3.8023884 C_bot  0.15 k_c 0.0\n",
      "Loss  4.609008 C_bot  0.15 k_c 0.0\n",
      "2307 Train Loss 8.910662\n",
      "Loss  4.609008 C_bot  0.15 k_c 0.0\n",
      "Loss  6.131412 C_bot  0.15 k_c 0.0\n",
      "2308 Train Loss 10.416216\n",
      "Loss  6.131412 C_bot  0.15 k_c 0.0\n",
      "Loss  9.004231 C_bot  0.15 k_c 0.0\n",
      "2309 Train Loss 13.314611\n",
      "Loss  9.004231 C_bot  0.15 k_c 0.0\n",
      "Loss  14.33695 C_bot  0.15 k_c 0.0\n",
      "2310 Train Loss 18.618055\n",
      "Loss  14.33695 C_bot  0.15 k_c 0.0\n",
      "Loss  24.715502 C_bot  0.15 k_c 0.0\n",
      "2311 Train Loss 29.051008\n",
      "Loss  24.715502 C_bot  0.15 k_c 0.0\n",
      "Loss  43.160606 C_bot  0.15 k_c 0.0\n",
      "2312 Train Loss 47.444958\n",
      "Loss  43.160606 C_bot  0.15 k_c 0.0\n",
      "Loss  79.23428 C_bot  0.15 k_c 0.0\n",
      "2313 Train Loss 83.645874\n",
      "Loss  79.23428 C_bot  0.15 k_c 0.0\n",
      "Loss  134.88686 C_bot  0.15 k_c 0.0\n",
      "2314 Train Loss 139.2005\n",
      "Loss  134.88686 C_bot  0.15 k_c 0.0\n",
      "Loss  233.08778 C_bot  0.15 k_c 0.0\n",
      "2315 Train Loss 237.72089\n",
      "Loss  233.08778 C_bot  0.15 k_c 0.0\n",
      "Loss  324.09778 C_bot  0.15 k_c 0.0\n",
      "2316 Train Loss 328.47366\n",
      "Loss  324.09778 C_bot  0.15 k_c 0.0\n",
      "Loss  406.2699 C_bot  0.15 k_c 0.0\n",
      "2317 Train Loss 411.29953\n",
      "Loss  406.2699 C_bot  0.15 k_c 0.0\n",
      "Loss  329.88776 C_bot  0.15 k_c 0.0\n",
      "2318 Train Loss 334.29446\n",
      "Loss  329.88776 C_bot  0.15 k_c 0.0\n",
      "Loss  164.30098 C_bot  0.15 k_c 0.0\n",
      "2319 Train Loss 169.3722\n",
      "Loss  164.30098 C_bot  0.15 k_c 0.0\n",
      "Loss  21.45961 C_bot  0.15 k_c 0.0\n",
      "2320 Train Loss 26.167665\n",
      "Loss  21.45961 C_bot  0.15 k_c 0.0\n",
      "Loss  27.313993 C_bot  0.15 k_c 0.0\n",
      "2321 Train Loss 32.10783\n",
      "Loss  27.313993 C_bot  0.15 k_c 0.0\n",
      "Loss  133.9885 C_bot  0.15 k_c 0.0\n",
      "2322 Train Loss 139.41508\n",
      "Loss  133.9885 C_bot  0.15 k_c 0.0\n",
      "Loss  187.36758 C_bot  0.15 k_c 0.0\n",
      "2323 Train Loss 192.1548\n",
      "Loss  187.36758 C_bot  0.15 k_c 0.0\n",
      "Loss  137.04791 C_bot  0.15 k_c 0.0\n",
      "2324 Train Loss 142.58046\n",
      "Loss  137.04791 C_bot  0.15 k_c 0.0\n",
      "Loss  35.009045 C_bot  0.15 k_c 0.0\n",
      "2325 Train Loss 39.997078\n",
      "Loss  35.009045 C_bot  0.15 k_c 0.0\n",
      "Loss  7.992103 C_bot  0.15 k_c 0.0\n",
      "2326 Train Loss 13.0963545\n",
      "Loss  7.992103 C_bot  0.15 k_c 0.0\n",
      "Loss  63.61705 C_bot  0.15 k_c 0.0\n",
      "2327 Train Loss 69.05034\n",
      "Loss  63.61705 C_bot  0.15 k_c 0.0\n",
      "Loss  106.0509 C_bot  0.15 k_c 0.0\n",
      "2328 Train Loss 111.03162\n",
      "Loss  106.0509 C_bot  0.15 k_c 0.0\n",
      "Loss  80.7151 C_bot  0.15 k_c 0.0\n",
      "2329 Train Loss 86.17421\n",
      "Loss  80.7151 C_bot  0.15 k_c 0.0\n",
      "Loss  20.53496 C_bot  0.15 k_c 0.0\n",
      "2330 Train Loss 25.596329\n",
      "Loss  20.53496 C_bot  0.15 k_c 0.0\n",
      "Loss  7.4658213 C_bot  0.15 k_c 0.0\n",
      "2331 Train Loss 12.535586\n",
      "Loss  7.4658213 C_bot  0.15 k_c 0.0\n",
      "Loss  43.3563 C_bot  0.15 k_c 0.0\n",
      "2332 Train Loss 48.579403\n",
      "Loss  43.3563 C_bot  0.15 k_c 0.0\n",
      "Loss  62.500576 C_bot  0.15 k_c 0.0\n",
      "2333 Train Loss 67.397255\n",
      "Loss  62.500576 C_bot  0.15 k_c 0.0\n",
      "Loss  37.7461 C_bot  0.15 k_c 0.0\n",
      "2334 Train Loss 42.84323\n",
      "Loss  37.7461 C_bot  0.15 k_c 0.0\n",
      "Loss  6.550136 C_bot  0.15 k_c 0.0\n",
      "2335 Train Loss 11.462288\n",
      "Loss  6.550136 C_bot  0.15 k_c 0.0\n",
      "Loss  12.114762 C_bot  0.15 k_c 0.0\n",
      "2336 Train Loss 16.999466\n",
      "Loss  12.114762 C_bot  0.15 k_c 0.0\n",
      "Loss  35.775944 C_bot  0.15 k_c 0.0\n",
      "2337 Train Loss 40.788544\n",
      "Loss  35.775944 C_bot  0.15 k_c 0.0\n",
      "Loss  35.119335 C_bot  0.15 k_c 0.0\n",
      "2338 Train Loss 39.984642\n",
      "Loss  35.119335 C_bot  0.15 k_c 0.0\n",
      "Loss  13.478291 C_bot  0.15 k_c 0.0\n",
      "2339 Train Loss 18.42069\n",
      "Loss  13.478291 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9791005 C_bot  0.15 k_c 0.0\n",
      "2340 Train Loss 8.870374\n",
      "Loss  3.9791005 C_bot  0.15 k_c 0.0\n",
      "Loss  16.433949 C_bot  0.15 k_c 0.0\n",
      "2341 Train Loss 21.27625\n",
      "Loss  16.433949 C_bot  0.15 k_c 0.0\n",
      "Loss  25.999115 C_bot  0.15 k_c 0.0\n",
      "2342 Train Loss 30.897028\n",
      "Loss  25.999115 C_bot  0.15 k_c 0.0\n",
      "Loss  16.490618 C_bot  0.15 k_c 0.0\n",
      "2343 Train Loss 21.290882\n",
      "Loss  16.490618 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5418153 C_bot  0.15 k_c 0.0\n",
      "2344 Train Loss 9.351293\n",
      "Loss  4.5418153 C_bot  0.15 k_c 0.0\n",
      "Loss  7.11338 C_bot  0.15 k_c 0.0\n",
      "2345 Train Loss 11.915436\n",
      "Loss  7.11338 C_bot  0.15 k_c 0.0\n",
      "Loss  16.091444 C_bot  0.15 k_c 0.0\n",
      "2346 Train Loss 20.83859\n",
      "Loss  16.091444 C_bot  0.15 k_c 0.0\n",
      "Loss  15.4217205 C_bot  0.15 k_c 0.0\n",
      "2347 Train Loss 20.213095\n",
      "Loss  15.4217205 C_bot  0.15 k_c 0.0\n",
      "Loss  6.7237477 C_bot  0.15 k_c 0.0\n",
      "2348 Train Loss 11.4438095\n",
      "Loss  6.7237477 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9025 C_bot  0.15 k_c 0.0\n",
      "2349 Train Loss 8.613216\n",
      "Loss  3.9025 C_bot  0.15 k_c 0.0\n",
      "Loss  9.208744 C_bot  0.15 k_c 0.0\n",
      "2350 Train Loss 13.934346\n",
      "Loss  9.208744 C_bot  0.15 k_c 0.0\n",
      "Loss  12.164101 C_bot  0.15 k_c 0.0\n",
      "2351 Train Loss 16.816435\n",
      "Loss  12.164101 C_bot  0.15 k_c 0.0\n",
      "Loss  7.988904 C_bot  0.15 k_c 0.0\n",
      "2352 Train Loss 12.676704\n",
      "Loss  7.988904 C_bot  0.15 k_c 0.0\n",
      "Loss  3.720908 C_bot  0.15 k_c 0.0\n",
      "2353 Train Loss 8.361664\n",
      "Loss  3.720908 C_bot  0.15 k_c 0.0\n",
      "Loss  5.281537 C_bot  0.15 k_c 0.0\n",
      "2354 Train Loss 9.898355\n",
      "Loss  5.281537 C_bot  0.15 k_c 0.0\n",
      "Loss  8.665836 C_bot  0.15 k_c 0.0\n",
      "2355 Train Loss 13.313612\n",
      "Loss  8.665836 C_bot  0.15 k_c 0.0\n",
      "Loss  7.953904 C_bot  0.15 k_c 0.0\n",
      "2356 Train Loss 12.536146\n",
      "Loss  7.953904 C_bot  0.15 k_c 0.0\n",
      "Loss  4.525872 C_bot  0.15 k_c 0.0\n",
      "2357 Train Loss 9.129498\n",
      "Loss  4.525872 C_bot  0.15 k_c 0.0\n",
      "Loss  3.670493 C_bot  0.15 k_c 0.0\n",
      "2358 Train Loss 8.253181\n",
      "Loss  3.670493 C_bot  0.15 k_c 0.0\n",
      "Loss  5.8277936 C_bot  0.15 k_c 0.0\n",
      "2359 Train Loss 10.377012\n",
      "Loss  5.8277936 C_bot  0.15 k_c 0.0\n",
      "Loss  6.8818073 C_bot  0.15 k_c 0.0\n",
      "2360 Train Loss 11.456827\n",
      "Loss  6.8818073 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2282834 C_bot  0.15 k_c 0.0\n",
      "2361 Train Loss 9.75698\n",
      "Loss  5.2282834 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5306313 C_bot  0.15 k_c 0.0\n",
      "2362 Train Loss 8.064576\n",
      "Loss  3.5306313 C_bot  0.15 k_c 0.0\n",
      "Loss  4.076734 C_bot  0.15 k_c 0.0\n",
      "2363 Train Loss 8.606982\n",
      "Loss  4.076734 C_bot  0.15 k_c 0.0\n",
      "Loss  5.436138 C_bot  0.15 k_c 0.0\n",
      "2364 Train Loss 9.939144\n",
      "Loss  5.436138 C_bot  0.15 k_c 0.0\n",
      "Loss  5.26125 C_bot  0.15 k_c 0.0\n",
      "2365 Train Loss 9.781773\n",
      "Loss  5.26125 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9558268 C_bot  0.15 k_c 0.0\n",
      "2366 Train Loss 8.451361\n",
      "Loss  3.9558268 C_bot  0.15 k_c 0.0\n",
      "Loss  3.445378 C_bot  0.15 k_c 0.0\n",
      "2367 Train Loss 7.939741\n",
      "Loss  3.445378 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1798534 C_bot  0.15 k_c 0.0\n",
      "2368 Train Loss 8.677549\n",
      "Loss  4.1798534 C_bot  0.15 k_c 0.0\n",
      "Loss  4.760143 C_bot  0.15 k_c 0.0\n",
      "2369 Train Loss 9.237978\n",
      "Loss  4.760143 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2583466 C_bot  0.15 k_c 0.0\n",
      "2370 Train Loss 8.746579\n",
      "Loss  4.2583466 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5099583 C_bot  0.15 k_c 0.0\n",
      "2371 Train Loss 7.98435\n",
      "Loss  3.5099583 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5115535 C_bot  0.15 k_c 0.0\n",
      "2372 Train Loss 7.9818335\n",
      "Loss  3.5115535 C_bot  0.15 k_c 0.0\n",
      "Loss  4.036287 C_bot  0.15 k_c 0.0\n",
      "2373 Train Loss 8.510752\n",
      "Loss  4.036287 C_bot  0.15 k_c 0.0\n",
      "Loss  4.1925573 C_bot  0.15 k_c 0.0\n",
      "2374 Train Loss 8.651997\n",
      "Loss  4.1925573 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7488341 C_bot  0.15 k_c 0.0\n",
      "2375 Train Loss 8.214408\n",
      "Loss  3.7488341 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3837793 C_bot  0.15 k_c 0.0\n",
      "2376 Train Loss 7.840712\n",
      "Loss  3.3837793 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5197017 C_bot  0.15 k_c 0.0\n",
      "2377 Train Loss 7.9712863\n",
      "Loss  3.5197017 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8228056 C_bot  0.15 k_c 0.0\n",
      "2378 Train Loss 8.278276\n",
      "Loss  3.8228056 C_bot  0.15 k_c 0.0\n",
      "Loss  3.818724 C_bot  0.15 k_c 0.0\n",
      "2379 Train Loss 8.261263\n",
      "Loss  3.818724 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5133183 C_bot  0.15 k_c 0.0\n",
      "2380 Train Loss 7.9596014\n",
      "Loss  3.5133183 C_bot  0.15 k_c 0.0\n",
      "Loss  3.347188 C_bot  0.15 k_c 0.0\n",
      "2381 Train Loss 7.787057\n",
      "Loss  3.347188 C_bot  0.15 k_c 0.0\n",
      "Loss  3.47595 C_bot  0.15 k_c 0.0\n",
      "2382 Train Loss 7.9100723\n",
      "Loss  3.47595 C_bot  0.15 k_c 0.0\n",
      "Loss  3.637357 C_bot  0.15 k_c 0.0\n",
      "2383 Train Loss 8.075165\n",
      "Loss  3.637357 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5972998 C_bot  0.15 k_c 0.0\n",
      "2384 Train Loss 8.024336\n",
      "Loss  3.5972998 C_bot  0.15 k_c 0.0\n",
      "Loss  3.405985 C_bot  0.15 k_c 0.0\n",
      "2385 Train Loss 7.835959\n",
      "Loss  3.405985 C_bot  0.15 k_c 0.0\n",
      "Loss  3.326968 C_bot  0.15 k_c 0.0\n",
      "2386 Train Loss 7.75221\n",
      "Loss  3.326968 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4158955 C_bot  0.15 k_c 0.0\n",
      "2387 Train Loss 7.8358045\n",
      "Loss  3.4158955 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5023077 C_bot  0.15 k_c 0.0\n",
      "2388 Train Loss 7.9249806\n",
      "Loss  3.5023077 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4698515 C_bot  0.15 k_c 0.0\n",
      "2389 Train Loss 7.8833976\n",
      "Loss  3.4698515 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3533883 C_bot  0.15 k_c 0.0\n",
      "2390 Train Loss 7.7685685\n",
      "Loss  3.3533883 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3081937 C_bot  0.15 k_c 0.0\n",
      "2391 Train Loss 7.7192955\n",
      "Loss  3.3081937 C_bot  0.15 k_c 0.0\n",
      "Loss  3.360417 C_bot  0.15 k_c 0.0\n",
      "2392 Train Loss 7.767042\n",
      "Loss  3.360417 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4099128 C_bot  0.15 k_c 0.0\n",
      "2393 Train Loss 7.8182526\n",
      "Loss  3.4099128 C_bot  0.15 k_c 0.0\n",
      "Loss  3.392562 C_bot  0.15 k_c 0.0\n",
      "2394 Train Loss 7.7938657\n",
      "Loss  3.392562 C_bot  0.15 k_c 0.0\n",
      "Loss  3.321679 C_bot  0.15 k_c 0.0\n",
      "2395 Train Loss 7.7239976\n",
      "Loss  3.321679 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2894633 C_bot  0.15 k_c 0.0\n",
      "2396 Train Loss 7.688242\n",
      "Loss  3.2894633 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3152323 C_bot  0.15 k_c 0.0\n",
      "2397 Train Loss 7.710493\n",
      "Loss  3.3152323 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3454275 C_bot  0.15 k_c 0.0\n",
      "2398 Train Loss 7.741301\n",
      "Loss  3.3454275 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3411238 C_bot  0.15 k_c 0.0\n",
      "2399 Train Loss 7.731383\n",
      "Loss  3.3411238 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2988422 C_bot  0.15 k_c 0.0\n",
      "2400 Train Loss 7.6896057\n",
      "Loss  3.2988422 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2731726 C_bot  0.15 k_c 0.0\n",
      "2401 Train Loss 7.66068\n",
      "Loss  3.2731726 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2813118 C_bot  0.15 k_c 0.0\n",
      "2402 Train Loss 7.6662707\n",
      "Loss  3.2813118 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2994323 C_bot  0.15 k_c 0.0\n",
      "2403 Train Loss 7.684356\n",
      "Loss  3.2994323 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3029177 C_bot  0.15 k_c 0.0\n",
      "2404 Train Loss 7.6834116\n",
      "Loss  3.3029177 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2792323 C_bot  0.15 k_c 0.0\n",
      "2405 Train Loss 7.6599813\n",
      "Loss  3.2792323 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2592406 C_bot  0.15 k_c 0.0\n",
      "2406 Train Loss 7.63679\n",
      "Loss  3.2592406 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2563725 C_bot  0.15 k_c 0.0\n",
      "2407 Train Loss 7.631976\n",
      "Loss  3.2563725 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2653284 C_bot  0.15 k_c 0.0\n",
      "2408 Train Loss 7.6401706\n",
      "Loss  3.2653284 C_bot  0.15 k_c 0.0\n",
      "Loss  3.272021 C_bot  0.15 k_c 0.0\n",
      "2409 Train Loss 7.6432076\n",
      "Loss  3.272021 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2606838 C_bot  0.15 k_c 0.0\n",
      "2410 Train Loss 7.631868\n",
      "Loss  3.2606838 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2467408 C_bot  0.15 k_c 0.0\n",
      "2411 Train Loss 7.614828\n",
      "Loss  3.2467408 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2383733 C_bot  0.15 k_c 0.0\n",
      "2412 Train Loss 7.605122\n",
      "Loss  3.2383733 C_bot  0.15 k_c 0.0\n",
      "Loss  3.240102 C_bot  0.15 k_c 0.0\n",
      "2413 Train Loss 7.6056023\n",
      "Loss  3.240102 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2454462 C_bot  0.15 k_c 0.0\n",
      "2414 Train Loss 7.608028\n",
      "Loss  3.2454462 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2415614 C_bot  0.15 k_c 0.0\n",
      "2415 Train Loss 7.6038537\n",
      "Loss  3.2415614 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2339675 C_bot  0.15 k_c 0.0\n",
      "2416 Train Loss 7.593252\n",
      "Loss  3.2339675 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2246358 C_bot  0.15 k_c 0.0\n",
      "2417 Train Loss 7.582931\n",
      "Loss  3.2246358 C_bot  0.15 k_c 0.0\n",
      "Loss  3.221486 C_bot  0.15 k_c 0.0\n",
      "2418 Train Loss 7.5780463\n",
      "Loss  3.221486 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2233393 C_bot  0.15 k_c 0.0\n",
      "2419 Train Loss 7.577629\n",
      "Loss  3.2233393 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2225673 C_bot  0.15 k_c 0.0\n",
      "2420 Train Loss 7.5762196\n",
      "Loss  3.2225673 C_bot  0.15 k_c 0.0\n",
      "Loss  3.219842 C_bot  0.15 k_c 0.0\n",
      "2421 Train Loss 7.570768\n",
      "Loss  3.219842 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2125335 C_bot  0.15 k_c 0.0\n",
      "2422 Train Loss 7.5627046\n",
      "Loss  3.2125335 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2075891 C_bot  0.15 k_c 0.0\n",
      "2423 Train Loss 7.555713\n",
      "Loss  3.2075891 C_bot  0.15 k_c 0.0\n",
      "Loss  3.205512 C_bot  0.15 k_c 0.0\n",
      "2424 Train Loss 7.5519404\n",
      "Loss  3.205512 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2046366 C_bot  0.15 k_c 0.0\n",
      "2425 Train Loss 7.5499787\n",
      "Loss  3.2046366 C_bot  0.15 k_c 0.0\n",
      "Loss  3.204178 C_bot  0.15 k_c 0.0\n",
      "2426 Train Loss 7.5471563\n",
      "Loss  3.204178 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1999152 C_bot  0.15 k_c 0.0\n",
      "2427 Train Loss 7.5421114\n",
      "Loss  3.1999152 C_bot  0.15 k_c 0.0\n",
      "Loss  3.195794 C_bot  0.15 k_c 0.0\n",
      "2428 Train Loss 7.535827\n",
      "Loss  3.195794 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1916738 C_bot  0.15 k_c 0.0\n",
      "2429 Train Loss 7.530462\n",
      "Loss  3.1916738 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1893263 C_bot  0.15 k_c 0.0\n",
      "2430 Train Loss 7.526673\n",
      "Loss  3.1893263 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1884496 C_bot  0.15 k_c 0.0\n",
      "2431 Train Loss 7.523922\n",
      "Loss  3.1884496 C_bot  0.15 k_c 0.0\n",
      "Loss  3.186079 C_bot  0.15 k_c 0.0\n",
      "2432 Train Loss 7.520618\n",
      "Loss  3.186079 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1837335 C_bot  0.15 k_c 0.0\n",
      "2433 Train Loss 7.5162125\n",
      "Loss  3.1837335 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1797988 C_bot  0.15 k_c 0.0\n",
      "2434 Train Loss 7.5112696\n",
      "Loss  3.1797988 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1765919 C_bot  0.15 k_c 0.0\n",
      "2435 Train Loss 7.50636\n",
      "Loss  3.1765919 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1740973 C_bot  0.15 k_c 0.0\n",
      "2436 Train Loss 7.502415\n",
      "Loss  3.1740973 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1721027 C_bot  0.15 k_c 0.0\n",
      "2437 Train Loss 7.4992323\n",
      "Loss  3.1721027 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1704352 C_bot  0.15 k_c 0.0\n",
      "2438 Train Loss 7.4957767\n",
      "Loss  3.1704352 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1675553 C_bot  0.15 k_c 0.0\n",
      "2439 Train Loss 7.491934\n",
      "Loss  3.1675553 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1650288 C_bot  0.15 k_c 0.0\n",
      "2440 Train Loss 7.487654\n",
      "Loss  3.1650288 C_bot  0.15 k_c 0.0\n",
      "Loss  3.161734 C_bot  0.15 k_c 0.0\n",
      "2441 Train Loss 7.4832306\n",
      "Loss  3.161734 C_bot  0.15 k_c 0.0\n",
      "Loss  3.15934 C_bot  0.15 k_c 0.0\n",
      "2442 Train Loss 7.479407\n",
      "Loss  3.15934 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1571395 C_bot  0.15 k_c 0.0\n",
      "2443 Train Loss 7.475752\n",
      "Loss  3.1571395 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1550417 C_bot  0.15 k_c 0.0\n",
      "2444 Train Loss 7.4725494\n",
      "Loss  3.1550417 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1530707 C_bot  0.15 k_c 0.0\n",
      "2445 Train Loss 7.4689455\n",
      "Loss  3.1530707 C_bot  0.15 k_c 0.0\n",
      "Loss  3.150216 C_bot  0.15 k_c 0.0\n",
      "2446 Train Loss 7.465081\n",
      "Loss  3.150216 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1477973 C_bot  0.15 k_c 0.0\n",
      "2447 Train Loss 7.4611216\n",
      "Loss  3.1477973 C_bot  0.15 k_c 0.0\n",
      "Loss  3.145089 C_bot  0.15 k_c 0.0\n",
      "2448 Train Loss 7.4572616\n",
      "Loss  3.145089 C_bot  0.15 k_c 0.0\n",
      "Loss  3.142716 C_bot  0.15 k_c 0.0\n",
      "2449 Train Loss 7.453597\n",
      "Loss  3.142716 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1405873 C_bot  0.15 k_c 0.0\n",
      "2450 Train Loss 7.4501038\n",
      "Loss  3.1405873 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1382577 C_bot  0.15 k_c 0.0\n",
      "2451 Train Loss 7.446691\n",
      "Loss  3.1382577 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1361551 C_bot  0.15 k_c 0.0\n",
      "2452 Train Loss 7.4431195\n",
      "Loss  3.1361551 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1336298 C_bot  0.15 k_c 0.0\n",
      "2453 Train Loss 7.43956\n",
      "Loss  3.1336298 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1313877 C_bot  0.15 k_c 0.0\n",
      "2454 Train Loss 7.435916\n",
      "Loss  3.1313877 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1288025 C_bot  0.15 k_c 0.0\n",
      "2455 Train Loss 7.4322047\n",
      "Loss  3.1288025 C_bot  0.15 k_c 0.0\n",
      "Loss  3.126563 C_bot  0.15 k_c 0.0\n",
      "2456 Train Loss 7.4287357\n",
      "Loss  3.126563 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1244462 C_bot  0.15 k_c 0.0\n",
      "2457 Train Loss 7.425354\n",
      "Loss  3.1244462 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1220965 C_bot  0.15 k_c 0.0\n",
      "2458 Train Loss 7.4219265\n",
      "Loss  3.1220965 C_bot  0.15 k_c 0.0\n",
      "Loss  3.120124 C_bot  0.15 k_c 0.0\n",
      "2459 Train Loss 7.418615\n",
      "Loss  3.120124 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1176548 C_bot  0.15 k_c 0.0\n",
      "2460 Train Loss 7.415119\n",
      "Loss  3.1176548 C_bot  0.15 k_c 0.0\n",
      "Loss  3.115557 C_bot  0.15 k_c 0.0\n",
      "2461 Train Loss 7.411714\n",
      "Loss  3.115557 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1132252 C_bot  0.15 k_c 0.0\n",
      "2462 Train Loss 7.408306\n",
      "Loss  3.1132252 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1110113 C_bot  0.15 k_c 0.0\n",
      "2463 Train Loss 7.404896\n",
      "Loss  3.1110113 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1087623 C_bot  0.15 k_c 0.0\n",
      "2464 Train Loss 7.4014854\n",
      "Loss  3.1087623 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1065304 C_bot  0.15 k_c 0.0\n",
      "2465 Train Loss 7.398169\n",
      "Loss  3.1065304 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1044672 C_bot  0.15 k_c 0.0\n",
      "2466 Train Loss 7.3948874\n",
      "Loss  3.1044672 C_bot  0.15 k_c 0.0\n",
      "Loss  3.102216 C_bot  0.15 k_c 0.0\n",
      "2467 Train Loss 7.391614\n",
      "Loss  3.102216 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1000383 C_bot  0.15 k_c 0.0\n",
      "2468 Train Loss 7.38822\n",
      "Loss  3.1000383 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0979223 C_bot  0.15 k_c 0.0\n",
      "2469 Train Loss 7.385076\n",
      "Loss  3.0979223 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0956259 C_bot  0.15 k_c 0.0\n",
      "2470 Train Loss 7.3816185\n",
      "Loss  3.0956259 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0935125 C_bot  0.15 k_c 0.0\n",
      "2471 Train Loss 7.378434\n",
      "Loss  3.0935125 C_bot  0.15 k_c 0.0\n",
      "Loss  3.091349 C_bot  0.15 k_c 0.0\n",
      "2472 Train Loss 7.3751855\n",
      "Loss  3.091349 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0891733 C_bot  0.15 k_c 0.0\n",
      "2473 Train Loss 7.371897\n",
      "Loss  3.0891733 C_bot  0.15 k_c 0.0\n",
      "Loss  3.087165 C_bot  0.15 k_c 0.0\n",
      "2474 Train Loss 7.368865\n",
      "Loss  3.087165 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0849721 C_bot  0.15 k_c 0.0\n",
      "2475 Train Loss 7.365543\n",
      "Loss  3.0849721 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0827627 C_bot  0.15 k_c 0.0\n",
      "2476 Train Loss 7.362336\n",
      "Loss  3.0827627 C_bot  0.15 k_c 0.0\n",
      "Loss  3.08066 C_bot  0.15 k_c 0.0\n",
      "2477 Train Loss 7.359123\n",
      "Loss  3.08066 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0785139 C_bot  0.15 k_c 0.0\n",
      "2478 Train Loss 7.3559694\n",
      "Loss  3.0785139 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0764751 C_bot  0.15 k_c 0.0\n",
      "2479 Train Loss 7.3528647\n",
      "Loss  3.0764751 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0743418 C_bot  0.15 k_c 0.0\n",
      "2480 Train Loss 7.3496985\n",
      "Loss  3.0743418 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0723102 C_bot  0.15 k_c 0.0\n",
      "2481 Train Loss 7.346653\n",
      "Loss  3.0723102 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0701065 C_bot  0.15 k_c 0.0\n",
      "2482 Train Loss 7.3433948\n",
      "Loss  3.0701065 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0681312 C_bot  0.15 k_c 0.0\n",
      "2483 Train Loss 7.340445\n",
      "Loss  3.0681312 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0661473 C_bot  0.15 k_c 0.0\n",
      "2484 Train Loss 7.337402\n",
      "Loss  3.0661473 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0638328 C_bot  0.15 k_c 0.0\n",
      "2485 Train Loss 7.3341327\n",
      "Loss  3.0638328 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0618956 C_bot  0.15 k_c 0.0\n",
      "2486 Train Loss 7.3311496\n",
      "Loss  3.0618956 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0598607 C_bot  0.15 k_c 0.0\n",
      "2487 Train Loss 7.328159\n",
      "Loss  3.0598607 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0577688 C_bot  0.15 k_c 0.0\n",
      "2488 Train Loss 7.32505\n",
      "Loss  3.0577688 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0555744 C_bot  0.15 k_c 0.0\n",
      "2489 Train Loss 7.3218884\n",
      "Loss  3.0555744 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0536122 C_bot  0.15 k_c 0.0\n",
      "2490 Train Loss 7.318944\n",
      "Loss  3.0536122 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0515594 C_bot  0.15 k_c 0.0\n",
      "2491 Train Loss 7.315913\n",
      "Loss  3.0515594 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0495296 C_bot  0.15 k_c 0.0\n",
      "2492 Train Loss 7.312932\n",
      "Loss  3.0495296 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0474603 C_bot  0.15 k_c 0.0\n",
      "2493 Train Loss 7.3098764\n",
      "Loss  3.0474603 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0453541 C_bot  0.15 k_c 0.0\n",
      "2494 Train Loss 7.3068447\n",
      "Loss  3.0453541 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0434391 C_bot  0.15 k_c 0.0\n",
      "2495 Train Loss 7.3039446\n",
      "Loss  3.0434391 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0414095 C_bot  0.15 k_c 0.0\n",
      "2496 Train Loss 7.3010025\n",
      "Loss  3.0414095 C_bot  0.15 k_c 0.0\n",
      "Loss  3.039409 C_bot  0.15 k_c 0.0\n",
      "2497 Train Loss 7.298025\n",
      "Loss  3.039409 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0372818 C_bot  0.15 k_c 0.0\n",
      "2498 Train Loss 7.2949934\n",
      "Loss  3.0372818 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0353496 C_bot  0.15 k_c 0.0\n",
      "2499 Train Loss 7.2920985\n",
      "Loss  3.0353496 C_bot  0.15 k_c 0.0\n",
      "Loss  3.033366 C_bot  0.15 k_c 0.0\n",
      "2500 Train Loss 7.2892165\n",
      "Loss  3.033366 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0313084 C_bot  0.15 k_c 0.0\n",
      "2501 Train Loss 7.28621\n",
      "Loss  3.0313084 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0292091 C_bot  0.15 k_c 0.0\n",
      "2502 Train Loss 7.283216\n",
      "Loss  3.0292091 C_bot  0.15 k_c 0.0\n",
      "Loss  3.027237 C_bot  0.15 k_c 0.0\n",
      "2503 Train Loss 7.2803116\n",
      "Loss  3.027237 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0253143 C_bot  0.15 k_c 0.0\n",
      "2504 Train Loss 7.2774944\n",
      "Loss  3.0253143 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0233202 C_bot  0.15 k_c 0.0\n",
      "2505 Train Loss 7.274585\n",
      "Loss  3.0233202 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0213103 C_bot  0.15 k_c 0.0\n",
      "2506 Train Loss 7.27168\n",
      "Loss  3.0213103 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0193238 C_bot  0.15 k_c 0.0\n",
      "2507 Train Loss 7.2687964\n",
      "Loss  3.0193238 C_bot  0.15 k_c 0.0\n",
      "Loss  3.01733 C_bot  0.15 k_c 0.0\n",
      "2508 Train Loss 7.2659082\n",
      "Loss  3.01733 C_bot  0.15 k_c 0.0\n",
      "Loss  3.015447 C_bot  0.15 k_c 0.0\n",
      "2509 Train Loss 7.263142\n",
      "Loss  3.015447 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0134368 C_bot  0.15 k_c 0.0\n",
      "2510 Train Loss 7.260244\n",
      "Loss  3.0134368 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0114553 C_bot  0.15 k_c 0.0\n",
      "2511 Train Loss 7.257389\n",
      "Loss  3.0114553 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0095518 C_bot  0.15 k_c 0.0\n",
      "2512 Train Loss 7.254605\n",
      "Loss  3.0095518 C_bot  0.15 k_c 0.0\n",
      "Loss  3.007576 C_bot  0.15 k_c 0.0\n",
      "2513 Train Loss 7.2517653\n",
      "Loss  3.007576 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0056543 C_bot  0.15 k_c 0.0\n",
      "2514 Train Loss 7.2489686\n",
      "Loss  3.0056543 C_bot  0.15 k_c 0.0\n",
      "Loss  3.003677 C_bot  0.15 k_c 0.0\n",
      "2515 Train Loss 7.246134\n",
      "Loss  3.003677 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0017178 C_bot  0.15 k_c 0.0\n",
      "2516 Train Loss 7.243311\n",
      "Loss  3.0017178 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9997396 C_bot  0.15 k_c 0.0\n",
      "2517 Train Loss 7.2404804\n",
      "Loss  2.9997396 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9978378 C_bot  0.15 k_c 0.0\n",
      "2518 Train Loss 7.2377257\n",
      "Loss  2.9978378 C_bot  0.15 k_c 0.0\n",
      "Loss  2.995872 C_bot  0.15 k_c 0.0\n",
      "2519 Train Loss 7.234913\n",
      "Loss  2.995872 C_bot  0.15 k_c 0.0\n",
      "Loss  2.99398 C_bot  0.15 k_c 0.0\n",
      "2520 Train Loss 7.23218\n",
      "Loss  2.99398 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9919257 C_bot  0.15 k_c 0.0\n",
      "2521 Train Loss 7.2292805\n",
      "Loss  2.9919257 C_bot  0.15 k_c 0.0\n",
      "Loss  2.990094 C_bot  0.15 k_c 0.0\n",
      "2522 Train Loss 7.22662\n",
      "Loss  2.990094 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9882503 C_bot  0.15 k_c 0.0\n",
      "2523 Train Loss 7.2239313\n",
      "Loss  2.9882503 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9862573 C_bot  0.15 k_c 0.0\n",
      "2524 Train Loss 7.221124\n",
      "Loss  2.9862573 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9842448 C_bot  0.15 k_c 0.0\n",
      "2525 Train Loss 7.2182674\n",
      "Loss  2.9842448 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9823725 C_bot  0.15 k_c 0.0\n",
      "2526 Train Loss 7.215594\n",
      "Loss  2.9823725 C_bot  0.15 k_c 0.0\n",
      "Loss  2.980475 C_bot  0.15 k_c 0.0\n",
      "2527 Train Loss 7.212853\n",
      "Loss  2.980475 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9786432 C_bot  0.15 k_c 0.0\n",
      "2528 Train Loss 7.210234\n",
      "Loss  2.9786432 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9766514 C_bot  0.15 k_c 0.0\n",
      "2529 Train Loss 7.2073984\n",
      "Loss  2.9766514 C_bot  0.15 k_c 0.0\n",
      "Loss  2.974687 C_bot  0.15 k_c 0.0\n",
      "2530 Train Loss 7.204663\n",
      "Loss  2.974687 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9728248 C_bot  0.15 k_c 0.0\n",
      "2531 Train Loss 7.20195\n",
      "Loss  2.9728248 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9709637 C_bot  0.15 k_c 0.0\n",
      "2532 Train Loss 7.1993394\n",
      "Loss  2.9709637 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9691145 C_bot  0.15 k_c 0.0\n",
      "2533 Train Loss 7.196628\n",
      "Loss  2.9691145 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9671443 C_bot  0.15 k_c 0.0\n",
      "2534 Train Loss 7.193937\n",
      "Loss  2.9671443 C_bot  0.15 k_c 0.0\n",
      "Loss  2.965315 C_bot  0.15 k_c 0.0\n",
      "2535 Train Loss 7.1912246\n",
      "Loss  2.965315 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9633548 C_bot  0.15 k_c 0.0\n",
      "2536 Train Loss 7.1885843\n",
      "Loss  2.9633548 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9616551 C_bot  0.15 k_c 0.0\n",
      "2537 Train Loss 7.185966\n",
      "Loss  2.9616551 C_bot  0.15 k_c 0.0\n",
      "Loss  2.959789 C_bot  0.15 k_c 0.0\n",
      "2538 Train Loss 7.1834793\n",
      "Loss  2.959789 C_bot  0.15 k_c 0.0\n",
      "Loss  2.958146 C_bot  0.15 k_c 0.0\n",
      "2539 Train Loss 7.1808543\n",
      "Loss  2.958146 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9562688 C_bot  0.15 k_c 0.0\n",
      "2540 Train Loss 7.17845\n",
      "Loss  2.9562688 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9551327 C_bot  0.15 k_c 0.0\n",
      "2541 Train Loss 7.1762247\n",
      "Loss  2.9551327 C_bot  0.15 k_c 0.0\n",
      "Loss  2.953422 C_bot  0.15 k_c 0.0\n",
      "2542 Train Loss 7.174139\n",
      "Loss  2.953422 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9529743 C_bot  0.15 k_c 0.0\n",
      "2543 Train Loss 7.17242\n",
      "Loss  2.9529743 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9521084 C_bot  0.15 k_c 0.0\n",
      "2544 Train Loss 7.171433\n",
      "Loss  2.9521084 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9541605 C_bot  0.15 k_c 0.0\n",
      "2545 Train Loss 7.1718965\n",
      "Loss  2.9541605 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9561832 C_bot  0.15 k_c 0.0\n",
      "2546 Train Loss 7.174234\n",
      "Loss  2.9561832 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9641647 C_bot  0.15 k_c 0.0\n",
      "2547 Train Loss 7.1800704\n",
      "Loss  2.9641647 C_bot  0.15 k_c 0.0\n",
      "Loss  2.975418 C_bot  0.15 k_c 0.0\n",
      "2548 Train Loss 7.1924043\n",
      "Loss  2.975418 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0021865 C_bot  0.15 k_c 0.0\n",
      "2549 Train Loss 7.2160444\n",
      "Loss  3.0021865 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0439832 C_bot  0.15 k_c 0.0\n",
      "2550 Train Loss 7.2602963\n",
      "Loss  3.0439832 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1303577 C_bot  0.15 k_c 0.0\n",
      "2551 Train Loss 7.3417835\n",
      "Loss  3.1303577 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2757955 C_bot  0.15 k_c 0.0\n",
      "2552 Train Loss 7.4922147\n",
      "Loss  3.2757955 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5594673 C_bot  0.15 k_c 0.0\n",
      "2553 Train Loss 7.767835\n",
      "Loss  3.5594673 C_bot  0.15 k_c 0.0\n",
      "Loss  4.063916 C_bot  0.15 k_c 0.0\n",
      "2554 Train Loss 8.282109\n",
      "Loss  4.063916 C_bot  0.15 k_c 0.0\n",
      "Loss  5.026176 C_bot  0.15 k_c 0.0\n",
      "2555 Train Loss 9.230632\n",
      "Loss  5.026176 C_bot  0.15 k_c 0.0\n",
      "Loss  6.811646 C_bot  0.15 k_c 0.0\n",
      "2556 Train Loss 11.035541\n",
      "Loss  6.811646 C_bot  0.15 k_c 0.0\n",
      "Loss  10.178384 C_bot  0.15 k_c 0.0\n",
      "2557 Train Loss 14.378507\n",
      "Loss  10.178384 C_bot  0.15 k_c 0.0\n",
      "Loss  16.63872 C_bot  0.15 k_c 0.0\n",
      "2558 Train Loss 20.878767\n",
      "Loss  16.63872 C_bot  0.15 k_c 0.0\n",
      "Loss  28.59417 C_bot  0.15 k_c 0.0\n",
      "2559 Train Loss 32.793358\n",
      "Loss  28.59417 C_bot  0.15 k_c 0.0\n",
      "Loss  52.086037 C_bot  0.15 k_c 0.0\n",
      "2560 Train Loss 56.37357\n",
      "Loss  52.086037 C_bot  0.15 k_c 0.0\n",
      "Loss  93.12356 C_bot  0.15 k_c 0.0\n",
      "2561 Train Loss 97.341095\n",
      "Loss  93.12356 C_bot  0.15 k_c 0.0\n",
      "Loss  172.84734 C_bot  0.15 k_c 0.0\n",
      "2562 Train Loss 177.282\n",
      "Loss  172.84734 C_bot  0.15 k_c 0.0\n",
      "Loss  287.44476 C_bot  0.15 k_c 0.0\n",
      "2563 Train Loss 291.73663\n",
      "Loss  287.44476 C_bot  0.15 k_c 0.0\n",
      "Loss  477.1523 C_bot  0.15 k_c 0.0\n",
      "2564 Train Loss 481.98914\n",
      "Loss  477.1523 C_bot  0.15 k_c 0.0\n",
      "Loss  597.87634 C_bot  0.15 k_c 0.0\n",
      "2565 Train Loss 602.2702\n",
      "Loss  597.87634 C_bot  0.15 k_c 0.0\n",
      "Loss  647.98267 C_bot  0.15 k_c 0.0\n",
      "2566 Train Loss 653.29846\n",
      "Loss  647.98267 C_bot  0.15 k_c 0.0\n",
      "Loss  392.56824 C_bot  0.15 k_c 0.0\n",
      "2567 Train Loss 396.95258\n",
      "Loss  392.56824 C_bot  0.15 k_c 0.0\n",
      "Loss  106.15996 C_bot  0.15 k_c 0.0\n",
      "2568 Train Loss 111.138954\n",
      "Loss  106.15996 C_bot  0.15 k_c 0.0\n",
      "Loss  7.260239 C_bot  0.15 k_c 0.0\n",
      "2569 Train Loss 12.094149\n",
      "Loss  7.260239 C_bot  0.15 k_c 0.0\n",
      "Loss  148.13708 C_bot  0.15 k_c 0.0\n",
      "2570 Train Loss 152.7746\n",
      "Loss  148.13708 C_bot  0.15 k_c 0.0\n",
      "Loss  304.60547 C_bot  0.15 k_c 0.0\n",
      "2571 Train Loss 310.309\n",
      "Loss  304.60547 C_bot  0.15 k_c 0.0\n",
      "Loss  229.81267 C_bot  0.15 k_c 0.0\n",
      "2572 Train Loss 234.53787\n",
      "Loss  229.81267 C_bot  0.15 k_c 0.0\n",
      "Loss  62.771397 C_bot  0.15 k_c 0.0\n",
      "2573 Train Loss 68.13025\n",
      "Loss  62.771397 C_bot  0.15 k_c 0.0\n",
      "Loss  9.488167 C_bot  0.15 k_c 0.0\n",
      "2574 Train Loss 14.657562\n",
      "Loss  9.488167 C_bot  0.15 k_c 0.0\n",
      "Loss  108.42032 C_bot  0.15 k_c 0.0\n",
      "2575 Train Loss 113.270294\n",
      "Loss  108.42032 C_bot  0.15 k_c 0.0\n",
      "Loss  173.24557 C_bot  0.15 k_c 0.0\n",
      "2576 Train Loss 178.82823\n",
      "Loss  173.24557 C_bot  0.15 k_c 0.0\n",
      "Loss  84.12226 C_bot  0.15 k_c 0.0\n",
      "2577 Train Loss 88.95684\n",
      "Loss  84.12226 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1835146 C_bot  0.15 k_c 0.0\n",
      "2578 Train Loss 10.204181\n",
      "Loss  5.1835146 C_bot  0.15 k_c 0.0\n",
      "Loss  46.179802 C_bot  0.15 k_c 0.0\n",
      "2579 Train Loss 51.314484\n",
      "Loss  46.179802 C_bot  0.15 k_c 0.0\n",
      "Loss  99.65128 C_bot  0.15 k_c 0.0\n",
      "2580 Train Loss 104.35674\n",
      "Loss  99.65128 C_bot  0.15 k_c 0.0\n",
      "Loss  62.93264 C_bot  0.15 k_c 0.0\n",
      "2581 Train Loss 67.9836\n",
      "Loss  62.93264 C_bot  0.15 k_c 0.0\n",
      "Loss  5.882079 C_bot  0.15 k_c 0.0\n",
      "2582 Train Loss 10.635612\n",
      "Loss  5.882079 C_bot  0.15 k_c 0.0\n",
      "Loss  28.815378 C_bot  0.15 k_c 0.0\n",
      "2583 Train Loss 33.485912\n",
      "Loss  28.815378 C_bot  0.15 k_c 0.0\n",
      "Loss  67.87824 C_bot  0.15 k_c 0.0\n",
      "2584 Train Loss 72.81093\n",
      "Loss  67.87824 C_bot  0.15 k_c 0.0\n",
      "Loss  39.57529 C_bot  0.15 k_c 0.0\n",
      "2585 Train Loss 44.239704\n",
      "Loss  39.57529 C_bot  0.15 k_c 0.0\n",
      "Loss  4.279019 C_bot  0.15 k_c 0.0\n",
      "2586 Train Loss 9.028767\n",
      "Loss  4.279019 C_bot  0.15 k_c 0.0\n",
      "Loss  22.565334 C_bot  0.15 k_c 0.0\n",
      "2587 Train Loss 27.366323\n",
      "Loss  22.565334 C_bot  0.15 k_c 0.0\n",
      "Loss  44.277214 C_bot  0.15 k_c 0.0\n",
      "2588 Train Loss 48.945118\n",
      "Loss  44.277214 C_bot  0.15 k_c 0.0\n",
      "Loss  24.0937 C_bot  0.15 k_c 0.0\n",
      "2589 Train Loss 28.854574\n",
      "Loss  24.0937 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6422675 C_bot  0.15 k_c 0.0\n",
      "2590 Train Loss 8.317484\n",
      "Loss  3.6422675 C_bot  0.15 k_c 0.0\n",
      "Loss  18.79391 C_bot  0.15 k_c 0.0\n",
      "2591 Train Loss 23.425634\n",
      "Loss  18.79391 C_bot  0.15 k_c 0.0\n",
      "Loss  31.056799 C_bot  0.15 k_c 0.0\n",
      "2592 Train Loss 35.781284\n",
      "Loss  31.056799 C_bot  0.15 k_c 0.0\n",
      "Loss  14.254919 C_bot  0.15 k_c 0.0\n",
      "2593 Train Loss 18.868744\n",
      "Loss  14.254919 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7077804 C_bot  0.15 k_c 0.0\n",
      "2594 Train Loss 8.331038\n",
      "Loss  3.7077804 C_bot  0.15 k_c 0.0\n",
      "Loss  15.784153 C_bot  0.15 k_c 0.0\n",
      "2595 Train Loss 20.442362\n",
      "Loss  15.784153 C_bot  0.15 k_c 0.0\n",
      "Loss  20.709753 C_bot  0.15 k_c 0.0\n",
      "2596 Train Loss 25.265162\n",
      "Loss  20.709753 C_bot  0.15 k_c 0.0\n",
      "Loss  8.634442 C_bot  0.15 k_c 0.0\n",
      "2597 Train Loss 13.238123\n",
      "Loss  8.634442 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0433254 C_bot  0.15 k_c 0.0\n",
      "2598 Train Loss 8.609309\n",
      "Loss  4.0433254 C_bot  0.15 k_c 0.0\n",
      "Loss  12.870056 C_bot  0.15 k_c 0.0\n",
      "2599 Train Loss 17.377949\n",
      "Loss  12.870056 C_bot  0.15 k_c 0.0\n",
      "Loss  14.407292 C_bot  0.15 k_c 0.0\n",
      "2600 Train Loss 18.990913\n",
      "Loss  14.407292 C_bot  0.15 k_c 0.0\n",
      "Loss  5.8285527 C_bot  0.15 k_c 0.0\n",
      "2601 Train Loss 10.327829\n",
      "Loss  5.8285527 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3891263 C_bot  0.15 k_c 0.0\n",
      "2602 Train Loss 8.882849\n",
      "Loss  4.3891263 C_bot  0.15 k_c 0.0\n",
      "Loss  10.455748 C_bot  0.15 k_c 0.0\n",
      "2603 Train Loss 14.990935\n",
      "Loss  10.455748 C_bot  0.15 k_c 0.0\n",
      "Loss  10.115155 C_bot  0.15 k_c 0.0\n",
      "2604 Train Loss 14.562448\n",
      "Loss  10.115155 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3886304 C_bot  0.15 k_c 0.0\n",
      "2605 Train Loss 8.86757\n",
      "Loss  4.3886304 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3957725 C_bot  0.15 k_c 0.0\n",
      "2606 Train Loss 8.862354\n",
      "Loss  4.3957725 C_bot  0.15 k_c 0.0\n",
      "Loss  8.376695 C_bot  0.15 k_c 0.0\n",
      "2607 Train Loss 12.795188\n",
      "Loss  8.376695 C_bot  0.15 k_c 0.0\n",
      "Loss  7.472644 C_bot  0.15 k_c 0.0\n",
      "2608 Train Loss 11.934248\n",
      "Loss  7.472644 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7893443 C_bot  0.15 k_c 0.0\n",
      "2609 Train Loss 8.209234\n",
      "Loss  3.7893443 C_bot  0.15 k_c 0.0\n",
      "Loss  4.279538 C_bot  0.15 k_c 0.0\n",
      "2610 Train Loss 8.689488\n",
      "Loss  4.279538 C_bot  0.15 k_c 0.0\n",
      "Loss  6.799375 C_bot  0.15 k_c 0.0\n",
      "2611 Train Loss 11.23359\n",
      "Loss  6.799375 C_bot  0.15 k_c 0.0\n",
      "Loss  5.877135 C_bot  0.15 k_c 0.0\n",
      "2612 Train Loss 10.268393\n",
      "Loss  5.877135 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5005896 C_bot  0.15 k_c 0.0\n",
      "2613 Train Loss 7.903648\n",
      "Loss  3.5005896 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0339737 C_bot  0.15 k_c 0.0\n",
      "2614 Train Loss 8.435437\n",
      "Loss  4.0339737 C_bot  0.15 k_c 0.0\n",
      "Loss  5.6557565 C_bot  0.15 k_c 0.0\n",
      "2615 Train Loss 10.032712\n",
      "Loss  5.6557565 C_bot  0.15 k_c 0.0\n",
      "Loss  4.9052052 C_bot  0.15 k_c 0.0\n",
      "2616 Train Loss 9.301529\n",
      "Loss  4.9052052 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3921344 C_bot  0.15 k_c 0.0\n",
      "2617 Train Loss 7.770459\n",
      "Loss  3.3921344 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8119385 C_bot  0.15 k_c 0.0\n",
      "2618 Train Loss 8.183481\n",
      "Loss  3.8119385 C_bot  0.15 k_c 0.0\n",
      "Loss  4.842869 C_bot  0.15 k_c 0.0\n",
      "2619 Train Loss 9.226019\n",
      "Loss  4.842869 C_bot  0.15 k_c 0.0\n",
      "Loss  4.337132 C_bot  0.15 k_c 0.0\n",
      "2620 Train Loss 8.698266\n",
      "Loss  4.337132 C_bot  0.15 k_c 0.0\n",
      "Loss  3.327853 C_bot  0.15 k_c 0.0\n",
      "2621 Train Loss 7.693409\n",
      "Loss  3.327853 C_bot  0.15 k_c 0.0\n",
      "Loss  3.593162 C_bot  0.15 k_c 0.0\n",
      "2622 Train Loss 7.9570546\n",
      "Loss  3.593162 C_bot  0.15 k_c 0.0\n",
      "Loss  4.290341 C_bot  0.15 k_c 0.0\n",
      "2623 Train Loss 8.6394005\n",
      "Loss  4.290341 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9637132 C_bot  0.15 k_c 0.0\n",
      "2624 Train Loss 8.322243\n",
      "Loss  3.9637132 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2975364 C_bot  0.15 k_c 0.0\n",
      "2625 Train Loss 7.6444097\n",
      "Loss  3.2975364 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4364498 C_bot  0.15 k_c 0.0\n",
      "2626 Train Loss 7.778585\n",
      "Loss  3.4364498 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8943202 C_bot  0.15 k_c 0.0\n",
      "2627 Train Loss 8.242809\n",
      "Loss  3.8943202 C_bot  0.15 k_c 0.0\n",
      "Loss  3.736807 C_bot  0.15 k_c 0.0\n",
      "2628 Train Loss 8.070848\n",
      "Loss  3.736807 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2724996 C_bot  0.15 k_c 0.0\n",
      "2629 Train Loss 7.610405\n",
      "Loss  3.2724996 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3140075 C_bot  0.15 k_c 0.0\n",
      "2630 Train Loss 7.649878\n",
      "Loss  3.3140075 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6373863 C_bot  0.15 k_c 0.0\n",
      "2631 Train Loss 7.9635305\n",
      "Loss  3.6373863 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5669694 C_bot  0.15 k_c 0.0\n",
      "2632 Train Loss 7.900138\n",
      "Loss  3.5669694 C_bot  0.15 k_c 0.0\n",
      "Loss  3.263389 C_bot  0.15 k_c 0.0\n",
      "2633 Train Loss 7.5874405\n",
      "Loss  3.263389 C_bot  0.15 k_c 0.0\n",
      "Loss  3.240911 C_bot  0.15 k_c 0.0\n",
      "2634 Train Loss 7.5629086\n",
      "Loss  3.240911 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4447708 C_bot  0.15 k_c 0.0\n",
      "2635 Train Loss 7.7699885\n",
      "Loss  3.4447708 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4552145 C_bot  0.15 k_c 0.0\n",
      "2636 Train Loss 7.7706985\n",
      "Loss  3.4552145 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2469716 C_bot  0.15 k_c 0.0\n",
      "2637 Train Loss 7.5658984\n",
      "Loss  3.2469716 C_bot  0.15 k_c 0.0\n",
      "Loss  3.189241 C_bot  0.15 k_c 0.0\n",
      "2638 Train Loss 7.5050344\n",
      "Loss  3.189241 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3209596 C_bot  0.15 k_c 0.0\n",
      "2639 Train Loss 7.6314836\n",
      "Loss  3.3209596 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3545303 C_bot  0.15 k_c 0.0\n",
      "2640 Train Loss 7.6688676\n",
      "Loss  3.3545303 C_bot  0.15 k_c 0.0\n",
      "Loss  3.236152 C_bot  0.15 k_c 0.0\n",
      "2641 Train Loss 7.543769\n",
      "Loss  3.236152 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1614516 C_bot  0.15 k_c 0.0\n",
      "2642 Train Loss 7.4688787\n",
      "Loss  3.1614516 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2263021 C_bot  0.15 k_c 0.0\n",
      "2643 Train Loss 7.533989\n",
      "Loss  3.2263021 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2815325 C_bot  0.15 k_c 0.0\n",
      "2644 Train Loss 7.5835\n",
      "Loss  3.2815325 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2144132 C_bot  0.15 k_c 0.0\n",
      "2645 Train Loss 7.518828\n",
      "Loss  3.2144132 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1464682 C_bot  0.15 k_c 0.0\n",
      "2646 Train Loss 7.447306\n",
      "Loss  3.1464682 C_bot  0.15 k_c 0.0\n",
      "Loss  3.169368 C_bot  0.15 k_c 0.0\n",
      "2647 Train Loss 7.467948\n",
      "Loss  3.169368 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2130637 C_bot  0.15 k_c 0.0\n",
      "2648 Train Loss 7.5130706\n",
      "Loss  3.2130637 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1946301 C_bot  0.15 k_c 0.0\n",
      "2649 Train Loss 7.489756\n",
      "Loss  3.1946301 C_bot  0.15 k_c 0.0\n",
      "Loss  3.137149 C_bot  0.15 k_c 0.0\n",
      "2650 Train Loss 7.4329424\n",
      "Loss  3.137149 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1307395 C_bot  0.15 k_c 0.0\n",
      "2651 Train Loss 7.4249616\n",
      "Loss  3.1307395 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1645763 C_bot  0.15 k_c 0.0\n",
      "2652 Train Loss 7.455564\n",
      "Loss  3.1645763 C_bot  0.15 k_c 0.0\n",
      "Loss  3.164361 C_bot  0.15 k_c 0.0\n",
      "2653 Train Loss 7.4566364\n",
      "Loss  3.164361 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1310027 C_bot  0.15 k_c 0.0\n",
      "2654 Train Loss 7.4196568\n",
      "Loss  3.1310027 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1096237 C_bot  0.15 k_c 0.0\n",
      "2655 Train Loss 7.3976326\n",
      "Loss  3.1096237 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1229615 C_bot  0.15 k_c 0.0\n",
      "2656 Train Loss 7.4105186\n",
      "Loss  3.1229615 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1367006 C_bot  0.15 k_c 0.0\n",
      "2657 Train Loss 7.420868\n",
      "Loss  3.1367006 C_bot  0.15 k_c 0.0\n",
      "Loss  3.118057 C_bot  0.15 k_c 0.0\n",
      "2658 Train Loss 7.402891\n",
      "Loss  3.118057 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0970562 C_bot  0.15 k_c 0.0\n",
      "2659 Train Loss 7.3793044\n",
      "Loss  3.0970562 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0963287 C_bot  0.15 k_c 0.0\n",
      "2660 Train Loss 7.3770375\n",
      "Loss  3.0963287 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1055222 C_bot  0.15 k_c 0.0\n",
      "2661 Train Loss 7.3861904\n",
      "Loss  3.1055222 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1040199 C_bot  0.15 k_c 0.0\n",
      "2662 Train Loss 7.3815765\n",
      "Loss  3.1040199 C_bot  0.15 k_c 0.0\n",
      "Loss  3.086354 C_bot  0.15 k_c 0.0\n",
      "2663 Train Loss 7.363941\n",
      "Loss  3.086354 C_bot  0.15 k_c 0.0\n",
      "Loss  3.077387 C_bot  0.15 k_c 0.0\n",
      "2664 Train Loss 7.3531923\n",
      "Loss  3.077387 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0817184 C_bot  0.15 k_c 0.0\n",
      "2665 Train Loss 7.355559\n",
      "Loss  3.0817184 C_bot  0.15 k_c 0.0\n",
      "Loss  3.083075 C_bot  0.15 k_c 0.0\n",
      "2666 Train Loss 7.356855\n",
      "Loss  3.083075 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0761979 C_bot  0.15 k_c 0.0\n",
      "2667 Train Loss 7.3472877\n",
      "Loss  3.0761979 C_bot  0.15 k_c 0.0\n",
      "Loss  3.064275 C_bot  0.15 k_c 0.0\n",
      "2668 Train Loss 7.3348103\n",
      "Loss  3.064275 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0611522 C_bot  0.15 k_c 0.0\n",
      "2669 Train Loss 7.330371\n",
      "Loss  3.0611522 C_bot  0.15 k_c 0.0\n",
      "Loss  3.064068 C_bot  0.15 k_c 0.0\n",
      "2670 Train Loss 7.3312263\n",
      "Loss  3.064068 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0606022 C_bot  0.15 k_c 0.0\n",
      "2671 Train Loss 7.3275223\n",
      "Loss  3.0606022 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0536628 C_bot  0.15 k_c 0.0\n",
      "2672 Train Loss 7.3183556\n",
      "Loss  3.0536628 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0464985 C_bot  0.15 k_c 0.0\n",
      "2673 Train Loss 7.3102937\n",
      "Loss  3.0464985 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0445933 C_bot  0.15 k_c 0.0\n",
      "2674 Train Loss 7.3073244\n",
      "Loss  3.0445933 C_bot  0.15 k_c 0.0\n",
      "Loss  3.044888 C_bot  0.15 k_c 0.0\n",
      "2675 Train Loss 7.305641\n",
      "Loss  3.044888 C_bot  0.15 k_c 0.0\n",
      "Loss  3.040011 C_bot  0.15 k_c 0.0\n",
      "2676 Train Loss 7.300315\n",
      "Loss  3.040011 C_bot  0.15 k_c 0.0\n",
      "Loss  3.034053 C_bot  0.15 k_c 0.0\n",
      "2677 Train Loss 7.292473\n",
      "Loss  3.034053 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0292544 C_bot  0.15 k_c 0.0\n",
      "2678 Train Loss 7.286594\n",
      "Loss  3.0292544 C_bot  0.15 k_c 0.0\n",
      "Loss  3.027092 C_bot  0.15 k_c 0.0\n",
      "2679 Train Loss 7.2834735\n",
      "Loss  3.027092 C_bot  0.15 k_c 0.0\n",
      "Loss  3.025688 C_bot  0.15 k_c 0.0\n",
      "2680 Train Loss 7.28025\n",
      "Loss  3.025688 C_bot  0.15 k_c 0.0\n",
      "Loss  3.020713 C_bot  0.15 k_c 0.0\n",
      "2681 Train Loss 7.274676\n",
      "Loss  3.020713 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0157206 C_bot  0.15 k_c 0.0\n",
      "2682 Train Loss 7.268048\n",
      "Loss  3.0157206 C_bot  0.15 k_c 0.0\n",
      "Loss  3.011788 C_bot  0.15 k_c 0.0\n",
      "2683 Train Loss 7.2629714\n",
      "Loss  3.011788 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0089555 C_bot  0.15 k_c 0.0\n",
      "2684 Train Loss 7.259206\n",
      "Loss  3.0089555 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0065017 C_bot  0.15 k_c 0.0\n",
      "2685 Train Loss 7.255087\n",
      "Loss  3.0065017 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0017745 C_bot  0.15 k_c 0.0\n",
      "2686 Train Loss 7.2496395\n",
      "Loss  3.0017745 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9972923 C_bot  0.15 k_c 0.0\n",
      "2687 Train Loss 7.243688\n",
      "Loss  2.9972923 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9934354 C_bot  0.15 k_c 0.0\n",
      "2688 Train Loss 7.238688\n",
      "Loss  2.9934354 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9902468 C_bot  0.15 k_c 0.0\n",
      "2689 Train Loss 7.234568\n",
      "Loss  2.9902468 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9871762 C_bot  0.15 k_c 0.0\n",
      "2690 Train Loss 7.2299833\n",
      "Loss  2.9871762 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9825523 C_bot  0.15 k_c 0.0\n",
      "2691 Train Loss 7.2245812\n",
      "Loss  2.9825523 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9782727 C_bot  0.15 k_c 0.0\n",
      "2692 Train Loss 7.2189465\n",
      "Loss  2.9782727 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9743702 C_bot  0.15 k_c 0.0\n",
      "2693 Train Loss 7.2139435\n",
      "Loss  2.9743702 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9707322 C_bot  0.15 k_c 0.0\n",
      "2694 Train Loss 7.209361\n",
      "Loss  2.9707322 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9672422 C_bot  0.15 k_c 0.0\n",
      "2695 Train Loss 7.2044907\n",
      "Loss  2.9672422 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9627707 C_bot  0.15 k_c 0.0\n",
      "2696 Train Loss 7.1992106\n",
      "Loss  2.9627707 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9584925 C_bot  0.15 k_c 0.0\n",
      "2697 Train Loss 7.193667\n",
      "Loss  2.9584925 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9542296 C_bot  0.15 k_c 0.0\n",
      "2698 Train Loss 7.188377\n",
      "Loss  2.9542296 C_bot  0.15 k_c 0.0\n",
      "Loss  2.95017 C_bot  0.15 k_c 0.0\n",
      "2699 Train Loss 7.1833677\n",
      "Loss  2.95017 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9463778 C_bot  0.15 k_c 0.0\n",
      "2700 Train Loss 7.1783333\n",
      "Loss  2.9463778 C_bot  0.15 k_c 0.0\n",
      "Loss  2.941847 C_bot  0.15 k_c 0.0\n",
      "2701 Train Loss 7.172987\n",
      "Loss  2.941847 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9374697 C_bot  0.15 k_c 0.0\n",
      "2702 Train Loss 7.167428\n",
      "Loss  2.9374697 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9329543 C_bot  0.15 k_c 0.0\n",
      "2703 Train Loss 7.1619625\n",
      "Loss  2.9329543 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9285843 C_bot  0.15 k_c 0.0\n",
      "2704 Train Loss 7.156652\n",
      "Loss  2.9285843 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9243953 C_bot  0.15 k_c 0.0\n",
      "2705 Train Loss 7.151349\n",
      "Loss  2.9243953 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9196112 C_bot  0.15 k_c 0.0\n",
      "2706 Train Loss 7.1457667\n",
      "Loss  2.9196112 C_bot  0.15 k_c 0.0\n",
      "Loss  2.915155 C_bot  0.15 k_c 0.0\n",
      "2707 Train Loss 7.1402197\n",
      "Loss  2.915155 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9103775 C_bot  0.15 k_c 0.0\n",
      "2708 Train Loss 7.1345816\n",
      "Loss  2.9103775 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9056592 C_bot  0.15 k_c 0.0\n",
      "2709 Train Loss 7.1289515\n",
      "Loss  2.9056592 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9010537 C_bot  0.15 k_c 0.0\n",
      "2710 Train Loss 7.1233587\n",
      "Loss  2.9010537 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8962376 C_bot  0.15 k_c 0.0\n",
      "2711 Train Loss 7.117773\n",
      "Loss  2.8962376 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8915696 C_bot  0.15 k_c 0.0\n",
      "2712 Train Loss 7.1121054\n",
      "Loss  2.8915696 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8864725 C_bot  0.15 k_c 0.0\n",
      "2713 Train Loss 7.1062326\n",
      "Loss  2.8864725 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8816075 C_bot  0.15 k_c 0.0\n",
      "2714 Train Loss 7.100492\n",
      "Loss  2.8816075 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8767345 C_bot  0.15 k_c 0.0\n",
      "2715 Train Loss 7.0947523\n",
      "Loss  2.8767345 C_bot  0.15 k_c 0.0\n",
      "Loss  2.87185 C_bot  0.15 k_c 0.0\n",
      "2716 Train Loss 7.089124\n",
      "Loss  2.87185 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8669112 C_bot  0.15 k_c 0.0\n",
      "2717 Train Loss 7.0832815\n",
      "Loss  2.8669112 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8618681 C_bot  0.15 k_c 0.0\n",
      "2718 Train Loss 7.077525\n",
      "Loss  2.8618681 C_bot  0.15 k_c 0.0\n",
      "Loss  2.856829 C_bot  0.15 k_c 0.0\n",
      "2719 Train Loss 7.0716476\n",
      "Loss  2.856829 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8517985 C_bot  0.15 k_c 0.0\n",
      "2720 Train Loss 7.065847\n",
      "Loss  2.8517985 C_bot  0.15 k_c 0.0\n",
      "Loss  2.846823 C_bot  0.15 k_c 0.0\n",
      "2721 Train Loss 7.0601363\n",
      "Loss  2.846823 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8417878 C_bot  0.15 k_c 0.0\n",
      "2722 Train Loss 7.054279\n",
      "Loss  2.8417878 C_bot  0.15 k_c 0.0\n",
      "Loss  2.836758 C_bot  0.15 k_c 0.0\n",
      "2723 Train Loss 7.0485663\n",
      "Loss  2.836758 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8318715 C_bot  0.15 k_c 0.0\n",
      "2724 Train Loss 7.042871\n",
      "Loss  2.8318715 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8267796 C_bot  0.15 k_c 0.0\n",
      "2725 Train Loss 7.037072\n",
      "Loss  2.8267796 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8218896 C_bot  0.15 k_c 0.0\n",
      "2726 Train Loss 7.0314336\n",
      "Loss  2.8218896 C_bot  0.15 k_c 0.0\n",
      "Loss  2.817036 C_bot  0.15 k_c 0.0\n",
      "2727 Train Loss 7.0258207\n",
      "Loss  2.817036 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8121536 C_bot  0.15 k_c 0.0\n",
      "2728 Train Loss 7.0202403\n",
      "Loss  2.8121536 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8074808 C_bot  0.15 k_c 0.0\n",
      "2729 Train Loss 7.014777\n",
      "Loss  2.8074808 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8025117 C_bot  0.15 k_c 0.0\n",
      "2730 Train Loss 7.009117\n",
      "Loss  2.8025117 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7978947 C_bot  0.15 k_c 0.0\n",
      "2731 Train Loss 7.0037174\n",
      "Loss  2.7978947 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7931097 C_bot  0.15 k_c 0.0\n",
      "2732 Train Loss 6.9982038\n",
      "Loss  2.7931097 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7885544 C_bot  0.15 k_c 0.0\n",
      "2733 Train Loss 6.992898\n",
      "Loss  2.7885544 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7841165 C_bot  0.15 k_c 0.0\n",
      "2734 Train Loss 6.987682\n",
      "Loss  2.7841165 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7795208 C_bot  0.15 k_c 0.0\n",
      "2735 Train Loss 6.982353\n",
      "Loss  2.7795208 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7750437 C_bot  0.15 k_c 0.0\n",
      "2736 Train Loss 6.9770684\n",
      "Loss  2.7750437 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7707949 C_bot  0.15 k_c 0.0\n",
      "2737 Train Loss 6.9720716\n",
      "Loss  2.7707949 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7664104 C_bot  0.15 k_c 0.0\n",
      "2738 Train Loss 6.966877\n",
      "Loss  2.7664104 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7620764 C_bot  0.15 k_c 0.0\n",
      "2739 Train Loss 6.9617553\n",
      "Loss  2.7620764 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7579277 C_bot  0.15 k_c 0.0\n",
      "2740 Train Loss 6.956805\n",
      "Loss  2.7579277 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7538366 C_bot  0.15 k_c 0.0\n",
      "2741 Train Loss 6.9518867\n",
      "Loss  2.7538366 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7496834 C_bot  0.15 k_c 0.0\n",
      "2742 Train Loss 6.9469337\n",
      "Loss  2.7496834 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7455688 C_bot  0.15 k_c 0.0\n",
      "2743 Train Loss 6.9419627\n",
      "Loss  2.7455688 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7416046 C_bot  0.15 k_c 0.0\n",
      "2744 Train Loss 6.9371824\n",
      "Loss  2.7416046 C_bot  0.15 k_c 0.0\n",
      "Loss  2.73765 C_bot  0.15 k_c 0.0\n",
      "2745 Train Loss 6.93236\n",
      "Loss  2.73765 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7336571 C_bot  0.15 k_c 0.0\n",
      "2746 Train Loss 6.927521\n",
      "Loss  2.7336571 C_bot  0.15 k_c 0.0\n",
      "Loss  2.729674 C_bot  0.15 k_c 0.0\n",
      "2747 Train Loss 6.922671\n",
      "Loss  2.729674 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7258055 C_bot  0.15 k_c 0.0\n",
      "2748 Train Loss 6.9179225\n",
      "Loss  2.7258055 C_bot  0.15 k_c 0.0\n",
      "Loss  2.721972 C_bot  0.15 k_c 0.0\n",
      "2749 Train Loss 6.9132223\n",
      "Loss  2.721972 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7181287 C_bot  0.15 k_c 0.0\n",
      "2750 Train Loss 6.908472\n",
      "Loss  2.7181287 C_bot  0.15 k_c 0.0\n",
      "Loss  2.714184 C_bot  0.15 k_c 0.0\n",
      "2751 Train Loss 6.9036536\n",
      "Loss  2.714184 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7104013 C_bot  0.15 k_c 0.0\n",
      "2752 Train Loss 6.89895\n",
      "Loss  2.7104013 C_bot  0.15 k_c 0.0\n",
      "Loss  2.706605 C_bot  0.15 k_c 0.0\n",
      "2753 Train Loss 6.8942633\n",
      "Loss  2.706605 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7028208 C_bot  0.15 k_c 0.0\n",
      "2754 Train Loss 6.8895545\n",
      "Loss  2.7028208 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6989136 C_bot  0.15 k_c 0.0\n",
      "2755 Train Loss 6.8847356\n",
      "Loss  2.6989136 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6951883 C_bot  0.15 k_c 0.0\n",
      "2756 Train Loss 6.8800883\n",
      "Loss  2.6951883 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6913705 C_bot  0.15 k_c 0.0\n",
      "2757 Train Loss 6.8753405\n",
      "Loss  2.6913705 C_bot  0.15 k_c 0.0\n",
      "Loss  2.68766 C_bot  0.15 k_c 0.0\n",
      "2758 Train Loss 6.870706\n",
      "Loss  2.68766 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6839743 C_bot  0.15 k_c 0.0\n",
      "2759 Train Loss 6.8660803\n",
      "Loss  2.6839743 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6802394 C_bot  0.15 k_c 0.0\n",
      "2760 Train Loss 6.861418\n",
      "Loss  2.6802394 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6765254 C_bot  0.15 k_c 0.0\n",
      "2761 Train Loss 6.856759\n",
      "Loss  2.6765254 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6728454 C_bot  0.15 k_c 0.0\n",
      "2762 Train Loss 6.8521457\n",
      "Loss  2.6728454 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6691632 C_bot  0.15 k_c 0.0\n",
      "2763 Train Loss 6.8475194\n",
      "Loss  2.6691632 C_bot  0.15 k_c 0.0\n",
      "Loss  2.665379 C_bot  0.15 k_c 0.0\n",
      "2764 Train Loss 6.842796\n",
      "Loss  2.665379 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6617775 C_bot  0.15 k_c 0.0\n",
      "2765 Train Loss 6.838254\n",
      "Loss  2.6617775 C_bot  0.15 k_c 0.0\n",
      "Loss  2.658052 C_bot  0.15 k_c 0.0\n",
      "2766 Train Loss 6.8335853\n",
      "Loss  2.658052 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6543267 C_bot  0.15 k_c 0.0\n",
      "2767 Train Loss 6.8289223\n",
      "Loss  2.6543267 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6508138 C_bot  0.15 k_c 0.0\n",
      "2768 Train Loss 6.8244677\n",
      "Loss  2.6508138 C_bot  0.15 k_c 0.0\n",
      "Loss  2.647055 C_bot  0.15 k_c 0.0\n",
      "2769 Train Loss 6.8197746\n",
      "Loss  2.647055 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6434505 C_bot  0.15 k_c 0.0\n",
      "2770 Train Loss 6.8152313\n",
      "Loss  2.6434505 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6398342 C_bot  0.15 k_c 0.0\n",
      "2771 Train Loss 6.8106847\n",
      "Loss  2.6398342 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6362162 C_bot  0.15 k_c 0.0\n",
      "2772 Train Loss 6.806135\n",
      "Loss  2.6362162 C_bot  0.15 k_c 0.0\n",
      "Loss  2.632696 C_bot  0.15 k_c 0.0\n",
      "2773 Train Loss 6.801688\n",
      "Loss  2.632696 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6291373 C_bot  0.15 k_c 0.0\n",
      "2774 Train Loss 6.7972074\n",
      "Loss  2.6291373 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6254745 C_bot  0.15 k_c 0.0\n",
      "2775 Train Loss 6.7926235\n",
      "Loss  2.6254745 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6219144 C_bot  0.15 k_c 0.0\n",
      "2776 Train Loss 6.7881517\n",
      "Loss  2.6219144 C_bot  0.15 k_c 0.0\n",
      "Loss  2.618343 C_bot  0.15 k_c 0.0\n",
      "2777 Train Loss 6.7836676\n",
      "Loss  2.618343 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6147628 C_bot  0.15 k_c 0.0\n",
      "2778 Train Loss 6.779186\n",
      "Loss  2.6147628 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6112995 C_bot  0.15 k_c 0.0\n",
      "2779 Train Loss 6.7748194\n",
      "Loss  2.6112995 C_bot  0.15 k_c 0.0\n",
      "Loss  2.607841 C_bot  0.15 k_c 0.0\n",
      "2780 Train Loss 6.7704716\n",
      "Loss  2.607841 C_bot  0.15 k_c 0.0\n",
      "Loss  2.604278 C_bot  0.15 k_c 0.0\n",
      "2781 Train Loss 6.7660165\n",
      "Loss  2.604278 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6008582 C_bot  0.15 k_c 0.0\n",
      "2782 Train Loss 6.761722\n",
      "Loss  2.6008582 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5973556 C_bot  0.15 k_c 0.0\n",
      "2783 Train Loss 6.757338\n",
      "Loss  2.5973556 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5938904 C_bot  0.15 k_c 0.0\n",
      "2784 Train Loss 6.753014\n",
      "Loss  2.5938904 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5903933 C_bot  0.15 k_c 0.0\n",
      "2785 Train Loss 6.748649\n",
      "Loss  2.5903933 C_bot  0.15 k_c 0.0\n",
      "Loss  2.58699 C_bot  0.15 k_c 0.0\n",
      "2786 Train Loss 6.744404\n",
      "Loss  2.58699 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5836527 C_bot  0.15 k_c 0.0\n",
      "2787 Train Loss 6.7402134\n",
      "Loss  2.5836527 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5801187 C_bot  0.15 k_c 0.0\n",
      "2788 Train Loss 6.735855\n",
      "Loss  2.5801187 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5767572 C_bot  0.15 k_c 0.0\n",
      "2789 Train Loss 6.731655\n",
      "Loss  2.5767572 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5734239 C_bot  0.15 k_c 0.0\n",
      "2790 Train Loss 6.727518\n",
      "Loss  2.5734239 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5700414 C_bot  0.15 k_c 0.0\n",
      "2791 Train Loss 6.7233143\n",
      "Loss  2.5700414 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5667431 C_bot  0.15 k_c 0.0\n",
      "2792 Train Loss 6.7192326\n",
      "Loss  2.5667431 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5634058 C_bot  0.15 k_c 0.0\n",
      "2793 Train Loss 6.7150893\n",
      "Loss  2.5634058 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5601115 C_bot  0.15 k_c 0.0\n",
      "2794 Train Loss 6.711035\n",
      "Loss  2.5601115 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5567904 C_bot  0.15 k_c 0.0\n",
      "2795 Train Loss 6.7069235\n",
      "Loss  2.5567904 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5535622 C_bot  0.15 k_c 0.0\n",
      "2796 Train Loss 6.7029605\n",
      "Loss  2.5535622 C_bot  0.15 k_c 0.0\n",
      "Loss  2.550388 C_bot  0.15 k_c 0.0\n",
      "2797 Train Loss 6.6990075\n",
      "Loss  2.550388 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5470614 C_bot  0.15 k_c 0.0\n",
      "2798 Train Loss 6.694976\n",
      "Loss  2.5470614 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5439775 C_bot  0.15 k_c 0.0\n",
      "2799 Train Loss 6.691122\n",
      "Loss  2.5439775 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5407064 C_bot  0.15 k_c 0.0\n",
      "2800 Train Loss 6.687178\n",
      "Loss  2.5407064 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5376582 C_bot  0.15 k_c 0.0\n",
      "2801 Train Loss 6.683363\n",
      "Loss  2.5376582 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5344608 C_bot  0.15 k_c 0.0\n",
      "2802 Train Loss 6.6795306\n",
      "Loss  2.5344608 C_bot  0.15 k_c 0.0\n",
      "Loss  2.531415 C_bot  0.15 k_c 0.0\n",
      "2803 Train Loss 6.6757116\n",
      "Loss  2.531415 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5282567 C_bot  0.15 k_c 0.0\n",
      "2804 Train Loss 6.6719656\n",
      "Loss  2.5282567 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5253935 C_bot  0.15 k_c 0.0\n",
      "2805 Train Loss 6.668307\n",
      "Loss  2.5253935 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5223148 C_bot  0.15 k_c 0.0\n",
      "2806 Train Loss 6.664706\n",
      "Loss  2.5223148 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5195582 C_bot  0.15 k_c 0.0\n",
      "2807 Train Loss 6.661104\n",
      "Loss  2.5195582 C_bot  0.15 k_c 0.0\n",
      "Loss  2.51661 C_bot  0.15 k_c 0.0\n",
      "2808 Train Loss 6.657731\n",
      "Loss  2.51661 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5143058 C_bot  0.15 k_c 0.0\n",
      "2809 Train Loss 6.6544867\n",
      "Loss  2.5143058 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5116699 C_bot  0.15 k_c 0.0\n",
      "2810 Train Loss 6.6515765\n",
      "Loss  2.5116699 C_bot  0.15 k_c 0.0\n",
      "Loss  2.510317 C_bot  0.15 k_c 0.0\n",
      "2811 Train Loss 6.649115\n",
      "Loss  2.510317 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5086195 C_bot  0.15 k_c 0.0\n",
      "2812 Train Loss 6.6473894\n",
      "Loss  2.5086195 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5096388 C_bot  0.15 k_c 0.0\n",
      "2813 Train Loss 6.6469975\n",
      "Loss  2.5096388 C_bot  0.15 k_c 0.0\n",
      "Loss  2.511225 C_bot  0.15 k_c 0.0\n",
      "2814 Train Loss 6.648982\n",
      "Loss  2.511225 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5194204 C_bot  0.15 k_c 0.0\n",
      "2815 Train Loss 6.655223\n",
      "Loss  2.5194204 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5319707 C_bot  0.15 k_c 0.0\n",
      "2816 Train Loss 6.668933\n",
      "Loss  2.5319707 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5623455 C_bot  0.15 k_c 0.0\n",
      "2817 Train Loss 6.696369\n",
      "Loss  2.5623455 C_bot  0.15 k_c 0.0\n",
      "Loss  2.612824 C_bot  0.15 k_c 0.0\n",
      "2818 Train Loss 6.7494135\n",
      "Loss  2.612824 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7174683 C_bot  0.15 k_c 0.0\n",
      "2819 Train Loss 6.8493147\n",
      "Loss  2.7174683 C_bot  0.15 k_c 0.0\n",
      "Loss  2.901803 C_bot  0.15 k_c 0.0\n",
      "2820 Train Loss 7.038902\n",
      "Loss  2.901803 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2658246 C_bot  0.15 k_c 0.0\n",
      "2821 Train Loss 7.3948603\n",
      "Loss  3.2658246 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9416208 C_bot  0.15 k_c 0.0\n",
      "2822 Train Loss 8.081255\n",
      "Loss  3.9416208 C_bot  0.15 k_c 0.0\n",
      "Loss  5.2495008 C_bot  0.15 k_c 0.0\n",
      "2823 Train Loss 9.374999\n",
      "Loss  5.2495008 C_bot  0.15 k_c 0.0\n",
      "Loss  7.780964 C_bot  0.15 k_c 0.0\n",
      "2824 Train Loss 11.928368\n",
      "Loss  7.780964 C_bot  0.15 k_c 0.0\n",
      "Loss  12.597084 C_bot  0.15 k_c 0.0\n",
      "2825 Train Loss 16.719484\n",
      "Loss  12.597084 C_bot  0.15 k_c 0.0\n",
      "Loss  22.2214 C_bot  0.15 k_c 0.0\n",
      "2826 Train Loss 26.392038\n",
      "Loss  22.2214 C_bot  0.15 k_c 0.0\n",
      "Loss  39.69761 C_bot  0.15 k_c 0.0\n",
      "2827 Train Loss 43.823997\n",
      "Loss  39.69761 C_bot  0.15 k_c 0.0\n",
      "Loss  74.73433 C_bot  0.15 k_c 0.0\n",
      "2828 Train Loss 78.978874\n",
      "Loss  74.73433 C_bot  0.15 k_c 0.0\n",
      "Loss  129.0875 C_bot  0.15 k_c 0.0\n",
      "2829 Train Loss 133.24486\n",
      "Loss  129.0875 C_bot  0.15 k_c 0.0\n",
      "Loss  225.4998 C_bot  0.15 k_c 0.0\n",
      "2830 Train Loss 229.9695\n",
      "Loss  225.4998 C_bot  0.15 k_c 0.0\n",
      "Loss  307.98984 C_bot  0.15 k_c 0.0\n",
      "2831 Train Loss 312.21042\n",
      "Loss  307.98984 C_bot  0.15 k_c 0.0\n",
      "Loss  368.15814 C_bot  0.15 k_c 0.0\n",
      "2832 Train Loss 373.01703\n",
      "Loss  368.15814 C_bot  0.15 k_c 0.0\n",
      "Loss  272.83963 C_bot  0.15 k_c 0.0\n",
      "2833 Train Loss 277.11722\n",
      "Loss  272.83963 C_bot  0.15 k_c 0.0\n",
      "Loss  109.52119 C_bot  0.15 k_c 0.0\n",
      "2834 Train Loss 114.369835\n",
      "Loss  109.52119 C_bot  0.15 k_c 0.0\n",
      "Loss  6.7281384 C_bot  0.15 k_c 0.0\n",
      "2835 Train Loss 11.35511\n",
      "Loss  6.7281384 C_bot  0.15 k_c 0.0\n",
      "Loss  47.136265 C_bot  0.15 k_c 0.0\n",
      "2836 Train Loss 51.75974\n",
      "Loss  47.136265 C_bot  0.15 k_c 0.0\n",
      "Loss  151.96944 C_bot  0.15 k_c 0.0\n",
      "2837 Train Loss 157.25609\n",
      "Loss  151.96944 C_bot  0.15 k_c 0.0\n",
      "Loss  178.9921 C_bot  0.15 k_c 0.0\n",
      "2838 Train Loss 183.67126\n",
      "Loss  178.9921 C_bot  0.15 k_c 0.0\n",
      "Loss  116.852196 C_bot  0.15 k_c 0.0\n",
      "2839 Train Loss 122.16915\n",
      "Loss  116.852196 C_bot  0.15 k_c 0.0\n",
      "Loss  25.426863 C_bot  0.15 k_c 0.0\n",
      "2840 Train Loss 30.298676\n",
      "Loss  25.426863 C_bot  0.15 k_c 0.0\n",
      "Loss  9.340472 C_bot  0.15 k_c 0.0\n",
      "2841 Train Loss 14.280658\n",
      "Loss  9.340472 C_bot  0.15 k_c 0.0\n",
      "Loss  63.284668 C_bot  0.15 k_c 0.0\n",
      "2842 Train Loss 68.515854\n",
      "Loss  63.284668 C_bot  0.15 k_c 0.0\n",
      "Loss  100.51946 C_bot  0.15 k_c 0.0\n",
      "2843 Train Loss 105.33012\n",
      "Loss  100.51946 C_bot  0.15 k_c 0.0\n",
      "Loss  77.45514 C_bot  0.15 k_c 0.0\n",
      "2844 Train Loss 82.63008\n",
      "Loss  77.45514 C_bot  0.15 k_c 0.0\n",
      "Loss  20.979967 C_bot  0.15 k_c 0.0\n",
      "2845 Train Loss 25.777403\n",
      "Loss  20.979967 C_bot  0.15 k_c 0.0\n",
      "Loss  5.499308 C_bot  0.15 k_c 0.0\n",
      "2846 Train Loss 10.284832\n",
      "Loss  5.499308 C_bot  0.15 k_c 0.0\n",
      "Loss  36.918354 C_bot  0.15 k_c 0.0\n",
      "2847 Train Loss 41.79671\n",
      "Loss  36.918354 C_bot  0.15 k_c 0.0\n",
      "Loss  57.47234 C_bot  0.15 k_c 0.0\n",
      "2848 Train Loss 62.116352\n",
      "Loss  57.47234 C_bot  0.15 k_c 0.0\n",
      "Loss  37.93876 C_bot  0.15 k_c 0.0\n",
      "2849 Train Loss 42.739216\n",
      "Loss  37.93876 C_bot  0.15 k_c 0.0\n",
      "Loss  7.4448028 C_bot  0.15 k_c 0.0\n",
      "2850 Train Loss 12.110432\n",
      "Loss  7.4448028 C_bot  0.15 k_c 0.0\n",
      "Loss  8.71919 C_bot  0.15 k_c 0.0\n",
      "2851 Train Loss 13.378319\n",
      "Loss  8.71919 C_bot  0.15 k_c 0.0\n",
      "Loss  30.575813 C_bot  0.15 k_c 0.0\n",
      "2852 Train Loss 35.30475\n",
      "Loss  30.575813 C_bot  0.15 k_c 0.0\n",
      "Loss  33.66766 C_bot  0.15 k_c 0.0\n",
      "2853 Train Loss 38.28899\n",
      "Loss  33.66766 C_bot  0.15 k_c 0.0\n",
      "Loss  14.976697 C_bot  0.15 k_c 0.0\n",
      "2854 Train Loss 19.62538\n",
      "Loss  14.976697 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3115973 C_bot  0.15 k_c 0.0\n",
      "2855 Train Loss 7.9031672\n",
      "Loss  3.3115973 C_bot  0.15 k_c 0.0\n",
      "Loss  12.785378 C_bot  0.15 k_c 0.0\n",
      "2856 Train Loss 17.34385\n",
      "Loss  12.785378 C_bot  0.15 k_c 0.0\n",
      "Loss  23.430153 C_bot  0.15 k_c 0.0\n",
      "2857 Train Loss 28.032103\n",
      "Loss  23.430153 C_bot  0.15 k_c 0.0\n",
      "Loss  16.936476 C_bot  0.15 k_c 0.0\n",
      "2858 Train Loss 21.459633\n",
      "Loss  16.936476 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0418468 C_bot  0.15 k_c 0.0\n",
      "2859 Train Loss 9.57797\n",
      "Loss  5.0418468 C_bot  0.15 k_c 0.0\n",
      "Loss  5.10658 C_bot  0.15 k_c 0.0\n",
      "2860 Train Loss 9.624347\n",
      "Loss  5.10658 C_bot  0.15 k_c 0.0\n",
      "Loss  13.5012 C_bot  0.15 k_c 0.0\n",
      "2861 Train Loss 17.961607\n",
      "Loss  13.5012 C_bot  0.15 k_c 0.0\n",
      "Loss  14.918293 C_bot  0.15 k_c 0.0\n",
      "2862 Train Loss 19.426472\n",
      "Loss  14.918293 C_bot  0.15 k_c 0.0\n",
      "Loss  7.414946 C_bot  0.15 k_c 0.0\n",
      "2863 Train Loss 11.843638\n",
      "Loss  7.414946 C_bot  0.15 k_c 0.0\n",
      "Loss  3.160643 C_bot  0.15 k_c 0.0\n",
      "2864 Train Loss 7.593988\n",
      "Loss  3.160643 C_bot  0.15 k_c 0.0\n",
      "Loss  7.082596 C_bot  0.15 k_c 0.0\n",
      "2865 Train Loss 11.531977\n",
      "Loss  7.082596 C_bot  0.15 k_c 0.0\n",
      "Loss  10.947696 C_bot  0.15 k_c 0.0\n",
      "2866 Train Loss 15.331615\n",
      "Loss  10.947696 C_bot  0.15 k_c 0.0\n",
      "Loss  8.2784395 C_bot  0.15 k_c 0.0\n",
      "2867 Train Loss 12.712496\n",
      "Loss  8.2784395 C_bot  0.15 k_c 0.0\n",
      "Loss  3.722222 C_bot  0.15 k_c 0.0\n",
      "2868 Train Loss 8.104733\n",
      "Loss  3.722222 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9129903 C_bot  0.15 k_c 0.0\n",
      "2869 Train Loss 8.28451\n",
      "Loss  3.9129903 C_bot  0.15 k_c 0.0\n",
      "Loss  7.1731515 C_bot  0.15 k_c 0.0\n",
      "2870 Train Loss 11.573386\n",
      "Loss  7.1731515 C_bot  0.15 k_c 0.0\n",
      "Loss  7.64945 C_bot  0.15 k_c 0.0\n",
      "2871 Train Loss 11.99321\n",
      "Loss  7.64945 C_bot  0.15 k_c 0.0\n",
      "Loss  4.732608 C_bot  0.15 k_c 0.0\n",
      "2872 Train Loss 9.106533\n",
      "Loss  4.732608 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0377553 C_bot  0.15 k_c 0.0\n",
      "2873 Train Loss 7.3902264\n",
      "Loss  3.0377553 C_bot  0.15 k_c 0.0\n",
      "Loss  4.498053 C_bot  0.15 k_c 0.0\n",
      "2874 Train Loss 8.833527\n",
      "Loss  4.498053 C_bot  0.15 k_c 0.0\n",
      "Loss  6.0844617 C_bot  0.15 k_c 0.0\n",
      "2875 Train Loss 10.444618\n",
      "Loss  6.0844617 C_bot  0.15 k_c 0.0\n",
      "Loss  5.227999 C_bot  0.15 k_c 0.0\n",
      "2876 Train Loss 9.551933\n",
      "Loss  5.227999 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3729174 C_bot  0.15 k_c 0.0\n",
      "2877 Train Loss 7.709599\n",
      "Loss  3.3729174 C_bot  0.15 k_c 0.0\n",
      "Loss  3.193157 C_bot  0.15 k_c 0.0\n",
      "2878 Train Loss 7.5233746\n",
      "Loss  3.193157 C_bot  0.15 k_c 0.0\n",
      "Loss  4.440439 C_bot  0.15 k_c 0.0\n",
      "2879 Train Loss 8.753818\n",
      "Loss  4.440439 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8790445 C_bot  0.15 k_c 0.0\n",
      "2880 Train Loss 9.2080345\n",
      "Loss  4.8790445 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9088898 C_bot  0.15 k_c 0.0\n",
      "2881 Train Loss 8.215107\n",
      "Loss  3.9088898 C_bot  0.15 k_c 0.0\n",
      "Loss  3.009798 C_bot  0.15 k_c 0.0\n",
      "2882 Train Loss 7.3189383\n",
      "Loss  3.009798 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3170679 C_bot  0.15 k_c 0.0\n",
      "2883 Train Loss 7.6246023\n",
      "Loss  3.3170679 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0636444 C_bot  0.15 k_c 0.0\n",
      "2884 Train Loss 8.355927\n",
      "Loss  4.0636444 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0106945 C_bot  0.15 k_c 0.0\n",
      "2885 Train Loss 8.31294\n",
      "Loss  4.0106945 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3113992 C_bot  0.15 k_c 0.0\n",
      "2886 Train Loss 7.5977144\n",
      "Loss  3.3113992 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9510803 C_bot  0.15 k_c 0.0\n",
      "2887 Train Loss 7.236842\n",
      "Loss  2.9510803 C_bot  0.15 k_c 0.0\n",
      "Loss  3.2877538 C_bot  0.15 k_c 0.0\n",
      "2888 Train Loss 7.574092\n",
      "Loss  3.2877538 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6718273 C_bot  0.15 k_c 0.0\n",
      "2889 Train Loss 7.9442906\n",
      "Loss  3.6718273 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4969475 C_bot  0.15 k_c 0.0\n",
      "2890 Train Loss 7.7773128\n",
      "Loss  3.4969475 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0680501 C_bot  0.15 k_c 0.0\n",
      "2891 Train Loss 7.3366585\n",
      "Loss  3.0680501 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9429584 C_bot  0.15 k_c 0.0\n",
      "2892 Train Loss 7.2103357\n",
      "Loss  2.9429584 C_bot  0.15 k_c 0.0\n",
      "Loss  3.187405 C_bot  0.15 k_c 0.0\n",
      "2893 Train Loss 7.4570956\n",
      "Loss  3.187405 C_bot  0.15 k_c 0.0\n",
      "Loss  3.377863 C_bot  0.15 k_c 0.0\n",
      "2894 Train Loss 7.6358886\n",
      "Loss  3.377863 C_bot  0.15 k_c 0.0\n",
      "Loss  3.222139 C_bot  0.15 k_c 0.0\n",
      "2895 Train Loss 7.4867563\n",
      "Loss  3.222139 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9719708 C_bot  0.15 k_c 0.0\n",
      "2896 Train Loss 7.2275124\n",
      "Loss  2.9719708 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9273977 C_bot  0.15 k_c 0.0\n",
      "2897 Train Loss 7.1810064\n",
      "Loss  2.9273977 C_bot  0.15 k_c 0.0\n",
      "Loss  3.080247 C_bot  0.15 k_c 0.0\n",
      "2898 Train Loss 7.335516\n",
      "Loss  3.080247 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1809783 C_bot  0.15 k_c 0.0\n",
      "2899 Train Loss 7.4269238\n",
      "Loss  3.1809783 C_bot  0.15 k_c 0.0\n",
      "Loss  3.075506 C_bot  0.15 k_c 0.0\n",
      "2900 Train Loss 7.3260164\n",
      "Loss  3.075506 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9274447 C_bot  0.15 k_c 0.0\n",
      "2901 Train Loss 7.1713753\n",
      "Loss  2.9274447 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9021206 C_bot  0.15 k_c 0.0\n",
      "2902 Train Loss 7.1444883\n",
      "Loss  2.9021206 C_bot  0.15 k_c 0.0\n",
      "Loss  2.990249 C_bot  0.15 k_c 0.0\n",
      "2903 Train Loss 7.233574\n",
      "Loss  2.990249 C_bot  0.15 k_c 0.0\n",
      "Loss  3.051142 C_bot  0.15 k_c 0.0\n",
      "2904 Train Loss 7.2874737\n",
      "Loss  3.051142 C_bot  0.15 k_c 0.0\n",
      "Loss  2.990272 C_bot  0.15 k_c 0.0\n",
      "2905 Train Loss 7.2295256\n",
      "Loss  2.990272 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9004664 C_bot  0.15 k_c 0.0\n",
      "2906 Train Loss 7.134478\n",
      "Loss  2.9004664 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8758159 C_bot  0.15 k_c 0.0\n",
      "2907 Train Loss 7.108619\n",
      "Loss  2.8758159 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9226308 C_bot  0.15 k_c 0.0\n",
      "2908 Train Loss 7.155658\n",
      "Loss  2.9226308 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9646657 C_bot  0.15 k_c 0.0\n",
      "2909 Train Loss 7.192545\n",
      "Loss  2.9646657 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9358184 C_bot  0.15 k_c 0.0\n",
      "2910 Train Loss 7.165778\n",
      "Loss  2.9358184 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8813758 C_bot  0.15 k_c 0.0\n",
      "2911 Train Loss 7.106942\n",
      "Loss  2.8813758 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8549504 C_bot  0.15 k_c 0.0\n",
      "2912 Train Loss 7.0796843\n",
      "Loss  2.8549504 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8751032 C_bot  0.15 k_c 0.0\n",
      "2913 Train Loss 7.0992403\n",
      "Loss  2.8751032 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9047258 C_bot  0.15 k_c 0.0\n",
      "2914 Train Loss 7.124779\n",
      "Loss  2.9047258 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8958588 C_bot  0.15 k_c 0.0\n",
      "2915 Train Loss 7.117201\n",
      "Loss  2.8958588 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8653853 C_bot  0.15 k_c 0.0\n",
      "2916 Train Loss 7.082765\n",
      "Loss  2.8653853 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8400917 C_bot  0.15 k_c 0.0\n",
      "2917 Train Loss 7.0570493\n",
      "Loss  2.8400917 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8433318 C_bot  0.15 k_c 0.0\n",
      "2918 Train Loss 7.059144\n",
      "Loss  2.8433318 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8614204 C_bot  0.15 k_c 0.0\n",
      "2919 Train Loss 7.0741243\n",
      "Loss  2.8614204 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8627162 C_bot  0.15 k_c 0.0\n",
      "2920 Train Loss 7.076144\n",
      "Loss  2.8627162 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8490937 C_bot  0.15 k_c 0.0\n",
      "2921 Train Loss 7.0588694\n",
      "Loss  2.8490937 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8285847 C_bot  0.15 k_c 0.0\n",
      "2922 Train Loss 7.038174\n",
      "Loss  2.8285847 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8226786 C_bot  0.15 k_c 0.0\n",
      "2923 Train Loss 7.030526\n",
      "Loss  2.8226786 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8301249 C_bot  0.15 k_c 0.0\n",
      "2924 Train Loss 7.035665\n",
      "Loss  2.8301249 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8345184 C_bot  0.15 k_c 0.0\n",
      "2925 Train Loss 7.04016\n",
      "Loss  2.8345184 C_bot  0.15 k_c 0.0\n",
      "Loss  2.831545 C_bot  0.15 k_c 0.0\n",
      "2926 Train Loss 7.0340033\n",
      "Loss  2.831545 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8178036 C_bot  0.15 k_c 0.0\n",
      "2927 Train Loss 7.0202103\n",
      "Loss  2.8178036 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8089886 C_bot  0.15 k_c 0.0\n",
      "2928 Train Loss 7.009263\n",
      "Loss  2.8089886 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8079267 C_bot  0.15 k_c 0.0\n",
      "2929 Train Loss 7.00661\n",
      "Loss  2.8079267 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8105083 C_bot  0.15 k_c 0.0\n",
      "2930 Train Loss 7.0086017\n",
      "Loss  2.8105083 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8123853 C_bot  0.15 k_c 0.0\n",
      "2931 Train Loss 7.0078306\n",
      "Loss  2.8123853 C_bot  0.15 k_c 0.0\n",
      "Loss  2.805629 C_bot  0.15 k_c 0.0\n",
      "2932 Train Loss 7.0009065\n",
      "Loss  2.805629 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7985022 C_bot  0.15 k_c 0.0\n",
      "2933 Train Loss 6.991516\n",
      "Loss  2.7985022 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7928522 C_bot  0.15 k_c 0.0\n",
      "2934 Train Loss 6.984868\n",
      "Loss  2.7928522 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7918012 C_bot  0.15 k_c 0.0\n",
      "2935 Train Loss 6.9827213\n",
      "Loss  2.7918012 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7931676 C_bot  0.15 k_c 0.0\n",
      "2936 Train Loss 6.98209\n",
      "Loss  2.7931676 C_bot  0.15 k_c 0.0\n",
      "Loss  2.790935 C_bot  0.15 k_c 0.0\n",
      "2937 Train Loss 6.979455\n",
      "Loss  2.790935 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7874274 C_bot  0.15 k_c 0.0\n",
      "2938 Train Loss 6.9737678\n",
      "Loss  2.7874274 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7814863 C_bot  0.15 k_c 0.0\n",
      "2939 Train Loss 6.9671516\n",
      "Loss  2.7814863 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7778983 C_bot  0.15 k_c 0.0\n",
      "2940 Train Loss 6.9620585\n",
      "Loss  2.7778983 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7766423 C_bot  0.15 k_c 0.0\n",
      "2941 Train Loss 6.9593754\n",
      "Loss  2.7766423 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7753057 C_bot  0.15 k_c 0.0\n",
      "2942 Train Loss 6.9572964\n",
      "Loss  2.7753057 C_bot  0.15 k_c 0.0\n",
      "Loss  2.774253 C_bot  0.15 k_c 0.0\n",
      "2943 Train Loss 6.9543586\n",
      "Loss  2.774253 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7702713 C_bot  0.15 k_c 0.0\n",
      "2944 Train Loss 6.9498057\n",
      "Loss  2.7702713 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7668781 C_bot  0.15 k_c 0.0\n",
      "2945 Train Loss 6.944714\n",
      "Loss  2.7668781 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7633684 C_bot  0.15 k_c 0.0\n",
      "2946 Train Loss 6.9402094\n",
      "Loss  2.7633684 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7610884 C_bot  0.15 k_c 0.0\n",
      "2947 Train Loss 6.9368024\n",
      "Loss  2.7610884 C_bot  0.15 k_c 0.0\n",
      "Loss  2.760016 C_bot  0.15 k_c 0.0\n",
      "2948 Train Loss 6.934223\n",
      "Loss  2.760016 C_bot  0.15 k_c 0.0\n",
      "Loss  2.757759 C_bot  0.15 k_c 0.0\n",
      "2949 Train Loss 6.9312587\n",
      "Loss  2.757759 C_bot  0.15 k_c 0.0\n",
      "Loss  2.755744 C_bot  0.15 k_c 0.0\n",
      "2950 Train Loss 6.9275856\n",
      "Loss  2.755744 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7521956 C_bot  0.15 k_c 0.0\n",
      "2951 Train Loss 6.9232945\n",
      "Loss  2.7521956 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7494779 C_bot  0.15 k_c 0.0\n",
      "2952 Train Loss 6.919181\n",
      "Loss  2.7494779 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7468784 C_bot  0.15 k_c 0.0\n",
      "2953 Train Loss 6.9154835\n",
      "Loss  2.7468784 C_bot  0.15 k_c 0.0\n",
      "Loss  2.744837 C_bot  0.15 k_c 0.0\n",
      "2954 Train Loss 6.9124384\n",
      "Loss  2.744837 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7431905 C_bot  0.15 k_c 0.0\n",
      "2955 Train Loss 6.9093757\n",
      "Loss  2.7431905 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7407424 C_bot  0.15 k_c 0.0\n",
      "2956 Train Loss 6.9061384\n",
      "Loss  2.7407424 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7385375 C_bot  0.15 k_c 0.0\n",
      "2957 Train Loss 6.9024787\n",
      "Loss  2.7385375 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7356145 C_bot  0.15 k_c 0.0\n",
      "2958 Train Loss 6.8986926\n",
      "Loss  2.7356145 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7333112 C_bot  0.15 k_c 0.0\n",
      "2959 Train Loss 6.895147\n",
      "Loss  2.7333112 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7310412 C_bot  0.15 k_c 0.0\n",
      "2960 Train Loss 6.8917704\n",
      "Loss  2.7310412 C_bot  0.15 k_c 0.0\n",
      "Loss  2.728921 C_bot  0.15 k_c 0.0\n",
      "2961 Train Loss 6.8886714\n",
      "Loss  2.728921 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7270346 C_bot  0.15 k_c 0.0\n",
      "2962 Train Loss 6.885483\n",
      "Loss  2.7270346 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7245264 C_bot  0.15 k_c 0.0\n",
      "2963 Train Loss 6.882124\n",
      "Loss  2.7245264 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7225287 C_bot  0.15 k_c 0.0\n",
      "2964 Train Loss 6.8788137\n",
      "Loss  2.7225287 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7200165 C_bot  0.15 k_c 0.0\n",
      "2965 Train Loss 6.8754015\n",
      "Loss  2.7200165 C_bot  0.15 k_c 0.0\n",
      "Loss  2.717776 C_bot  0.15 k_c 0.0\n",
      "2966 Train Loss 6.871999\n",
      "Loss  2.717776 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7156267 C_bot  0.15 k_c 0.0\n",
      "2967 Train Loss 6.868798\n",
      "Loss  2.7156267 C_bot  0.15 k_c 0.0\n",
      "Loss  2.713297 C_bot  0.15 k_c 0.0\n",
      "2968 Train Loss 6.865491\n",
      "Loss  2.713297 C_bot  0.15 k_c 0.0\n",
      "Loss  2.711487 C_bot  0.15 k_c 0.0\n",
      "2969 Train Loss 6.8624997\n",
      "Loss  2.711487 C_bot  0.15 k_c 0.0\n",
      "Loss  2.709115 C_bot  0.15 k_c 0.0\n",
      "2970 Train Loss 6.8592544\n",
      "Loss  2.709115 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7071257 C_bot  0.15 k_c 0.0\n",
      "2971 Train Loss 6.8560624\n",
      "Loss  2.7071257 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7046986 C_bot  0.15 k_c 0.0\n",
      "2972 Train Loss 6.852749\n",
      "Loss  2.7046986 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7026 C_bot  0.15 k_c 0.0\n",
      "2973 Train Loss 6.849532\n",
      "Loss  2.7026 C_bot  0.15 k_c 0.0\n",
      "Loss  2.700416 C_bot  0.15 k_c 0.0\n",
      "2974 Train Loss 6.846379\n",
      "Loss  2.700416 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6981733 C_bot  0.15 k_c 0.0\n",
      "2975 Train Loss 6.843142\n",
      "Loss  2.6981733 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6961129 C_bot  0.15 k_c 0.0\n",
      "2976 Train Loss 6.8400235\n",
      "Loss  2.6961129 C_bot  0.15 k_c 0.0\n",
      "Loss  2.694147 C_bot  0.15 k_c 0.0\n",
      "2977 Train Loss 6.8371572\n",
      "Loss  2.694147 C_bot  0.15 k_c 0.0\n",
      "Loss  2.692026 C_bot  0.15 k_c 0.0\n",
      "2978 Train Loss 6.8339376\n",
      "Loss  2.692026 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6898594 C_bot  0.15 k_c 0.0\n",
      "2979 Train Loss 6.8309007\n",
      "Loss  2.6898594 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6876643 C_bot  0.15 k_c 0.0\n",
      "2980 Train Loss 6.827633\n",
      "Loss  2.6876643 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6855032 C_bot  0.15 k_c 0.0\n",
      "2981 Train Loss 6.8245745\n",
      "Loss  2.6855032 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6834369 C_bot  0.15 k_c 0.0\n",
      "2982 Train Loss 6.8215055\n",
      "Loss  2.6834369 C_bot  0.15 k_c 0.0\n",
      "Loss  2.681258 C_bot  0.15 k_c 0.0\n",
      "2983 Train Loss 6.8183775\n",
      "Loss  2.681258 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6791804 C_bot  0.15 k_c 0.0\n",
      "2984 Train Loss 6.8153744\n",
      "Loss  2.6791804 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6771145 C_bot  0.15 k_c 0.0\n",
      "2985 Train Loss 6.812313\n",
      "Loss  2.6771145 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6751297 C_bot  0.15 k_c 0.0\n",
      "2986 Train Loss 6.8094554\n",
      "Loss  2.6751297 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6730783 C_bot  0.15 k_c 0.0\n",
      "2987 Train Loss 6.8063946\n",
      "Loss  2.6730783 C_bot  0.15 k_c 0.0\n",
      "Loss  2.670902 C_bot  0.15 k_c 0.0\n",
      "2988 Train Loss 6.803361\n",
      "Loss  2.670902 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6689634 C_bot  0.15 k_c 0.0\n",
      "2989 Train Loss 6.8004317\n",
      "Loss  2.6689634 C_bot  0.15 k_c 0.0\n",
      "Loss  2.666745 C_bot  0.15 k_c 0.0\n",
      "2990 Train Loss 6.797345\n",
      "Loss  2.666745 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6647038 C_bot  0.15 k_c 0.0\n",
      "2991 Train Loss 6.794354\n",
      "Loss  2.6647038 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6626532 C_bot  0.15 k_c 0.0\n",
      "2992 Train Loss 6.7914104\n",
      "Loss  2.6626532 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6605887 C_bot  0.15 k_c 0.0\n",
      "2993 Train Loss 6.788443\n",
      "Loss  2.6605887 C_bot  0.15 k_c 0.0\n",
      "Loss  2.658506 C_bot  0.15 k_c 0.0\n",
      "2994 Train Loss 6.7854404\n",
      "Loss  2.658506 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6563003 C_bot  0.15 k_c 0.0\n",
      "2995 Train Loss 6.78237\n",
      "Loss  2.6563003 C_bot  0.15 k_c 0.0\n",
      "Loss  2.654375 C_bot  0.15 k_c 0.0\n",
      "2996 Train Loss 6.7795143\n",
      "Loss  2.654375 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6524043 C_bot  0.15 k_c 0.0\n",
      "2997 Train Loss 6.7766986\n",
      "Loss  2.6524043 C_bot  0.15 k_c 0.0\n",
      "Loss  2.650284 C_bot  0.15 k_c 0.0\n",
      "2998 Train Loss 6.773654\n",
      "Loss  2.650284 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6483173 C_bot  0.15 k_c 0.0\n",
      "2999 Train Loss 6.7708497\n",
      "Loss  2.6483173 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6462548 C_bot  0.15 k_c 0.0\n",
      "3000 Train Loss 6.76788\n",
      "Loss  2.6462548 C_bot  0.15 k_c 0.0\n",
      "Loss  2.644157 C_bot  0.15 k_c 0.0\n",
      "3001 Train Loss 6.764942\n",
      "Loss  2.644157 C_bot  0.15 k_c 0.0\n",
      "Loss  2.642264 C_bot  0.15 k_c 0.0\n",
      "3002 Train Loss 6.7621646\n",
      "Loss  2.642264 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6401384 C_bot  0.15 k_c 0.0\n",
      "3003 Train Loss 6.7591915\n",
      "Loss  2.6401384 C_bot  0.15 k_c 0.0\n",
      "Loss  2.638065 C_bot  0.15 k_c 0.0\n",
      "3004 Train Loss 6.7562623\n",
      "Loss  2.638065 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6361725 C_bot  0.15 k_c 0.0\n",
      "3005 Train Loss 6.75351\n",
      "Loss  2.6361725 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6340177 C_bot  0.15 k_c 0.0\n",
      "3006 Train Loss 6.7505283\n",
      "Loss  2.6340177 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6321704 C_bot  0.15 k_c 0.0\n",
      "3007 Train Loss 6.7478113\n",
      "Loss  2.6321704 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6300533 C_bot  0.15 k_c 0.0\n",
      "3008 Train Loss 6.744891\n",
      "Loss  2.6300533 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6281886 C_bot  0.15 k_c 0.0\n",
      "3009 Train Loss 6.742156\n",
      "Loss  2.6281886 C_bot  0.15 k_c 0.0\n",
      "Loss  2.625988 C_bot  0.15 k_c 0.0\n",
      "3010 Train Loss 6.739168\n",
      "Loss  2.625988 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6241992 C_bot  0.15 k_c 0.0\n",
      "3011 Train Loss 6.7365136\n",
      "Loss  2.6241992 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6220453 C_bot  0.15 k_c 0.0\n",
      "3012 Train Loss 6.73358\n",
      "Loss  2.6220453 C_bot  0.15 k_c 0.0\n",
      "Loss  2.620161 C_bot  0.15 k_c 0.0\n",
      "3013 Train Loss 6.730844\n",
      "Loss  2.620161 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6180675 C_bot  0.15 k_c 0.0\n",
      "3014 Train Loss 6.727972\n",
      "Loss  2.6180675 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6162074 C_bot  0.15 k_c 0.0\n",
      "3015 Train Loss 6.725275\n",
      "Loss  2.6162074 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6140692 C_bot  0.15 k_c 0.0\n",
      "3016 Train Loss 6.7223606\n",
      "Loss  2.6140692 C_bot  0.15 k_c 0.0\n",
      "Loss  2.612166 C_bot  0.15 k_c 0.0\n",
      "3017 Train Loss 6.719635\n",
      "Loss  2.612166 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6102052 C_bot  0.15 k_c 0.0\n",
      "3018 Train Loss 6.716901\n",
      "Loss  2.6102052 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6082346 C_bot  0.15 k_c 0.0\n",
      "3019 Train Loss 6.714122\n",
      "Loss  2.6082346 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6062376 C_bot  0.15 k_c 0.0\n",
      "3020 Train Loss 6.711354\n",
      "Loss  2.6062376 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6042333 C_bot  0.15 k_c 0.0\n",
      "3021 Train Loss 6.708553\n",
      "Loss  2.6042333 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6023505 C_bot  0.15 k_c 0.0\n",
      "3022 Train Loss 6.7059026\n",
      "Loss  2.6023505 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6002827 C_bot  0.15 k_c 0.0\n",
      "3023 Train Loss 6.703052\n",
      "Loss  2.6002827 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5982192 C_bot  0.15 k_c 0.0\n",
      "3024 Train Loss 6.700224\n",
      "Loss  2.5982192 C_bot  0.15 k_c 0.0\n",
      "Loss  2.596348 C_bot  0.15 k_c 0.0\n",
      "3025 Train Loss 6.69758\n",
      "Loss  2.596348 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5943859 C_bot  0.15 k_c 0.0\n",
      "3026 Train Loss 6.694859\n",
      "Loss  2.5943859 C_bot  0.15 k_c 0.0\n",
      "Loss  2.592383 C_bot  0.15 k_c 0.0\n",
      "3027 Train Loss 6.692093\n",
      "Loss  2.592383 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5904825 C_bot  0.15 k_c 0.0\n",
      "3028 Train Loss 6.689438\n",
      "Loss  2.5904825 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5884454 C_bot  0.15 k_c 0.0\n",
      "3029 Train Loss 6.686649\n",
      "Loss  2.5884454 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5865097 C_bot  0.15 k_c 0.0\n",
      "3030 Train Loss 6.683964\n",
      "Loss  2.5865097 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5844886 C_bot  0.15 k_c 0.0\n",
      "3031 Train Loss 6.681202\n",
      "Loss  2.5844886 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5825837 C_bot  0.15 k_c 0.0\n",
      "3032 Train Loss 6.67855\n",
      "Loss  2.5825837 C_bot  0.15 k_c 0.0\n",
      "Loss  2.580622 C_bot  0.15 k_c 0.0\n",
      "3033 Train Loss 6.675857\n",
      "Loss  2.580622 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5786462 C_bot  0.15 k_c 0.0\n",
      "3034 Train Loss 6.6731396\n",
      "Loss  2.5786462 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5768335 C_bot  0.15 k_c 0.0\n",
      "3035 Train Loss 6.6706038\n",
      "Loss  2.5768335 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5748703 C_bot  0.15 k_c 0.0\n",
      "3036 Train Loss 6.6679053\n",
      "Loss  2.5748703 C_bot  0.15 k_c 0.0\n",
      "Loss  2.572854 C_bot  0.15 k_c 0.0\n",
      "3037 Train Loss 6.665174\n",
      "Loss  2.572854 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5709777 C_bot  0.15 k_c 0.0\n",
      "3038 Train Loss 6.662568\n",
      "Loss  2.5709777 C_bot  0.15 k_c 0.0\n",
      "Loss  2.568889 C_bot  0.15 k_c 0.0\n",
      "3039 Train Loss 6.6597743\n",
      "Loss  2.568889 C_bot  0.15 k_c 0.0\n",
      "Loss  2.566965 C_bot  0.15 k_c 0.0\n",
      "3040 Train Loss 6.657122\n",
      "Loss  2.566965 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5650468 C_bot  0.15 k_c 0.0\n",
      "3041 Train Loss 6.6545124\n",
      "Loss  2.5650468 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5632105 C_bot  0.15 k_c 0.0\n",
      "3042 Train Loss 6.651947\n",
      "Loss  2.5632105 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5611758 C_bot  0.15 k_c 0.0\n",
      "3043 Train Loss 6.6492367\n",
      "Loss  2.5611758 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5592458 C_bot  0.15 k_c 0.0\n",
      "3044 Train Loss 6.6465726\n",
      "Loss  2.5592458 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5573258 C_bot  0.15 k_c 0.0\n",
      "3045 Train Loss 6.6439996\n",
      "Loss  2.5573258 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5554483 C_bot  0.15 k_c 0.0\n",
      "3046 Train Loss 6.641374\n",
      "Loss  2.5554483 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5534382 C_bot  0.15 k_c 0.0\n",
      "3047 Train Loss 6.6387434\n",
      "Loss  2.5534382 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5516584 C_bot  0.15 k_c 0.0\n",
      "3048 Train Loss 6.636189\n",
      "Loss  2.5516584 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5497148 C_bot  0.15 k_c 0.0\n",
      "3049 Train Loss 6.6336737\n",
      "Loss  2.5497148 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5479517 C_bot  0.15 k_c 0.0\n",
      "3050 Train Loss 6.6310887\n",
      "Loss  2.5479517 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5460687 C_bot  0.15 k_c 0.0\n",
      "3051 Train Loss 6.6287084\n",
      "Loss  2.5460687 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5445826 C_bot  0.15 k_c 0.0\n",
      "3052 Train Loss 6.626319\n",
      "Loss  2.5445826 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5427153 C_bot  0.15 k_c 0.0\n",
      "3053 Train Loss 6.624077\n",
      "Loss  2.5427153 C_bot  0.15 k_c 0.0\n",
      "Loss  2.541772 C_bot  0.15 k_c 0.0\n",
      "3054 Train Loss 6.6220846\n",
      "Loss  2.541772 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5404353 C_bot  0.15 k_c 0.0\n",
      "3055 Train Loss 6.6205797\n",
      "Loss  2.5404353 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5411432 C_bot  0.15 k_c 0.0\n",
      "3056 Train Loss 6.619983\n",
      "Loss  2.5411432 C_bot  0.15 k_c 0.0\n",
      "Loss  2.541695 C_bot  0.15 k_c 0.0\n",
      "3057 Train Loss 6.6207237\n",
      "Loss  2.541695 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5467663 C_bot  0.15 k_c 0.0\n",
      "3058 Train Loss 6.624033\n",
      "Loss  2.5467663 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5540104 C_bot  0.15 k_c 0.0\n",
      "3059 Train Loss 6.632105\n",
      "Loss  2.5540104 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5732179 C_bot  0.15 k_c 0.0\n",
      "3060 Train Loss 6.6487217\n",
      "Loss  2.5732179 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6037688 C_bot  0.15 k_c 0.0\n",
      "3061 Train Loss 6.6812754\n",
      "Loss  2.6037688 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6694946 C_bot  0.15 k_c 0.0\n",
      "3062 Train Loss 6.7428923\n",
      "Loss  2.6694946 C_bot  0.15 k_c 0.0\n",
      "Loss  2.782427 C_bot  0.15 k_c 0.0\n",
      "3063 Train Loss 6.860042\n",
      "Loss  2.782427 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0099583 C_bot  0.15 k_c 0.0\n",
      "3064 Train Loss 7.0806704\n",
      "Loss  3.0099583 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4235811 C_bot  0.15 k_c 0.0\n",
      "3065 Train Loss 7.5028253\n",
      "Loss  3.4235811 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2342753 C_bot  0.15 k_c 0.0\n",
      "3066 Train Loss 8.301493\n",
      "Loss  4.2342753 C_bot  0.15 k_c 0.0\n",
      "Loss  5.7739005 C_bot  0.15 k_c 0.0\n",
      "3067 Train Loss 9.85846\n",
      "Loss  5.7739005 C_bot  0.15 k_c 0.0\n",
      "Loss  8.751262 C_bot  0.15 k_c 0.0\n",
      "3068 Train Loss 12.814595\n",
      "Loss  8.751262 C_bot  0.15 k_c 0.0\n",
      "Loss  14.605702 C_bot  0.15 k_c 0.0\n",
      "3069 Train Loss 18.705732\n",
      "Loss  14.605702 C_bot  0.15 k_c 0.0\n",
      "Loss  25.699633 C_bot  0.15 k_c 0.0\n",
      "3070 Train Loss 29.762684\n",
      "Loss  25.699633 C_bot  0.15 k_c 0.0\n",
      "Loss  48.054184 C_bot  0.15 k_c 0.0\n",
      "3071 Train Loss 52.20151\n",
      "Loss  48.054184 C_bot  0.15 k_c 0.0\n",
      "Loss  87.844154 C_bot  0.15 k_c 0.0\n",
      "3072 Train Loss 91.92777\n",
      "Loss  87.844154 C_bot  0.15 k_c 0.0\n",
      "Loss  166.82285 C_bot  0.15 k_c 0.0\n",
      "3073 Train Loss 171.12354\n",
      "Loss  166.82285 C_bot  0.15 k_c 0.0\n",
      "Loss  280.0595 C_bot  0.15 k_c 0.0\n",
      "3074 Train Loss 284.22473\n",
      "Loss  280.0595 C_bot  0.15 k_c 0.0\n",
      "Loss  465.7864 C_bot  0.15 k_c 0.0\n",
      "3075 Train Loss 470.51553\n",
      "Loss  465.7864 C_bot  0.15 k_c 0.0\n",
      "Loss  569.5238 C_bot  0.15 k_c 0.0\n",
      "3076 Train Loss 573.7974\n",
      "Loss  569.5238 C_bot  0.15 k_c 0.0\n",
      "Loss  583.3193 C_bot  0.15 k_c 0.0\n",
      "3077 Train Loss 588.5088\n",
      "Loss  583.3193 C_bot  0.15 k_c 0.0\n",
      "Loss  321.36804 C_bot  0.15 k_c 0.0\n",
      "3078 Train Loss 325.66327\n",
      "Loss  321.36804 C_bot  0.15 k_c 0.0\n",
      "Loss  67.924515 C_bot  0.15 k_c 0.0\n",
      "3079 Train Loss 72.751205\n",
      "Loss  67.924515 C_bot  0.15 k_c 0.0\n",
      "Loss  15.113405 C_bot  0.15 k_c 0.0\n",
      "3080 Train Loss 19.916649\n",
      "Loss  15.113405 C_bot  0.15 k_c 0.0\n",
      "Loss  162.31721 C_bot  0.15 k_c 0.0\n",
      "3081 Train Loss 166.87106\n",
      "Loss  162.31721 C_bot  0.15 k_c 0.0\n",
      "Loss  300.95972 C_bot  0.15 k_c 0.0\n",
      "3082 Train Loss 306.56763\n",
      "Loss  300.95972 C_bot  0.15 k_c 0.0\n",
      "Loss  217.68236 C_bot  0.15 k_c 0.0\n",
      "3083 Train Loss 222.31079\n",
      "Loss  217.68236 C_bot  0.15 k_c 0.0\n",
      "Loss  59.879726 C_bot  0.15 k_c 0.0\n",
      "3084 Train Loss 65.07615\n",
      "Loss  59.879726 C_bot  0.15 k_c 0.0\n",
      "Loss  7.8626924 C_bot  0.15 k_c 0.0\n",
      "3085 Train Loss 12.833227\n",
      "Loss  7.8626924 C_bot  0.15 k_c 0.0\n",
      "Loss  98.0028 C_bot  0.15 k_c 0.0\n",
      "3086 Train Loss 102.66709\n",
      "Loss  98.0028 C_bot  0.15 k_c 0.0\n",
      "Loss  159.97806 C_bot  0.15 k_c 0.0\n",
      "3087 Train Loss 165.27176\n",
      "Loss  159.97806 C_bot  0.15 k_c 0.0\n",
      "Loss  79.03499 C_bot  0.15 k_c 0.0\n",
      "3088 Train Loss 83.639885\n",
      "Loss  79.03499 C_bot  0.15 k_c 0.0\n",
      "Loss  4.605719 C_bot  0.15 k_c 0.0\n",
      "3089 Train Loss 9.366603\n",
      "Loss  4.605719 C_bot  0.15 k_c 0.0\n",
      "Loss  41.675327 C_bot  0.15 k_c 0.0\n",
      "3090 Train Loss 46.526867\n",
      "Loss  41.675327 C_bot  0.15 k_c 0.0\n",
      "Loss  92.281525 C_bot  0.15 k_c 0.0\n",
      "3091 Train Loss 96.75436\n",
      "Loss  92.281525 C_bot  0.15 k_c 0.0\n",
      "Loss  59.16974 C_bot  0.15 k_c 0.0\n",
      "3092 Train Loss 63.965076\n",
      "Loss  59.16974 C_bot  0.15 k_c 0.0\n",
      "Loss  5.629292 C_bot  0.15 k_c 0.0\n",
      "3093 Train Loss 10.198165\n",
      "Loss  5.629292 C_bot  0.15 k_c 0.0\n",
      "Loss  25.80297 C_bot  0.15 k_c 0.0\n",
      "3094 Train Loss 30.352901\n",
      "Loss  25.80297 C_bot  0.15 k_c 0.0\n",
      "Loss  62.926056 C_bot  0.15 k_c 0.0\n",
      "3095 Train Loss 67.69639\n",
      "Loss  62.926056 C_bot  0.15 k_c 0.0\n",
      "Loss  38.235683 C_bot  0.15 k_c 0.0\n",
      "3096 Train Loss 42.799053\n",
      "Loss  38.235683 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2569585 C_bot  0.15 k_c 0.0\n",
      "3097 Train Loss 8.859635\n",
      "Loss  4.2569585 C_bot  0.15 k_c 0.0\n",
      "Loss  19.255302 C_bot  0.15 k_c 0.0\n",
      "3098 Train Loss 23.877068\n",
      "Loss  19.255302 C_bot  0.15 k_c 0.0\n",
      "Loss  41.111923 C_bot  0.15 k_c 0.0\n",
      "3099 Train Loss 45.62643\n",
      "Loss  41.111923 C_bot  0.15 k_c 0.0\n",
      "Loss  24.683853 C_bot  0.15 k_c 0.0\n",
      "3100 Train Loss 29.293026\n",
      "Loss  24.683853 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5051875 C_bot  0.15 k_c 0.0\n",
      "3101 Train Loss 8.037789\n",
      "Loss  3.5051875 C_bot  0.15 k_c 0.0\n",
      "Loss  15.224505 C_bot  0.15 k_c 0.0\n",
      "3102 Train Loss 19.718906\n",
      "Loss  15.224505 C_bot  0.15 k_c 0.0\n",
      "Loss  28.995813 C_bot  0.15 k_c 0.0\n",
      "3103 Train Loss 33.57187\n",
      "Loss  28.995813 C_bot  0.15 k_c 0.0\n",
      "Loss  15.566545 C_bot  0.15 k_c 0.0\n",
      "3104 Train Loss 20.004599\n",
      "Loss  15.566545 C_bot  0.15 k_c 0.0\n",
      "Loss  3.120545 C_bot  0.15 k_c 0.0\n",
      "3105 Train Loss 7.568641\n",
      "Loss  3.120545 C_bot  0.15 k_c 0.0\n",
      "Loss  12.3515415 C_bot  0.15 k_c 0.0\n",
      "3106 Train Loss 16.828074\n",
      "Loss  12.3515415 C_bot  0.15 k_c 0.0\n",
      "Loss  19.703325 C_bot  0.15 k_c 0.0\n",
      "3107 Train Loss 24.071997\n",
      "Loss  19.703325 C_bot  0.15 k_c 0.0\n",
      "Loss  10.111845 C_bot  0.15 k_c 0.0\n",
      "3108 Train Loss 14.558711\n",
      "Loss  10.111845 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1002915 C_bot  0.15 k_c 0.0\n",
      "3109 Train Loss 7.4909487\n",
      "Loss  3.1002915 C_bot  0.15 k_c 0.0\n",
      "Loss  10.031916 C_bot  0.15 k_c 0.0\n",
      "3110 Train Loss 14.368137\n",
      "Loss  10.031916 C_bot  0.15 k_c 0.0\n",
      "Loss  14.077494 C_bot  0.15 k_c 0.0\n",
      "3111 Train Loss 18.487032\n",
      "Loss  14.077494 C_bot  0.15 k_c 0.0\n",
      "Loss  7.018553 C_bot  0.15 k_c 0.0\n",
      "3112 Train Loss 11.330832\n",
      "Loss  7.018553 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1502035 C_bot  0.15 k_c 0.0\n",
      "3113 Train Loss 7.4723682\n",
      "Loss  3.1502035 C_bot  0.15 k_c 0.0\n",
      "Loss  8.108772 C_bot  0.15 k_c 0.0\n",
      "3114 Train Loss 12.461642\n",
      "Loss  8.108772 C_bot  0.15 k_c 0.0\n",
      "Loss  10.09978 C_bot  0.15 k_c 0.0\n",
      "3115 Train Loss 14.385067\n",
      "Loss  10.09978 C_bot  0.15 k_c 0.0\n",
      "Loss  5.216585 C_bot  0.15 k_c 0.0\n",
      "3116 Train Loss 9.542804\n",
      "Loss  5.216585 C_bot  0.15 k_c 0.0\n",
      "Loss  3.123088 C_bot  0.15 k_c 0.0\n",
      "3117 Train Loss 7.4267287\n",
      "Loss  3.123088 C_bot  0.15 k_c 0.0\n",
      "Loss  6.566979 C_bot  0.15 k_c 0.0\n",
      "3118 Train Loss 10.839875\n",
      "Loss  6.566979 C_bot  0.15 k_c 0.0\n",
      "Loss  7.6127768 C_bot  0.15 k_c 0.0\n",
      "3119 Train Loss 11.921659\n",
      "Loss  7.6127768 C_bot  0.15 k_c 0.0\n",
      "Loss  4.305252 C_bot  0.15 k_c 0.0\n",
      "3120 Train Loss 8.5725355\n",
      "Loss  4.305252 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0796473 C_bot  0.15 k_c 0.0\n",
      "3121 Train Loss 7.3495116\n",
      "Loss  3.0796473 C_bot  0.15 k_c 0.0\n",
      "Loss  5.373476 C_bot  0.15 k_c 0.0\n",
      "3122 Train Loss 9.658618\n",
      "Loss  5.373476 C_bot  0.15 k_c 0.0\n",
      "Loss  5.976216 C_bot  0.15 k_c 0.0\n",
      "3123 Train Loss 10.228476\n",
      "Loss  5.976216 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7681177 C_bot  0.15 k_c 0.0\n",
      "3124 Train Loss 8.035979\n",
      "Loss  3.7681177 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9839106 C_bot  0.15 k_c 0.0\n",
      "3125 Train Loss 7.240172\n",
      "Loss  2.9839106 C_bot  0.15 k_c 0.0\n",
      "Loss  4.5016894 C_bot  0.15 k_c 0.0\n",
      "3126 Train Loss 8.741173\n",
      "Loss  4.5016894 C_bot  0.15 k_c 0.0\n",
      "Loss  4.91657 C_bot  0.15 k_c 0.0\n",
      "3127 Train Loss 9.172218\n",
      "Loss  4.91657 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4905753 C_bot  0.15 k_c 0.0\n",
      "3128 Train Loss 7.724355\n",
      "Loss  3.4905753 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9035807 C_bot  0.15 k_c 0.0\n",
      "3129 Train Loss 7.137412\n",
      "Loss  2.9035807 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8597453 C_bot  0.15 k_c 0.0\n",
      "3130 Train Loss 8.0996065\n",
      "Loss  3.8597453 C_bot  0.15 k_c 0.0\n",
      "Loss  4.221116 C_bot  0.15 k_c 0.0\n",
      "3131 Train Loss 8.441288\n",
      "Loss  4.221116 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3024228 C_bot  0.15 k_c 0.0\n",
      "3132 Train Loss 7.5322595\n",
      "Loss  3.3024228 C_bot  0.15 k_c 0.0\n",
      "Loss  2.838295 C_bot  0.15 k_c 0.0\n",
      "3133 Train Loss 7.0601683\n",
      "Loss  2.838295 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4345613 C_bot  0.15 k_c 0.0\n",
      "3134 Train Loss 7.6470246\n",
      "Loss  3.4345613 C_bot  0.15 k_c 0.0\n",
      "Loss  3.7400415 C_bot  0.15 k_c 0.0\n",
      "3135 Train Loss 7.963324\n",
      "Loss  3.7400415 C_bot  0.15 k_c 0.0\n",
      "Loss  3.198743 C_bot  0.15 k_c 0.0\n",
      "3136 Train Loss 7.406824\n",
      "Loss  3.198743 C_bot  0.15 k_c 0.0\n",
      "Loss  2.804534 C_bot  0.15 k_c 0.0\n",
      "3137 Train Loss 7.014243\n",
      "Loss  2.804534 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1316004 C_bot  0.15 k_c 0.0\n",
      "3138 Train Loss 7.34328\n",
      "Loss  3.1316004 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4171565 C_bot  0.15 k_c 0.0\n",
      "3139 Train Loss 7.6163697\n",
      "Loss  3.4171565 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1038733 C_bot  0.15 k_c 0.0\n",
      "3140 Train Loss 7.3103175\n",
      "Loss  3.1038733 C_bot  0.15 k_c 0.0\n",
      "Loss  2.788051 C_bot  0.15 k_c 0.0\n",
      "3141 Train Loss 6.9875336\n",
      "Loss  2.788051 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9429135 C_bot  0.15 k_c 0.0\n",
      "3142 Train Loss 7.138257\n",
      "Loss  2.9429135 C_bot  0.15 k_c 0.0\n",
      "Loss  3.165294 C_bot  0.15 k_c 0.0\n",
      "3143 Train Loss 7.3659186\n",
      "Loss  3.165294 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0328286 C_bot  0.15 k_c 0.0\n",
      "3144 Train Loss 7.2237735\n",
      "Loss  3.0328286 C_bot  0.15 k_c 0.0\n",
      "Loss  2.77955 C_bot  0.15 k_c 0.0\n",
      "3145 Train Loss 6.9728727\n",
      "Loss  2.77955 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8140583 C_bot  0.15 k_c 0.0\n",
      "3146 Train Loss 7.0062885\n",
      "Loss  2.8140583 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9898322 C_bot  0.15 k_c 0.0\n",
      "3147 Train Loss 7.175742\n",
      "Loss  2.9898322 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9512656 C_bot  0.15 k_c 0.0\n",
      "3148 Train Loss 7.141782\n",
      "Loss  2.9512656 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7801163 C_bot  0.15 k_c 0.0\n",
      "3149 Train Loss 6.964736\n",
      "Loss  2.7801163 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7455864 C_bot  0.15 k_c 0.0\n",
      "3150 Train Loss 6.929282\n",
      "Loss  2.7455864 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8521814 C_bot  0.15 k_c 0.0\n",
      "3151 Train Loss 7.03754\n",
      "Loss  2.8521814 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8824136 C_bot  0.15 k_c 0.0\n",
      "3152 Train Loss 7.0614285\n",
      "Loss  2.8824136 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7724402 C_bot  0.15 k_c 0.0\n",
      "3153 Train Loss 6.953978\n",
      "Loss  2.7724402 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7094557 C_bot  0.15 k_c 0.0\n",
      "3154 Train Loss 6.8880215\n",
      "Loss  2.7094557 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7629933 C_bot  0.15 k_c 0.0\n",
      "3155 Train Loss 6.938578\n",
      "Loss  2.7629933 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8057382 C_bot  0.15 k_c 0.0\n",
      "3156 Train Loss 6.983585\n",
      "Loss  2.8057382 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7611825 C_bot  0.15 k_c 0.0\n",
      "3157 Train Loss 6.9338613\n",
      "Loss  2.7611825 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6935084 C_bot  0.15 k_c 0.0\n",
      "3158 Train Loss 6.8668003\n",
      "Loss  2.6935084 C_bot  0.15 k_c 0.0\n",
      "Loss  2.700019 C_bot  0.15 k_c 0.0\n",
      "3159 Train Loss 6.872256\n",
      "Loss  2.700019 C_bot  0.15 k_c 0.0\n",
      "Loss  2.743057 C_bot  0.15 k_c 0.0\n",
      "3160 Train Loss 6.911416\n",
      "Loss  2.743057 C_bot  0.15 k_c 0.0\n",
      "Loss  2.731729 C_bot  0.15 k_c 0.0\n",
      "3161 Train Loss 6.9018655\n",
      "Loss  2.731729 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6858275 C_bot  0.15 k_c 0.0\n",
      "3162 Train Loss 6.8521013\n",
      "Loss  2.6858275 C_bot  0.15 k_c 0.0\n",
      "Loss  2.665396 C_bot  0.15 k_c 0.0\n",
      "3163 Train Loss 6.830863\n",
      "Loss  2.665396 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6853344 C_bot  0.15 k_c 0.0\n",
      "3164 Train Loss 6.850749\n",
      "Loss  2.6853344 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6991937 C_bot  0.15 k_c 0.0\n",
      "3165 Train Loss 6.8607903\n",
      "Loss  2.6991937 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6721685 C_bot  0.15 k_c 0.0\n",
      "3166 Train Loss 6.834722\n",
      "Loss  2.6721685 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6456606 C_bot  0.15 k_c 0.0\n",
      "3167 Train Loss 6.8055553\n",
      "Loss  2.6456606 C_bot  0.15 k_c 0.0\n",
      "Loss  2.646409 C_bot  0.15 k_c 0.0\n",
      "3168 Train Loss 6.804654\n",
      "Loss  2.646409 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6577404 C_bot  0.15 k_c 0.0\n",
      "3169 Train Loss 6.816152\n",
      "Loss  2.6577404 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6542692 C_bot  0.15 k_c 0.0\n",
      "3170 Train Loss 6.809396\n",
      "Loss  2.6542692 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6311116 C_bot  0.15 k_c 0.0\n",
      "3171 Train Loss 6.786407\n",
      "Loss  2.6311116 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6188283 C_bot  0.15 k_c 0.0\n",
      "3172 Train Loss 6.77234\n",
      "Loss  2.6188283 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6232839 C_bot  0.15 k_c 0.0\n",
      "3173 Train Loss 6.774884\n",
      "Loss  2.6232839 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6249912 C_bot  0.15 k_c 0.0\n",
      "3174 Train Loss 6.776682\n",
      "Loss  2.6249912 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6160808 C_bot  0.15 k_c 0.0\n",
      "3175 Train Loss 6.765082\n",
      "Loss  2.6160808 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6000896 C_bot  0.15 k_c 0.0\n",
      "3176 Train Loss 6.748767\n",
      "Loss  2.6000896 C_bot  0.15 k_c 0.0\n",
      "Loss  2.593939 C_bot  0.15 k_c 0.0\n",
      "3177 Train Loss 6.741322\n",
      "Loss  2.593939 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5960896 C_bot  0.15 k_c 0.0\n",
      "3178 Train Loss 6.741592\n",
      "Loss  2.5960896 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5927377 C_bot  0.15 k_c 0.0\n",
      "3179 Train Loss 6.7381773\n",
      "Loss  2.5927377 C_bot  0.15 k_c 0.0\n",
      "Loss  2.583771 C_bot  0.15 k_c 0.0\n",
      "3180 Train Loss 6.727022\n",
      "Loss  2.583771 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5725486 C_bot  0.15 k_c 0.0\n",
      "3181 Train Loss 6.7152247\n",
      "Loss  2.5725486 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5681112 C_bot  0.15 k_c 0.0\n",
      "3182 Train Loss 6.7097535\n",
      "Loss  2.5681112 C_bot  0.15 k_c 0.0\n",
      "Loss  2.567648 C_bot  0.15 k_c 0.0\n",
      "3183 Train Loss 6.7075667\n",
      "Loss  2.567648 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5623312 C_bot  0.15 k_c 0.0\n",
      "3184 Train Loss 6.702029\n",
      "Loss  2.5623312 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5546107 C_bot  0.15 k_c 0.0\n",
      "3185 Train Loss 6.692471\n",
      "Loss  2.5546107 C_bot  0.15 k_c 0.0\n",
      "Loss  2.546179 C_bot  0.15 k_c 0.0\n",
      "3186 Train Loss 6.6833463\n",
      "Loss  2.546179 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5416489 C_bot  0.15 k_c 0.0\n",
      "3187 Train Loss 6.677881\n",
      "Loss  2.5416489 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5394266 C_bot  0.15 k_c 0.0\n",
      "3188 Train Loss 6.6741176\n",
      "Loss  2.5394266 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5338786 C_bot  0.15 k_c 0.0\n",
      "3189 Train Loss 6.6682253\n",
      "Loss  2.5338786 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5273955 C_bot  0.15 k_c 0.0\n",
      "3190 Train Loss 6.660145\n",
      "Loss  2.5273955 C_bot  0.15 k_c 0.0\n",
      "Loss  2.52023 C_bot  0.15 k_c 0.0\n",
      "3191 Train Loss 6.6522627\n",
      "Loss  2.52023 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5155334 C_bot  0.15 k_c 0.0\n",
      "3192 Train Loss 6.646643\n",
      "Loss  2.5155334 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5124848 C_bot  0.15 k_c 0.0\n",
      "3193 Train Loss 6.642226\n",
      "Loss  2.5124848 C_bot  0.15 k_c 0.0\n",
      "Loss  2.507342 C_bot  0.15 k_c 0.0\n",
      "3194 Train Loss 6.6366115\n",
      "Loss  2.507342 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5017245 C_bot  0.15 k_c 0.0\n",
      "3195 Train Loss 6.629549\n",
      "Loss  2.5017245 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4954665 C_bot  0.15 k_c 0.0\n",
      "3196 Train Loss 6.622572\n",
      "Loss  2.4954665 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4907758 C_bot  0.15 k_c 0.0\n",
      "3197 Train Loss 6.616914\n",
      "Loss  2.4907758 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4870605 C_bot  0.15 k_c 0.0\n",
      "3198 Train Loss 6.611977\n",
      "Loss  2.4870605 C_bot  0.15 k_c 0.0\n",
      "Loss  2.482452 C_bot  0.15 k_c 0.0\n",
      "3199 Train Loss 6.606767\n",
      "Loss  2.482452 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4777522 C_bot  0.15 k_c 0.0\n",
      "3200 Train Loss 6.600711\n",
      "Loss  2.4777522 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4722402 C_bot  0.15 k_c 0.0\n",
      "3201 Train Loss 6.594466\n",
      "Loss  2.4722402 C_bot  0.15 k_c 0.0\n",
      "Loss  2.467559 C_bot  0.15 k_c 0.0\n",
      "3202 Train Loss 6.588737\n",
      "Loss  2.467559 C_bot  0.15 k_c 0.0\n",
      "Loss  2.463661 C_bot  0.15 k_c 0.0\n",
      "3203 Train Loss 6.583727\n",
      "Loss  2.463661 C_bot  0.15 k_c 0.0\n",
      "Loss  2.459461 C_bot  0.15 k_c 0.0\n",
      "3204 Train Loss 6.578796\n",
      "Loss  2.459461 C_bot  0.15 k_c 0.0\n",
      "Loss  2.455499 C_bot  0.15 k_c 0.0\n",
      "3205 Train Loss 6.5735464\n",
      "Loss  2.455499 C_bot  0.15 k_c 0.0\n",
      "Loss  2.450566 C_bot  0.15 k_c 0.0\n",
      "3206 Train Loss 6.567862\n",
      "Loss  2.450566 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4462698 C_bot  0.15 k_c 0.0\n",
      "3207 Train Loss 6.5624413\n",
      "Loss  2.4462698 C_bot  0.15 k_c 0.0\n",
      "Loss  2.442231 C_bot  0.15 k_c 0.0\n",
      "3208 Train Loss 6.5573816\n",
      "Loss  2.442231 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4383726 C_bot  0.15 k_c 0.0\n",
      "3209 Train Loss 6.5526533\n",
      "Loss  2.4383726 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4346776 C_bot  0.15 k_c 0.0\n",
      "3210 Train Loss 6.547737\n",
      "Loss  2.4346776 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4303944 C_bot  0.15 k_c 0.0\n",
      "3211 Train Loss 6.542652\n",
      "Loss  2.4303944 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4264822 C_bot  0.15 k_c 0.0\n",
      "3212 Train Loss 6.5375576\n",
      "Loss  2.4264822 C_bot  0.15 k_c 0.0\n",
      "Loss  2.422403 C_bot  0.15 k_c 0.0\n",
      "3213 Train Loss 6.5325217\n",
      "Loss  2.422403 C_bot  0.15 k_c 0.0\n",
      "Loss  2.418513 C_bot  0.15 k_c 0.0\n",
      "3214 Train Loss 6.5276346\n",
      "Loss  2.418513 C_bot  0.15 k_c 0.0\n",
      "Loss  2.414895 C_bot  0.15 k_c 0.0\n",
      "3215 Train Loss 6.5228643\n",
      "Loss  2.414895 C_bot  0.15 k_c 0.0\n",
      "Loss  2.411088 C_bot  0.15 k_c 0.0\n",
      "3216 Train Loss 6.5181828\n",
      "Loss  2.411088 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4074354 C_bot  0.15 k_c 0.0\n",
      "3217 Train Loss 6.513324\n",
      "Loss  2.4074354 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4033985 C_bot  0.15 k_c 0.0\n",
      "3218 Train Loss 6.5083685\n",
      "Loss  2.4033985 C_bot  0.15 k_c 0.0\n",
      "Loss  2.399752 C_bot  0.15 k_c 0.0\n",
      "3219 Train Loss 6.503618\n",
      "Loss  2.399752 C_bot  0.15 k_c 0.0\n",
      "Loss  2.396116 C_bot  0.15 k_c 0.0\n",
      "3220 Train Loss 6.4989214\n",
      "Loss  2.396116 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3925254 C_bot  0.15 k_c 0.0\n",
      "3221 Train Loss 6.494361\n",
      "Loss  2.3925254 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3889802 C_bot  0.15 k_c 0.0\n",
      "3222 Train Loss 6.489653\n",
      "Loss  2.3889802 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3851984 C_bot  0.15 k_c 0.0\n",
      "3223 Train Loss 6.4849477\n",
      "Loss  2.3851984 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3817694 C_bot  0.15 k_c 0.0\n",
      "3224 Train Loss 6.4803686\n",
      "Loss  2.3817694 C_bot  0.15 k_c 0.0\n",
      "Loss  2.377964 C_bot  0.15 k_c 0.0\n",
      "3225 Train Loss 6.4755783\n",
      "Loss  2.377964 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3744435 C_bot  0.15 k_c 0.0\n",
      "3226 Train Loss 6.4710054\n",
      "Loss  2.3744435 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3709629 C_bot  0.15 k_c 0.0\n",
      "3227 Train Loss 6.4664445\n",
      "Loss  2.3709629 C_bot  0.15 k_c 0.0\n",
      "Loss  2.367348 C_bot  0.15 k_c 0.0\n",
      "3228 Train Loss 6.4618692\n",
      "Loss  2.367348 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3639972 C_bot  0.15 k_c 0.0\n",
      "3229 Train Loss 6.4573946\n",
      "Loss  2.3639972 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3603384 C_bot  0.15 k_c 0.0\n",
      "3230 Train Loss 6.452797\n",
      "Loss  2.3603384 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3568077 C_bot  0.15 k_c 0.0\n",
      "3231 Train Loss 6.4481797\n",
      "Loss  2.3568077 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3532915 C_bot  0.15 k_c 0.0\n",
      "3232 Train Loss 6.4436836\n",
      "Loss  2.3532915 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3498256 C_bot  0.15 k_c 0.0\n",
      "3233 Train Loss 6.4392138\n",
      "Loss  2.3498256 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3464594 C_bot  0.15 k_c 0.0\n",
      "3234 Train Loss 6.434816\n",
      "Loss  2.3464594 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3429947 C_bot  0.15 k_c 0.0\n",
      "3235 Train Loss 6.4304194\n",
      "Loss  2.3429947 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3396184 C_bot  0.15 k_c 0.0\n",
      "3236 Train Loss 6.4259996\n",
      "Loss  2.3396184 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3361826 C_bot  0.15 k_c 0.0\n",
      "3237 Train Loss 6.421658\n",
      "Loss  2.3361826 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3327656 C_bot  0.15 k_c 0.0\n",
      "3238 Train Loss 6.417236\n",
      "Loss  2.3327656 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3294225 C_bot  0.15 k_c 0.0\n",
      "3239 Train Loss 6.4129734\n",
      "Loss  2.3294225 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3260078 C_bot  0.15 k_c 0.0\n",
      "3240 Train Loss 6.40862\n",
      "Loss  2.3260078 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3226256 C_bot  0.15 k_c 0.0\n",
      "3241 Train Loss 6.4042964\n",
      "Loss  2.3226256 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3192458 C_bot  0.15 k_c 0.0\n",
      "3242 Train Loss 6.4000397\n",
      "Loss  2.3192458 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3160045 C_bot  0.15 k_c 0.0\n",
      "3243 Train Loss 6.3958573\n",
      "Loss  2.3160045 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3127275 C_bot  0.15 k_c 0.0\n",
      "3244 Train Loss 6.391736\n",
      "Loss  2.3127275 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3094425 C_bot  0.15 k_c 0.0\n",
      "3245 Train Loss 6.38754\n",
      "Loss  2.3094425 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3061254 C_bot  0.15 k_c 0.0\n",
      "3246 Train Loss 6.3833857\n",
      "Loss  2.3061254 C_bot  0.15 k_c 0.0\n",
      "Loss  2.302918 C_bot  0.15 k_c 0.0\n",
      "3247 Train Loss 6.3793173\n",
      "Loss  2.302918 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2996902 C_bot  0.15 k_c 0.0\n",
      "3248 Train Loss 6.375246\n",
      "Loss  2.2996902 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2964723 C_bot  0.15 k_c 0.0\n",
      "3249 Train Loss 6.3712177\n",
      "Loss  2.2964723 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2932353 C_bot  0.15 k_c 0.0\n",
      "3250 Train Loss 6.3671384\n",
      "Loss  2.2932353 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2900338 C_bot  0.15 k_c 0.0\n",
      "3251 Train Loss 6.3631635\n",
      "Loss  2.2900338 C_bot  0.15 k_c 0.0\n",
      "Loss  2.286974 C_bot  0.15 k_c 0.0\n",
      "3252 Train Loss 6.3592777\n",
      "Loss  2.286974 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2838018 C_bot  0.15 k_c 0.0\n",
      "3253 Train Loss 6.355348\n",
      "Loss  2.2838018 C_bot  0.15 k_c 0.0\n",
      "Loss  2.280597 C_bot  0.15 k_c 0.0\n",
      "3254 Train Loss 6.351349\n",
      "Loss  2.280597 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2775009 C_bot  0.15 k_c 0.0\n",
      "3255 Train Loss 6.347498\n",
      "Loss  2.2775009 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2743664 C_bot  0.15 k_c 0.0\n",
      "3256 Train Loss 6.343604\n",
      "Loss  2.2743664 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2713623 C_bot  0.15 k_c 0.0\n",
      "3257 Train Loss 6.3398447\n",
      "Loss  2.2713623 C_bot  0.15 k_c 0.0\n",
      "Loss  2.268347 C_bot  0.15 k_c 0.0\n",
      "3258 Train Loss 6.3361025\n",
      "Loss  2.268347 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2652884 C_bot  0.15 k_c 0.0\n",
      "3259 Train Loss 6.332293\n",
      "Loss  2.2652884 C_bot  0.15 k_c 0.0\n",
      "Loss  2.262267 C_bot  0.15 k_c 0.0\n",
      "3260 Train Loss 6.328564\n",
      "Loss  2.262267 C_bot  0.15 k_c 0.0\n",
      "Loss  2.259288 C_bot  0.15 k_c 0.0\n",
      "3261 Train Loss 6.3248487\n",
      "Loss  2.259288 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2561936 C_bot  0.15 k_c 0.0\n",
      "3262 Train Loss 6.3210535\n",
      "Loss  2.2561936 C_bot  0.15 k_c 0.0\n",
      "Loss  2.253314 C_bot  0.15 k_c 0.0\n",
      "3263 Train Loss 6.317457\n",
      "Loss  2.253314 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2503703 C_bot  0.15 k_c 0.0\n",
      "3264 Train Loss 6.3138165\n",
      "Loss  2.2503703 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2474694 C_bot  0.15 k_c 0.0\n",
      "3265 Train Loss 6.3102183\n",
      "Loss  2.2474694 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2444985 C_bot  0.15 k_c 0.0\n",
      "3266 Train Loss 6.306554\n",
      "Loss  2.2444985 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2415905 C_bot  0.15 k_c 0.0\n",
      "3267 Train Loss 6.302963\n",
      "Loss  2.2415905 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2386942 C_bot  0.15 k_c 0.0\n",
      "3268 Train Loss 6.29938\n",
      "Loss  2.2386942 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2358751 C_bot  0.15 k_c 0.0\n",
      "3269 Train Loss 6.295888\n",
      "Loss  2.2358751 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2330549 C_bot  0.15 k_c 0.0\n",
      "3270 Train Loss 6.2923884\n",
      "Loss  2.2330549 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2302852 C_bot  0.15 k_c 0.0\n",
      "3271 Train Loss 6.2889533\n",
      "Loss  2.2302852 C_bot  0.15 k_c 0.0\n",
      "Loss  2.22744 C_bot  0.15 k_c 0.0\n",
      "3272 Train Loss 6.2854395\n",
      "Loss  2.22744 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2247324 C_bot  0.15 k_c 0.0\n",
      "3273 Train Loss 6.2820697\n",
      "Loss  2.2247324 C_bot  0.15 k_c 0.0\n",
      "Loss  2.221852 C_bot  0.15 k_c 0.0\n",
      "3274 Train Loss 6.2785325\n",
      "Loss  2.221852 C_bot  0.15 k_c 0.0\n",
      "Loss  2.219049 C_bot  0.15 k_c 0.0\n",
      "3275 Train Loss 6.2750683\n",
      "Loss  2.219049 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2164016 C_bot  0.15 k_c 0.0\n",
      "3276 Train Loss 6.271776\n",
      "Loss  2.2164016 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2137303 C_bot  0.15 k_c 0.0\n",
      "3277 Train Loss 6.2684445\n",
      "Loss  2.2137303 C_bot  0.15 k_c 0.0\n",
      "Loss  2.210953 C_bot  0.15 k_c 0.0\n",
      "3278 Train Loss 6.2650337\n",
      "Loss  2.210953 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2082825 C_bot  0.15 k_c 0.0\n",
      "3279 Train Loss 6.2617025\n",
      "Loss  2.2082825 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2055593 C_bot  0.15 k_c 0.0\n",
      "3280 Train Loss 6.2583575\n",
      "Loss  2.2055593 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2029908 C_bot  0.15 k_c 0.0\n",
      "3281 Train Loss 6.255131\n",
      "Loss  2.2029908 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2002673 C_bot  0.15 k_c 0.0\n",
      "3282 Train Loss 6.2517934\n",
      "Loss  2.2002673 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1976662 C_bot  0.15 k_c 0.0\n",
      "3283 Train Loss 6.2485347\n",
      "Loss  2.1976662 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1949832 C_bot  0.15 k_c 0.0\n",
      "3284 Train Loss 6.245249\n",
      "Loss  2.1949832 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1924467 C_bot  0.15 k_c 0.0\n",
      "3285 Train Loss 6.242053\n",
      "Loss  2.1924467 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1899085 C_bot  0.15 k_c 0.0\n",
      "3286 Train Loss 6.23892\n",
      "Loss  2.1899085 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1873214 C_bot  0.15 k_c 0.0\n",
      "3287 Train Loss 6.235676\n",
      "Loss  2.1873214 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1847305 C_bot  0.15 k_c 0.0\n",
      "3288 Train Loss 6.232499\n",
      "Loss  2.1847305 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1821427 C_bot  0.15 k_c 0.0\n",
      "3289 Train Loss 6.2292547\n",
      "Loss  2.1821427 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1796668 C_bot  0.15 k_c 0.0\n",
      "3290 Train Loss 6.2262\n",
      "Loss  2.1796668 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1770935 C_bot  0.15 k_c 0.0\n",
      "3291 Train Loss 6.2229714\n",
      "Loss  2.1770935 C_bot  0.15 k_c 0.0\n",
      "Loss  2.174587 C_bot  0.15 k_c 0.0\n",
      "3292 Train Loss 6.2198925\n",
      "Loss  2.174587 C_bot  0.15 k_c 0.0\n",
      "Loss  2.17209 C_bot  0.15 k_c 0.0\n",
      "3293 Train Loss 6.2167416\n",
      "Loss  2.17209 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1694906 C_bot  0.15 k_c 0.0\n",
      "3294 Train Loss 6.2135797\n",
      "Loss  2.1694906 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1669977 C_bot  0.15 k_c 0.0\n",
      "3295 Train Loss 6.21043\n",
      "Loss  2.1669977 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1645787 C_bot  0.15 k_c 0.0\n",
      "3296 Train Loss 6.20746\n",
      "Loss  2.1645787 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1622217 C_bot  0.15 k_c 0.0\n",
      "3297 Train Loss 6.204442\n",
      "Loss  2.1622217 C_bot  0.15 k_c 0.0\n",
      "Loss  2.159645 C_bot  0.15 k_c 0.0\n",
      "3298 Train Loss 6.201331\n",
      "Loss  2.159645 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1572826 C_bot  0.15 k_c 0.0\n",
      "3299 Train Loss 6.1982946\n",
      "Loss  2.1572826 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1547453 C_bot  0.15 k_c 0.0\n",
      "3300 Train Loss 6.1952496\n",
      "Loss  2.1547453 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1524677 C_bot  0.15 k_c 0.0\n",
      "3301 Train Loss 6.192274\n",
      "Loss  2.1524677 C_bot  0.15 k_c 0.0\n",
      "Loss  2.150099 C_bot  0.15 k_c 0.0\n",
      "3302 Train Loss 6.189438\n",
      "Loss  2.150099 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1477733 C_bot  0.15 k_c 0.0\n",
      "3303 Train Loss 6.1863737\n",
      "Loss  2.1477733 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1453614 C_bot  0.15 k_c 0.0\n",
      "3304 Train Loss 6.1835575\n",
      "Loss  2.1453614 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1433744 C_bot  0.15 k_c 0.0\n",
      "3305 Train Loss 6.180763\n",
      "Loss  2.1433744 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1410167 C_bot  0.15 k_c 0.0\n",
      "3306 Train Loss 6.178099\n",
      "Loss  2.1410167 C_bot  0.15 k_c 0.0\n",
      "Loss  2.13938 C_bot  0.15 k_c 0.0\n",
      "3307 Train Loss 6.1755395\n",
      "Loss  2.13938 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1374266 C_bot  0.15 k_c 0.0\n",
      "3308 Train Loss 6.1734414\n",
      "Loss  2.1374266 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1369216 C_bot  0.15 k_c 0.0\n",
      "3309 Train Loss 6.171816\n",
      "Loss  2.1369216 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1361876 C_bot  0.15 k_c 0.0\n",
      "3310 Train Loss 6.17121\n",
      "Loss  2.1361876 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1385329 C_bot  0.15 k_c 0.0\n",
      "3311 Train Loss 6.1720963\n",
      "Loss  2.1385329 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1414196 C_bot  0.15 k_c 0.0\n",
      "3312 Train Loss 6.175572\n",
      "Loss  2.1414196 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1515937 C_bot  0.15 k_c 0.0\n",
      "3313 Train Loss 6.1837034\n",
      "Loss  2.1515937 C_bot  0.15 k_c 0.0\n",
      "Loss  2.166764 C_bot  0.15 k_c 0.0\n",
      "3314 Train Loss 6.2002726\n",
      "Loss  2.166764 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2008588 C_bot  0.15 k_c 0.0\n",
      "3315 Train Loss 6.2312927\n",
      "Loss  2.2008588 C_bot  0.15 k_c 0.0\n",
      "Loss  2.255839 C_bot  0.15 k_c 0.0\n",
      "3316 Train Loss 6.289137\n",
      "Loss  2.255839 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3663075 C_bot  0.15 k_c 0.0\n",
      "3317 Train Loss 6.3946886\n",
      "Loss  2.3663075 C_bot  0.15 k_c 0.0\n",
      "Loss  2.556378 C_bot  0.15 k_c 0.0\n",
      "3318 Train Loss 6.590357\n",
      "Loss  2.556378 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9222074 C_bot  0.15 k_c 0.0\n",
      "3319 Train Loss 6.947962\n",
      "Loss  2.9222074 C_bot  0.15 k_c 0.0\n",
      "Loss  3.586218 C_bot  0.15 k_c 0.0\n",
      "3320 Train Loss 7.622891\n",
      "Loss  3.586218 C_bot  0.15 k_c 0.0\n",
      "Loss  4.837255 C_bot  0.15 k_c 0.0\n",
      "3321 Train Loss 8.859789\n",
      "Loss  4.837255 C_bot  0.15 k_c 0.0\n",
      "Loss  7.204096 C_bot  0.15 k_c 0.0\n",
      "3322 Train Loss 11.24854\n",
      "Loss  7.204096 C_bot  0.15 k_c 0.0\n",
      "Loss  11.56962 C_bot  0.15 k_c 0.0\n",
      "3323 Train Loss 15.589532\n",
      "Loss  11.56962 C_bot  0.15 k_c 0.0\n",
      "Loss  20.088057 C_bot  0.15 k_c 0.0\n",
      "3324 Train Loss 24.154774\n",
      "Loss  20.088057 C_bot  0.15 k_c 0.0\n",
      "Loss  34.9565 C_bot  0.15 k_c 0.0\n",
      "3325 Train Loss 38.980312\n",
      "Loss  34.9565 C_bot  0.15 k_c 0.0\n",
      "Loss  63.88934 C_bot  0.15 k_c 0.0\n",
      "3326 Train Loss 68.02328\n",
      "Loss  63.88934 C_bot  0.15 k_c 0.0\n",
      "Loss  106.262695 C_bot  0.15 k_c 0.0\n",
      "3327 Train Loss 110.31265\n",
      "Loss  106.262695 C_bot  0.15 k_c 0.0\n",
      "Loss  177.82037 C_bot  0.15 k_c 0.0\n",
      "3328 Train Loss 182.14314\n",
      "Loss  177.82037 C_bot  0.15 k_c 0.0\n",
      "Loss  232.74963 C_bot  0.15 k_c 0.0\n",
      "3329 Train Loss 236.85034\n",
      "Loss  232.74963 C_bot  0.15 k_c 0.0\n",
      "Loss  263.9782 C_bot  0.15 k_c 0.0\n",
      "3330 Train Loss 268.59286\n",
      "Loss  263.9782 C_bot  0.15 k_c 0.0\n",
      "Loss  189.53432 C_bot  0.15 k_c 0.0\n",
      "3331 Train Loss 193.6999\n",
      "Loss  189.53432 C_bot  0.15 k_c 0.0\n",
      "Loss  72.87897 C_bot  0.15 k_c 0.0\n",
      "3332 Train Loss 77.482025\n",
      "Loss  72.87897 C_bot  0.15 k_c 0.0\n",
      "Loss  4.8396497 C_bot  0.15 k_c 0.0\n",
      "3333 Train Loss 9.296591\n",
      "Loss  4.8396497 C_bot  0.15 k_c 0.0\n",
      "Loss  34.42446 C_bot  0.15 k_c 0.0\n",
      "3334 Train Loss 38.88295\n",
      "Loss  34.42446 C_bot  0.15 k_c 0.0\n",
      "Loss  107.281395 C_bot  0.15 k_c 0.0\n",
      "3335 Train Loss 112.24318\n",
      "Loss  107.281395 C_bot  0.15 k_c 0.0\n",
      "Loss  130.97092 C_bot  0.15 k_c 0.0\n",
      "3336 Train Loss 135.4961\n",
      "Loss  130.97092 C_bot  0.15 k_c 0.0\n",
      "Loss  93.92924 C_bot  0.15 k_c 0.0\n",
      "3337 Train Loss 98.9378\n",
      "Loss  93.92924 C_bot  0.15 k_c 0.0\n",
      "Loss  27.95054 C_bot  0.15 k_c 0.0\n",
      "3338 Train Loss 32.593353\n",
      "Loss  27.95054 C_bot  0.15 k_c 0.0\n",
      "Loss  3.645369 C_bot  0.15 k_c 0.0\n",
      "3339 Train Loss 8.365046\n",
      "Loss  3.645369 C_bot  0.15 k_c 0.0\n",
      "Loss  31.369339 C_bot  0.15 k_c 0.0\n",
      "3340 Train Loss 36.211285\n",
      "Loss  31.369339 C_bot  0.15 k_c 0.0\n",
      "Loss  66.48556 C_bot  0.15 k_c 0.0\n",
      "3341 Train Loss 71.06525\n",
      "Loss  66.48556 C_bot  0.15 k_c 0.0\n",
      "Loss  69.09463 C_bot  0.15 k_c 0.0\n",
      "3342 Train Loss 73.92075\n",
      "Loss  69.09463 C_bot  0.15 k_c 0.0\n",
      "Loss  33.8626 C_bot  0.15 k_c 0.0\n",
      "3343 Train Loss 38.386944\n",
      "Loss  33.8626 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1712456 C_bot  0.15 k_c 0.0\n",
      "3344 Train Loss 9.742738\n",
      "Loss  5.1712456 C_bot  0.15 k_c 0.0\n",
      "Loss  9.591248 C_bot  0.15 k_c 0.0\n",
      "3345 Train Loss 14.151262\n",
      "Loss  9.591248 C_bot  0.15 k_c 0.0\n",
      "Loss  32.125084 C_bot  0.15 k_c 0.0\n",
      "3346 Train Loss 36.607693\n",
      "Loss  32.125084 C_bot  0.15 k_c 0.0\n",
      "Loss  40.432407 C_bot  0.15 k_c 0.0\n",
      "3347 Train Loss 45.01925\n",
      "Loss  40.432407 C_bot  0.15 k_c 0.0\n",
      "Loss  23.189173 C_bot  0.15 k_c 0.0\n",
      "3348 Train Loss 27.666069\n",
      "Loss  23.189173 C_bot  0.15 k_c 0.0\n",
      "Loss  4.972157 C_bot  0.15 k_c 0.0\n",
      "3349 Train Loss 9.462069\n",
      "Loss  4.972157 C_bot  0.15 k_c 0.0\n",
      "Loss  6.0559587 C_bot  0.15 k_c 0.0\n",
      "3350 Train Loss 10.529764\n",
      "Loss  6.0559587 C_bot  0.15 k_c 0.0\n",
      "Loss  19.400442 C_bot  0.15 k_c 0.0\n",
      "3351 Train Loss 23.83254\n",
      "Loss  19.400442 C_bot  0.15 k_c 0.0\n",
      "Loss  24.14167 C_bot  0.15 k_c 0.0\n",
      "3352 Train Loss 28.616175\n",
      "Loss  24.14167 C_bot  0.15 k_c 0.0\n",
      "Loss  13.7483635 C_bot  0.15 k_c 0.0\n",
      "3353 Train Loss 18.149895\n",
      "Loss  13.7483635 C_bot  0.15 k_c 0.0\n",
      "Loss  3.563815 C_bot  0.15 k_c 0.0\n",
      "3354 Train Loss 7.969818\n",
      "Loss  3.563815 C_bot  0.15 k_c 0.0\n",
      "Loss  5.474518 C_bot  0.15 k_c 0.0\n",
      "3355 Train Loss 9.870689\n",
      "Loss  5.474518 C_bot  0.15 k_c 0.0\n",
      "Loss  13.490009 C_bot  0.15 k_c 0.0\n",
      "3356 Train Loss 17.831593\n",
      "Loss  13.490009 C_bot  0.15 k_c 0.0\n",
      "Loss  14.9047365 C_bot  0.15 k_c 0.0\n",
      "3357 Train Loss 19.293959\n",
      "Loss  14.9047365 C_bot  0.15 k_c 0.0\n",
      "Loss  8.027362 C_bot  0.15 k_c 0.0\n",
      "3358 Train Loss 12.33918\n",
      "Loss  8.027362 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8210664 C_bot  0.15 k_c 0.0\n",
      "3359 Train Loss 7.1438913\n",
      "Loss  2.8210664 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1089487 C_bot  0.15 k_c 0.0\n",
      "3360 Train Loss 9.435493\n",
      "Loss  5.1089487 C_bot  0.15 k_c 0.0\n",
      "Loss  9.725602 C_bot  0.15 k_c 0.0\n",
      "3361 Train Loss 13.996024\n",
      "Loss  9.725602 C_bot  0.15 k_c 0.0\n",
      "Loss  9.463398 C_bot  0.15 k_c 0.0\n",
      "3362 Train Loss 13.783838\n",
      "Loss  9.463398 C_bot  0.15 k_c 0.0\n",
      "Loss  5.0532575 C_bot  0.15 k_c 0.0\n",
      "3363 Train Loss 9.312383\n",
      "Loss  5.0532575 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6496933 C_bot  0.15 k_c 0.0\n",
      "3364 Train Loss 6.916237\n",
      "Loss  2.6496933 C_bot  0.15 k_c 0.0\n",
      "Loss  4.6192203 C_bot  0.15 k_c 0.0\n",
      "3365 Train Loss 8.895539\n",
      "Loss  4.6192203 C_bot  0.15 k_c 0.0\n",
      "Loss  7.1354012 C_bot  0.15 k_c 0.0\n",
      "3366 Train Loss 11.368619\n",
      "Loss  7.1354012 C_bot  0.15 k_c 0.0\n",
      "Loss  6.394596 C_bot  0.15 k_c 0.0\n",
      "3367 Train Loss 10.663038\n",
      "Loss  6.394596 C_bot  0.15 k_c 0.0\n",
      "Loss  3.6983063 C_bot  0.15 k_c 0.0\n",
      "3368 Train Loss 7.9285507\n",
      "Loss  3.6983063 C_bot  0.15 k_c 0.0\n",
      "Loss  2.641802 C_bot  0.15 k_c 0.0\n",
      "3369 Train Loss 6.873827\n",
      "Loss  2.641802 C_bot  0.15 k_c 0.0\n",
      "Loss  4.0415726 C_bot  0.15 k_c 0.0\n",
      "3370 Train Loss 8.281675\n",
      "Loss  4.0415726 C_bot  0.15 k_c 0.0\n",
      "Loss  5.392703 C_bot  0.15 k_c 0.0\n",
      "3371 Train Loss 9.603932\n",
      "Loss  5.392703 C_bot  0.15 k_c 0.0\n",
      "Loss  4.722945 C_bot  0.15 k_c 0.0\n",
      "3372 Train Loss 8.954947\n",
      "Loss  4.722945 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1144817 C_bot  0.15 k_c 0.0\n",
      "3373 Train Loss 7.3223405\n",
      "Loss  3.1144817 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6111908 C_bot  0.15 k_c 0.0\n",
      "3374 Train Loss 6.817293\n",
      "Loss  2.6111908 C_bot  0.15 k_c 0.0\n",
      "Loss  3.4928873 C_bot  0.15 k_c 0.0\n",
      "3375 Train Loss 7.7025905\n",
      "Loss  3.4928873 C_bot  0.15 k_c 0.0\n",
      "Loss  4.2545877 C_bot  0.15 k_c 0.0\n",
      "3376 Train Loss 8.442982\n",
      "Loss  4.2545877 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8137548 C_bot  0.15 k_c 0.0\n",
      "3377 Train Loss 8.014206\n",
      "Loss  3.8137548 C_bot  0.15 k_c 0.0\n",
      "Loss  2.860411 C_bot  0.15 k_c 0.0\n",
      "3378 Train Loss 7.0435977\n",
      "Loss  2.860411 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5552197 C_bot  0.15 k_c 0.0\n",
      "3379 Train Loss 6.736952\n",
      "Loss  2.5552197 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0619793 C_bot  0.15 k_c 0.0\n",
      "3380 Train Loss 7.246425\n",
      "Loss  3.0619793 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5346043 C_bot  0.15 k_c 0.0\n",
      "3381 Train Loss 7.703023\n",
      "Loss  3.5346043 C_bot  0.15 k_c 0.0\n",
      "Loss  3.3096137 C_bot  0.15 k_c 0.0\n",
      "3382 Train Loss 7.4880085\n",
      "Loss  3.3096137 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7478225 C_bot  0.15 k_c 0.0\n",
      "3383 Train Loss 6.912388\n",
      "Loss  2.7478225 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5088491 C_bot  0.15 k_c 0.0\n",
      "3384 Train Loss 6.673366\n",
      "Loss  2.5088491 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7666867 C_bot  0.15 k_c 0.0\n",
      "3385 Train Loss 6.932786\n",
      "Loss  2.7666867 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0824006 C_bot  0.15 k_c 0.0\n",
      "3386 Train Loss 7.2366343\n",
      "Loss  3.0824006 C_bot  0.15 k_c 0.0\n",
      "Loss  3.013266 C_bot  0.15 k_c 0.0\n",
      "3387 Train Loss 7.1755896\n",
      "Loss  3.013266 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6929324 C_bot  0.15 k_c 0.0\n",
      "3388 Train Loss 6.8438296\n",
      "Loss  2.6929324 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4852245 C_bot  0.15 k_c 0.0\n",
      "3389 Train Loss 6.636885\n",
      "Loss  2.4852245 C_bot  0.15 k_c 0.0\n",
      "Loss  2.579484 C_bot  0.15 k_c 0.0\n",
      "3390 Train Loss 6.7304864\n",
      "Loss  2.579484 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7847364 C_bot  0.15 k_c 0.0\n",
      "3391 Train Loss 6.927604\n",
      "Loss  2.7847364 C_bot  0.15 k_c 0.0\n",
      "Loss  2.8091033 C_bot  0.15 k_c 0.0\n",
      "3392 Train Loss 6.9573555\n",
      "Loss  2.8091033 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6496012 C_bot  0.15 k_c 0.0\n",
      "3393 Train Loss 6.7890577\n",
      "Loss  2.6496012 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4804537 C_bot  0.15 k_c 0.0\n",
      "3394 Train Loss 6.621555\n",
      "Loss  2.4804537 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4747996 C_bot  0.15 k_c 0.0\n",
      "3395 Train Loss 6.613971\n",
      "Loss  2.4747996 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5880616 C_bot  0.15 k_c 0.0\n",
      "3396 Train Loss 6.7222314\n",
      "Loss  2.5880616 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6503036 C_bot  0.15 k_c 0.0\n",
      "3397 Train Loss 6.7878547\n",
      "Loss  2.6503036 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5997293 C_bot  0.15 k_c 0.0\n",
      "3398 Train Loss 6.7302103\n",
      "Loss  2.5997293 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4846995 C_bot  0.15 k_c 0.0\n",
      "3399 Train Loss 6.6172104\n",
      "Loss  2.4846995 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4327579 C_bot  0.15 k_c 0.0\n",
      "3400 Train Loss 6.5622244\n",
      "Loss  2.4327579 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4701006 C_bot  0.15 k_c 0.0\n",
      "3401 Train Loss 6.5967565\n",
      "Loss  2.4701006 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5266595 C_bot  0.15 k_c 0.0\n",
      "3402 Train Loss 6.6549916\n",
      "Loss  2.5266595 C_bot  0.15 k_c 0.0\n",
      "Loss  2.5374718 C_bot  0.15 k_c 0.0\n",
      "3403 Train Loss 6.660122\n",
      "Loss  2.5374718 C_bot  0.15 k_c 0.0\n",
      "Loss  2.481199 C_bot  0.15 k_c 0.0\n",
      "3404 Train Loss 6.6058736\n",
      "Loss  2.481199 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4259756 C_bot  0.15 k_c 0.0\n",
      "3405 Train Loss 6.546691\n",
      "Loss  2.4259756 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4127314 C_bot  0.15 k_c 0.0\n",
      "3406 Train Loss 6.5321636\n",
      "Loss  2.4127314 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4390457 C_bot  0.15 k_c 0.0\n",
      "3407 Train Loss 6.558405\n",
      "Loss  2.4390457 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4679153 C_bot  0.15 k_c 0.0\n",
      "3408 Train Loss 6.5829496\n",
      "Loss  2.4679153 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4571328 C_bot  0.15 k_c 0.0\n",
      "3409 Train Loss 6.573785\n",
      "Loss  2.4571328 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4255521 C_bot  0.15 k_c 0.0\n",
      "3410 Train Loss 6.537961\n",
      "Loss  2.4255521 C_bot  0.15 k_c 0.0\n",
      "Loss  2.394301 C_bot  0.15 k_c 0.0\n",
      "3411 Train Loss 6.506615\n",
      "Loss  2.394301 C_bot  0.15 k_c 0.0\n",
      "Loss  2.390125 C_bot  0.15 k_c 0.0\n",
      "3412 Train Loss 6.5008254\n",
      "Loss  2.390125 C_bot  0.15 k_c 0.0\n",
      "Loss  2.406114 C_bot  0.15 k_c 0.0\n",
      "3413 Train Loss 6.5139756\n",
      "Loss  2.406114 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4152634 C_bot  0.15 k_c 0.0\n",
      "3414 Train Loss 6.523692\n",
      "Loss  2.4152634 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4108956 C_bot  0.15 k_c 0.0\n",
      "3415 Train Loss 6.515475\n",
      "Loss  2.4108956 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3887076 C_bot  0.15 k_c 0.0\n",
      "3416 Train Loss 6.4936833\n",
      "Loss  2.3887076 C_bot  0.15 k_c 0.0\n",
      "Loss  2.372244 C_bot  0.15 k_c 0.0\n",
      "3417 Train Loss 6.47464\n",
      "Loss  2.372244 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3675435 C_bot  0.15 k_c 0.0\n",
      "3418 Train Loss 6.46854\n",
      "Loss  2.3675435 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3723867 C_bot  0.15 k_c 0.0\n",
      "3419 Train Loss 6.4727335\n",
      "Loss  2.3723867 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3789852 C_bot  0.15 k_c 0.0\n",
      "3420 Train Loss 6.4764576\n",
      "Loss  2.3789852 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3737788 C_bot  0.15 k_c 0.0\n",
      "3421 Train Loss 6.47142\n",
      "Loss  2.3737788 C_bot  0.15 k_c 0.0\n",
      "Loss  2.363674 C_bot  0.15 k_c 0.0\n",
      "3422 Train Loss 6.4584823\n",
      "Loss  2.363674 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3506758 C_bot  0.15 k_c 0.0\n",
      "3423 Train Loss 6.444997\n",
      "Loss  2.3506758 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3446229 C_bot  0.15 k_c 0.0\n",
      "3424 Train Loss 6.4373126\n",
      "Loss  2.3446229 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3448582 C_bot  0.15 k_c 0.0\n",
      "3425 Train Loss 6.4358053\n",
      "Loss  2.3448582 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3455122 C_bot  0.15 k_c 0.0\n",
      "3426 Train Loss 6.43597\n",
      "Loss  2.3455122 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3448577 C_bot  0.15 k_c 0.0\n",
      "3427 Train Loss 6.4328456\n",
      "Loss  2.3448577 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3375518 C_bot  0.15 k_c 0.0\n",
      "3428 Train Loss 6.425276\n",
      "Loss  2.3375518 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3299773 C_bot  0.15 k_c 0.0\n",
      "3429 Train Loss 6.4155126\n",
      "Loss  2.3299773 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3224947 C_bot  0.15 k_c 0.0\n",
      "3430 Train Loss 6.4071403\n",
      "Loss  2.3224947 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3186533 C_bot  0.15 k_c 0.0\n",
      "3431 Train Loss 6.401989\n",
      "Loss  2.3186533 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3175874 C_bot  0.15 k_c 0.0\n",
      "3432 Train Loss 6.3992057\n",
      "Loss  2.3175874 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3152556 C_bot  0.15 k_c 0.0\n",
      "3433 Train Loss 6.3962617\n",
      "Loss  2.3152556 C_bot  0.15 k_c 0.0\n",
      "Loss  2.312468 C_bot  0.15 k_c 0.0\n",
      "3434 Train Loss 6.391364\n",
      "Loss  2.312468 C_bot  0.15 k_c 0.0\n",
      "Loss  2.306134 C_bot  0.15 k_c 0.0\n",
      "3435 Train Loss 6.384485\n",
      "Loss  2.306134 C_bot  0.15 k_c 0.0\n",
      "Loss  2.300357 C_bot  0.15 k_c 0.0\n",
      "3436 Train Loss 6.3768506\n",
      "Loss  2.300357 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2946987 C_bot  0.15 k_c 0.0\n",
      "3437 Train Loss 6.3701925\n",
      "Loss  2.2946987 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2906356 C_bot  0.15 k_c 0.0\n",
      "3438 Train Loss 6.3648744\n",
      "Loss  2.2906356 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2878382 C_bot  0.15 k_c 0.0\n",
      "3439 Train Loss 6.360524\n",
      "Loss  2.2878382 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2842743 C_bot  0.15 k_c 0.0\n",
      "3440 Train Loss 6.35618\n",
      "Loss  2.2842743 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2809682 C_bot  0.15 k_c 0.0\n",
      "3441 Train Loss 6.351046\n",
      "Loss  2.2809682 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2755463 C_bot  0.15 k_c 0.0\n",
      "3442 Train Loss 6.34492\n",
      "Loss  2.2755463 C_bot  0.15 k_c 0.0\n",
      "Loss  2.270578 C_bot  0.15 k_c 0.0\n",
      "3443 Train Loss 6.3382506\n",
      "Loss  2.270578 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2648976 C_bot  0.15 k_c 0.0\n",
      "3444 Train Loss 6.3316054\n",
      "Loss  2.2648976 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2602234 C_bot  0.15 k_c 0.0\n",
      "3445 Train Loss 6.325609\n",
      "Loss  2.2602234 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2561436 C_bot  0.15 k_c 0.0\n",
      "3446 Train Loss 6.3202043\n",
      "Loss  2.2561436 C_bot  0.15 k_c 0.0\n",
      "Loss  2.251799 C_bot  0.15 k_c 0.0\n",
      "3447 Train Loss 6.3148994\n",
      "Loss  2.251799 C_bot  0.15 k_c 0.0\n",
      "Loss  2.248007 C_bot  0.15 k_c 0.0\n",
      "3448 Train Loss 6.3095527\n",
      "Loss  2.248007 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2429705 C_bot  0.15 k_c 0.0\n",
      "3449 Train Loss 6.3037105\n",
      "Loss  2.2429705 C_bot  0.15 k_c 0.0\n",
      "Loss  2.238234 C_bot  0.15 k_c 0.0\n",
      "3450 Train Loss 6.2974257\n",
      "Loss  2.238234 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2326112 C_bot  0.15 k_c 0.0\n",
      "3451 Train Loss 6.2909245\n",
      "Loss  2.2326112 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2275736 C_bot  0.15 k_c 0.0\n",
      "3452 Train Loss 6.2845445\n",
      "Loss  2.2275736 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2224393 C_bot  0.15 k_c 0.0\n",
      "3453 Train Loss 6.27834\n",
      "Loss  2.2224393 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2175689 C_bot  0.15 k_c 0.0\n",
      "3454 Train Loss 6.2723923\n",
      "Loss  2.2175689 C_bot  0.15 k_c 0.0\n",
      "Loss  2.212918 C_bot  0.15 k_c 0.0\n",
      "3455 Train Loss 6.26649\n",
      "Loss  2.212918 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2078311 C_bot  0.15 k_c 0.0\n",
      "3456 Train Loss 6.260519\n",
      "Loss  2.2078311 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2031295 C_bot  0.15 k_c 0.0\n",
      "3457 Train Loss 6.2544875\n",
      "Loss  2.2031295 C_bot  0.15 k_c 0.0\n",
      "Loss  2.197533 C_bot  0.15 k_c 0.0\n",
      "3458 Train Loss 6.248075\n",
      "Loss  2.197533 C_bot  0.15 k_c 0.0\n",
      "Loss  2.192592 C_bot  0.15 k_c 0.0\n",
      "3459 Train Loss 6.2418528\n",
      "Loss  2.192592 C_bot  0.15 k_c 0.0\n",
      "Loss  2.186973 C_bot  0.15 k_c 0.0\n",
      "3460 Train Loss 6.2353797\n",
      "Loss  2.186973 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1817174 C_bot  0.15 k_c 0.0\n",
      "3461 Train Loss 6.228981\n",
      "Loss  2.1817174 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1762686 C_bot  0.15 k_c 0.0\n",
      "3462 Train Loss 6.2225847\n",
      "Loss  2.1762686 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1710205 C_bot  0.15 k_c 0.0\n",
      "3463 Train Loss 6.2163544\n",
      "Loss  2.1710205 C_bot  0.15 k_c 0.0\n",
      "Loss  2.165846 C_bot  0.15 k_c 0.0\n",
      "3464 Train Loss 6.2101383\n",
      "Loss  2.165846 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1606257 C_bot  0.15 k_c 0.0\n",
      "3465 Train Loss 6.2040668\n",
      "Loss  2.1606257 C_bot  0.15 k_c 0.0\n",
      "Loss  2.155577 C_bot  0.15 k_c 0.0\n",
      "3466 Train Loss 6.1979246\n",
      "Loss  2.155577 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1501389 C_bot  0.15 k_c 0.0\n",
      "3467 Train Loss 6.1917114\n",
      "Loss  2.1501389 C_bot  0.15 k_c 0.0\n",
      "Loss  2.145075 C_bot  0.15 k_c 0.0\n",
      "3468 Train Loss 6.1855593\n",
      "Loss  2.145075 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1395516 C_bot  0.15 k_c 0.0\n",
      "3469 Train Loss 6.179282\n",
      "Loss  2.1395516 C_bot  0.15 k_c 0.0\n",
      "Loss  2.13446 C_bot  0.15 k_c 0.0\n",
      "3470 Train Loss 6.17315\n",
      "Loss  2.13446 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1290238 C_bot  0.15 k_c 0.0\n",
      "3471 Train Loss 6.166939\n",
      "Loss  2.1290238 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1238863 C_bot  0.15 k_c 0.0\n",
      "3472 Train Loss 6.1608324\n",
      "Loss  2.1238863 C_bot  0.15 k_c 0.0\n",
      "Loss  2.118608 C_bot  0.15 k_c 0.0\n",
      "3473 Train Loss 6.154738\n",
      "Loss  2.118608 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1135478 C_bot  0.15 k_c 0.0\n",
      "3474 Train Loss 6.148785\n",
      "Loss  2.1135478 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1085231 C_bot  0.15 k_c 0.0\n",
      "3475 Train Loss 6.1429033\n",
      "Loss  2.1085231 C_bot  0.15 k_c 0.0\n",
      "Loss  2.103461 C_bot  0.15 k_c 0.0\n",
      "3476 Train Loss 6.137018\n",
      "Loss  2.103461 C_bot  0.15 k_c 0.0\n",
      "Loss  2.098483 C_bot  0.15 k_c 0.0\n",
      "3477 Train Loss 6.131152\n",
      "Loss  2.098483 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0935094 C_bot  0.15 k_c 0.0\n",
      "3478 Train Loss 6.125409\n",
      "Loss  2.0935094 C_bot  0.15 k_c 0.0\n",
      "Loss  2.088724 C_bot  0.15 k_c 0.0\n",
      "3479 Train Loss 6.1197147\n",
      "Loss  2.088724 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0837784 C_bot  0.15 k_c 0.0\n",
      "3480 Train Loss 6.1140366\n",
      "Loss  2.0837784 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0790772 C_bot  0.15 k_c 0.0\n",
      "3481 Train Loss 6.1084194\n",
      "Loss  2.0790772 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0742395 C_bot  0.15 k_c 0.0\n",
      "3482 Train Loss 6.1028724\n",
      "Loss  2.0742395 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0696127 C_bot  0.15 k_c 0.0\n",
      "3483 Train Loss 6.097328\n",
      "Loss  2.0696127 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0648484 C_bot  0.15 k_c 0.0\n",
      "3484 Train Loss 6.091875\n",
      "Loss  2.0648484 C_bot  0.15 k_c 0.0\n",
      "Loss  2.060379 C_bot  0.15 k_c 0.0\n",
      "3485 Train Loss 6.0864906\n",
      "Loss  2.060379 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0556967 C_bot  0.15 k_c 0.0\n",
      "3486 Train Loss 6.081134\n",
      "Loss  2.0556967 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0512807 C_bot  0.15 k_c 0.0\n",
      "3487 Train Loss 6.075804\n",
      "Loss  2.0512807 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0465693 C_bot  0.15 k_c 0.0\n",
      "3488 Train Loss 6.0704355\n",
      "Loss  2.0465693 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0423691 C_bot  0.15 k_c 0.0\n",
      "3489 Train Loss 6.065318\n",
      "Loss  2.0423691 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0378006 C_bot  0.15 k_c 0.0\n",
      "3490 Train Loss 6.0601172\n",
      "Loss  2.0378006 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0336416 C_bot  0.15 k_c 0.0\n",
      "3491 Train Loss 6.0550284\n",
      "Loss  2.0336416 C_bot  0.15 k_c 0.0\n",
      "Loss  2.029256 C_bot  0.15 k_c 0.0\n",
      "3492 Train Loss 6.050049\n",
      "Loss  2.029256 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0253754 C_bot  0.15 k_c 0.0\n",
      "3493 Train Loss 6.04521\n",
      "Loss  2.0253754 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0210133 C_bot  0.15 k_c 0.0\n",
      "3494 Train Loss 6.0403085\n",
      "Loss  2.0210133 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0172505 C_bot  0.15 k_c 0.0\n",
      "3495 Train Loss 6.035536\n",
      "Loss  2.0172505 C_bot  0.15 k_c 0.0\n",
      "Loss  2.013207 C_bot  0.15 k_c 0.0\n",
      "3496 Train Loss 6.031034\n",
      "Loss  2.013207 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0098786 C_bot  0.15 k_c 0.0\n",
      "3497 Train Loss 6.0266137\n",
      "Loss  2.0098786 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0061977 C_bot  0.15 k_c 0.0\n",
      "3498 Train Loss 6.0225964\n",
      "Loss  2.0061977 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0038483 C_bot  0.15 k_c 0.0\n",
      "3499 Train Loss 6.019017\n",
      "Loss  2.0038483 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0011585 C_bot  0.15 k_c 0.0\n",
      "3500 Train Loss 6.0161815\n",
      "Loss  2.0011585 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0006618 C_bot  0.15 k_c 0.0\n",
      "3501 Train Loss 6.014233\n",
      "Loss  2.0006618 C_bot  0.15 k_c 0.0\n",
      "Loss  2.000338 C_bot  0.15 k_c 0.0\n",
      "3502 Train Loss 6.0140657\n",
      "Loss  2.000338 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0048254 C_bot  0.15 k_c 0.0\n",
      "3503 Train Loss 6.016731\n",
      "Loss  2.0048254 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0114145 C_bot  0.15 k_c 0.0\n",
      "3504 Train Loss 6.0239835\n",
      "Loss  2.0114145 C_bot  0.15 k_c 0.0\n",
      "Loss  2.029115 C_bot  0.15 k_c 0.0\n",
      "3505 Train Loss 6.0392356\n",
      "Loss  2.029115 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0561273 C_bot  0.15 k_c 0.0\n",
      "3506 Train Loss 6.0677805\n",
      "Loss  2.0561273 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1112487 C_bot  0.15 k_c 0.0\n",
      "3507 Train Loss 6.1193857\n",
      "Loss  2.1112487 C_bot  0.15 k_c 0.0\n",
      "Loss  2.200841 C_bot  0.15 k_c 0.0\n",
      "3508 Train Loss 6.2120457\n",
      "Loss  2.200841 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3698044 C_bot  0.15 k_c 0.0\n",
      "3509 Train Loss 6.375646\n",
      "Loss  2.3698044 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6585448 C_bot  0.15 k_c 0.0\n",
      "3510 Train Loss 6.67028\n",
      "Loss  2.6585448 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1893895 C_bot  0.15 k_c 0.0\n",
      "3511 Train Loss 7.192566\n",
      "Loss  3.1893895 C_bot  0.15 k_c 0.0\n",
      "Loss  4.139215 C_bot  0.15 k_c 0.0\n",
      "3512 Train Loss 8.153756\n",
      "Loss  4.139215 C_bot  0.15 k_c 0.0\n",
      "Loss  5.8676577 C_bot  0.15 k_c 0.0\n",
      "3513 Train Loss 9.868148\n",
      "Loss  5.8676577 C_bot  0.15 k_c 0.0\n",
      "Loss  9.084429 C_bot  0.15 k_c 0.0\n",
      "3514 Train Loss 13.107727\n",
      "Loss  9.084429 C_bot  0.15 k_c 0.0\n",
      "Loss  14.840805 C_bot  0.15 k_c 0.0\n",
      "3515 Train Loss 18.840767\n",
      "Loss  14.840805 C_bot  0.15 k_c 0.0\n",
      "Loss  25.86164 C_bot  0.15 k_c 0.0\n",
      "3516 Train Loss 29.911201\n",
      "Loss  25.86164 C_bot  0.15 k_c 0.0\n",
      "Loss  44.507454 C_bot  0.15 k_c 0.0\n",
      "3517 Train Loss 48.517155\n",
      "Loss  44.507454 C_bot  0.15 k_c 0.0\n",
      "Loss  79.83374 C_bot  0.15 k_c 0.0\n",
      "3518 Train Loss 83.96405\n",
      "Loss  79.83374 C_bot  0.15 k_c 0.0\n",
      "Loss  129.58046 C_bot  0.15 k_c 0.0\n",
      "3519 Train Loss 133.62761\n",
      "Loss  129.58046 C_bot  0.15 k_c 0.0\n",
      "Loss  209.6666 C_bot  0.15 k_c 0.0\n",
      "3520 Train Loss 214.01749\n",
      "Loss  209.6666 C_bot  0.15 k_c 0.0\n",
      "Loss  268.19458 C_bot  0.15 k_c 0.0\n",
      "3521 Train Loss 272.30417\n",
      "Loss  268.19458 C_bot  0.15 k_c 0.0\n",
      "Loss  297.93048 C_bot  0.15 k_c 0.0\n",
      "3522 Train Loss 302.59317\n",
      "Loss  297.93048 C_bot  0.15 k_c 0.0\n",
      "Loss  218.54088 C_bot  0.15 k_c 0.0\n",
      "3523 Train Loss 222.71463\n",
      "Loss  218.54088 C_bot  0.15 k_c 0.0\n",
      "Loss  98.29383 C_bot  0.15 k_c 0.0\n",
      "3524 Train Loss 102.93077\n",
      "Loss  98.29383 C_bot  0.15 k_c 0.0\n",
      "Loss  11.518922 C_bot  0.15 k_c 0.0\n",
      "3525 Train Loss 15.915928\n",
      "Loss  11.518922 C_bot  0.15 k_c 0.0\n",
      "Loss  19.92745 C_bot  0.15 k_c 0.0\n",
      "3526 Train Loss 24.368237\n",
      "Loss  19.92745 C_bot  0.15 k_c 0.0\n",
      "Loss  90.53105 C_bot  0.15 k_c 0.0\n",
      "3527 Train Loss 95.397385\n",
      "Loss  90.53105 C_bot  0.15 k_c 0.0\n",
      "Loss  138.12253 C_bot  0.15 k_c 0.0\n",
      "3528 Train Loss 142.584\n",
      "Loss  138.12253 C_bot  0.15 k_c 0.0\n",
      "Loss  128.14891 C_bot  0.15 k_c 0.0\n",
      "3529 Train Loss 133.11702\n",
      "Loss  128.14891 C_bot  0.15 k_c 0.0\n",
      "Loss  58.396816 C_bot  0.15 k_c 0.0\n",
      "3530 Train Loss 62.900303\n",
      "Loss  58.396816 C_bot  0.15 k_c 0.0\n",
      "Loss  7.4282312 C_bot  0.15 k_c 0.0\n",
      "3531 Train Loss 12.06198\n",
      "Loss  7.4282312 C_bot  0.15 k_c 0.0\n",
      "Loss  13.529412 C_bot  0.15 k_c 0.0\n",
      "3532 Train Loss 18.153412\n",
      "Loss  13.529412 C_bot  0.15 k_c 0.0\n",
      "Loss  53.092056 C_bot  0.15 k_c 0.0\n",
      "3533 Train Loss 57.527626\n",
      "Loss  53.092056 C_bot  0.15 k_c 0.0\n",
      "Loss  73.660835 C_bot  0.15 k_c 0.0\n",
      "3534 Train Loss 78.329605\n",
      "Loss  73.660835 C_bot  0.15 k_c 0.0\n",
      "Loss  46.795612 C_bot  0.15 k_c 0.0\n",
      "3535 Train Loss 51.175632\n",
      "Loss  46.795612 C_bot  0.15 k_c 0.0\n",
      "Loss  10.810504 C_bot  0.15 k_c 0.0\n",
      "3536 Train Loss 15.270187\n",
      "Loss  10.810504 C_bot  0.15 k_c 0.0\n",
      "Loss  4.836565 C_bot  0.15 k_c 0.0\n",
      "3537 Train Loss 9.2596855\n",
      "Loss  4.836565 C_bot  0.15 k_c 0.0\n",
      "Loss  26.932318 C_bot  0.15 k_c 0.0\n",
      "3538 Train Loss 31.312376\n",
      "Loss  26.932318 C_bot  0.15 k_c 0.0\n",
      "Loss  41.368496 C_bot  0.15 k_c 0.0\n",
      "3539 Train Loss 45.845806\n",
      "Loss  41.368496 C_bot  0.15 k_c 0.0\n",
      "Loss  26.986986 C_bot  0.15 k_c 0.0\n",
      "3540 Train Loss 31.362598\n",
      "Loss  26.986986 C_bot  0.15 k_c 0.0\n",
      "Loss  6.283747 C_bot  0.15 k_c 0.0\n",
      "3541 Train Loss 10.676693\n",
      "Loss  6.283747 C_bot  0.15 k_c 0.0\n",
      "Loss  4.7888 C_bot  0.15 k_c 0.0\n",
      "3542 Train Loss 9.1668005\n",
      "Loss  4.7888 C_bot  0.15 k_c 0.0\n",
      "Loss  18.727741 C_bot  0.15 k_c 0.0\n",
      "3543 Train Loss 23.073391\n",
      "Loss  18.727741 C_bot  0.15 k_c 0.0\n",
      "Loss  24.837784 C_bot  0.15 k_c 0.0\n",
      "3544 Train Loss 29.234089\n",
      "Loss  24.837784 C_bot  0.15 k_c 0.0\n",
      "Loss  14.153354 C_bot  0.15 k_c 0.0\n",
      "3545 Train Loss 18.472923\n",
      "Loss  14.153354 C_bot  0.15 k_c 0.0\n",
      "Loss  3.213799 C_bot  0.15 k_c 0.0\n",
      "3546 Train Loss 7.5383654\n",
      "Loss  3.213799 C_bot  0.15 k_c 0.0\n",
      "Loss  5.482519 C_bot  0.15 k_c 0.0\n",
      "3547 Train Loss 9.79976\n",
      "Loss  5.482519 C_bot  0.15 k_c 0.0\n",
      "Loss  14.031422 C_bot  0.15 k_c 0.0\n",
      "3548 Train Loss 18.293686\n",
      "Loss  14.031422 C_bot  0.15 k_c 0.0\n",
      "Loss  14.836925 C_bot  0.15 k_c 0.0\n",
      "3549 Train Loss 19.15651\n",
      "Loss  14.836925 C_bot  0.15 k_c 0.0\n",
      "Loss  7.096952 C_bot  0.15 k_c 0.0\n",
      "3550 Train Loss 11.340857\n",
      "Loss  7.096952 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3122196 C_bot  0.15 k_c 0.0\n",
      "3551 Train Loss 6.5667276\n",
      "Loss  2.3122196 C_bot  0.15 k_c 0.0\n",
      "Loss  5.6769376 C_bot  0.15 k_c 0.0\n",
      "3552 Train Loss 9.94361\n",
      "Loss  5.6769376 C_bot  0.15 k_c 0.0\n",
      "Loss  10.225324 C_bot  0.15 k_c 0.0\n",
      "3553 Train Loss 14.432903\n",
      "Loss  10.225324 C_bot  0.15 k_c 0.0\n",
      "Loss  8.784467 C_bot  0.15 k_c 0.0\n",
      "3554 Train Loss 13.04312\n",
      "Loss  8.784467 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8689845 C_bot  0.15 k_c 0.0\n",
      "3555 Train Loss 8.075132\n",
      "Loss  3.8689845 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3558462 C_bot  0.15 k_c 0.0\n",
      "3556 Train Loss 6.566038\n",
      "Loss  2.3558462 C_bot  0.15 k_c 0.0\n",
      "Loss  5.1849484 C_bot  0.15 k_c 0.0\n",
      "3557 Train Loss 9.41438\n",
      "Loss  5.1849484 C_bot  0.15 k_c 0.0\n",
      "Loss  7.2518797 C_bot  0.15 k_c 0.0\n",
      "3558 Train Loss 11.438422\n",
      "Loss  7.2518797 C_bot  0.15 k_c 0.0\n",
      "Loss  5.4954615 C_bot  0.15 k_c 0.0\n",
      "3559 Train Loss 9.712777\n",
      "Loss  5.4954615 C_bot  0.15 k_c 0.0\n",
      "Loss  2.680777 C_bot  0.15 k_c 0.0\n",
      "3560 Train Loss 6.8666315\n",
      "Loss  2.680777 C_bot  0.15 k_c 0.0\n",
      "Loss  2.4595127 C_bot  0.15 k_c 0.0\n",
      "3561 Train Loss 6.6413975\n",
      "Loss  2.4595127 C_bot  0.15 k_c 0.0\n",
      "Loss  4.3534794 C_bot  0.15 k_c 0.0\n",
      "3562 Train Loss 8.5486355\n",
      "Loss  4.3534794 C_bot  0.15 k_c 0.0\n",
      "Loss  5.211392 C_bot  0.15 k_c 0.0\n",
      "3563 Train Loss 9.377431\n",
      "Loss  5.211392 C_bot  0.15 k_c 0.0\n",
      "Loss  3.8520415 C_bot  0.15 k_c 0.0\n",
      "3564 Train Loss 8.034915\n",
      "Loss  3.8520415 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2854683 C_bot  0.15 k_c 0.0\n",
      "3565 Train Loss 6.4489417\n",
      "Loss  2.2854683 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3867402 C_bot  0.15 k_c 0.0\n",
      "3566 Train Loss 6.544021\n",
      "Loss  2.3867402 C_bot  0.15 k_c 0.0\n",
      "Loss  3.5413074 C_bot  0.15 k_c 0.0\n",
      "3567 Train Loss 7.705575\n",
      "Loss  3.5413074 C_bot  0.15 k_c 0.0\n",
      "Loss  3.9375844 C_bot  0.15 k_c 0.0\n",
      "3568 Train Loss 8.081377\n",
      "Loss  3.9375844 C_bot  0.15 k_c 0.0\n",
      "Loss  3.0622237 C_bot  0.15 k_c 0.0\n",
      "3569 Train Loss 7.215642\n",
      "Loss  3.0622237 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1552963 C_bot  0.15 k_c 0.0\n",
      "3570 Train Loss 6.295978\n",
      "Loss  2.1552963 C_bot  0.15 k_c 0.0\n",
      "Loss  2.2350607 C_bot  0.15 k_c 0.0\n",
      "3571 Train Loss 6.3710346\n",
      "Loss  2.2350607 C_bot  0.15 k_c 0.0\n",
      "Loss  2.9170728 C_bot  0.15 k_c 0.0\n",
      "3572 Train Loss 7.0575495\n",
      "Loss  2.9170728 C_bot  0.15 k_c 0.0\n",
      "Loss  3.1782627 C_bot  0.15 k_c 0.0\n",
      "3573 Train Loss 7.302799\n",
      "Loss  3.1782627 C_bot  0.15 k_c 0.0\n",
      "Loss  2.6854105 C_bot  0.15 k_c 0.0\n",
      "3574 Train Loss 6.816788\n",
      "Loss  2.6854105 C_bot  0.15 k_c 0.0\n",
      "Loss  2.122276 C_bot  0.15 k_c 0.0\n",
      "3575 Train Loss 6.242799\n",
      "Loss  2.122276 C_bot  0.15 k_c 0.0\n",
      "Loss  2.099737 C_bot  0.15 k_c 0.0\n",
      "3576 Train Loss 6.2168465\n",
      "Loss  2.099737 C_bot  0.15 k_c 0.0\n",
      "Loss  2.484157 C_bot  0.15 k_c 0.0\n",
      "3577 Train Loss 6.60373\n",
      "Loss  2.484157 C_bot  0.15 k_c 0.0\n",
      "Loss  2.7065132 C_bot  0.15 k_c 0.0\n",
      "3578 Train Loss 6.813938\n",
      "Loss  2.7065132 C_bot  0.15 k_c 0.0\n",
      "Loss  2.476178 C_bot  0.15 k_c 0.0\n",
      "3579 Train Loss 6.5889463\n",
      "Loss  2.476178 C_bot  0.15 k_c 0.0\n",
      "Loss  2.116622 C_bot  0.15 k_c 0.0\n",
      "3580 Train Loss 6.2201633\n",
      "Loss  2.116622 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0116162 C_bot  0.15 k_c 0.0\n",
      "3581 Train Loss 6.113491\n",
      "Loss  2.0116162 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1971502 C_bot  0.15 k_c 0.0\n",
      "3582 Train Loss 6.299532\n",
      "Loss  2.1971502 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3842444 C_bot  0.15 k_c 0.0\n",
      "3583 Train Loss 6.478421\n",
      "Loss  2.3842444 C_bot  0.15 k_c 0.0\n",
      "Loss  2.3241713 C_bot  0.15 k_c 0.0\n",
      "3584 Train Loss 6.422533\n",
      "Loss  2.3241713 C_bot  0.15 k_c 0.0\n",
      "Loss  2.116351 C_bot  0.15 k_c 0.0\n",
      "3585 Train Loss 6.2071376\n",
      "Loss  2.116351 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9792262 C_bot  0.15 k_c 0.0\n",
      "3586 Train Loss 6.0700583\n",
      "Loss  1.9792262 C_bot  0.15 k_c 0.0\n",
      "Loss  2.02888 C_bot  0.15 k_c 0.0\n",
      "3587 Train Loss 6.118736\n",
      "Loss  2.02888 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1593165 C_bot  0.15 k_c 0.0\n",
      "3588 Train Loss 6.24362\n",
      "Loss  2.1593165 C_bot  0.15 k_c 0.0\n",
      "Loss  2.1912744 C_bot  0.15 k_c 0.0\n",
      "3589 Train Loss 6.2787495\n",
      "Loss  2.1912744 C_bot  0.15 k_c 0.0\n",
      "Loss  2.104361 C_bot  0.15 k_c 0.0\n",
      "3590 Train Loss 6.1852193\n",
      "Loss  2.104361 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9851357 C_bot  0.15 k_c 0.0\n",
      "3591 Train Loss 6.0671716\n",
      "Loss  1.9851357 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9538184 C_bot  0.15 k_c 0.0\n",
      "3592 Train Loss 6.0333753\n",
      "Loss  1.9538184 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0109918 C_bot  0.15 k_c 0.0\n",
      "3593 Train Loss 6.087111\n",
      "Loss  2.0109918 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0667498 C_bot  0.15 k_c 0.0\n",
      "3594 Train Loss 6.144461\n",
      "Loss  2.0667498 C_bot  0.15 k_c 0.0\n",
      "Loss  2.0636404 C_bot  0.15 k_c 0.0\n",
      "3595 Train Loss 6.1356277\n",
      "Loss  2.0636404 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9967692 C_bot  0.15 k_c 0.0\n",
      "3596 Train Loss 6.070216\n",
      "Loss  1.9967692 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9407928 C_bot  0.15 k_c 0.0\n",
      "3597 Train Loss 6.010513\n",
      "Loss  1.9407928 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9339474 C_bot  0.15 k_c 0.0\n",
      "3598 Train Loss 6.0020123\n",
      "Loss  1.9339474 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9652578 C_bot  0.15 k_c 0.0\n",
      "3599 Train Loss 6.0331554\n",
      "Loss  1.9652578 C_bot  0.15 k_c 0.0\n",
      "Loss  1.995019 C_bot  0.15 k_c 0.0\n",
      "3600 Train Loss 6.058602\n",
      "Loss  1.995019 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9828882 C_bot  0.15 k_c 0.0\n",
      "3601 Train Loss 6.047643\n",
      "Loss  1.9828882 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9487106 C_bot  0.15 k_c 0.0\n",
      "3602 Train Loss 6.0093493\n",
      "Loss  1.9487106 C_bot  0.15 k_c 0.0\n",
      "Loss  1.9150413 C_bot  0.15 k_c 0.0\n",
      "3603 Train Loss 5.9753647\n",
      "Loss Less than 2.5...\n",
      "Training time: 1241.18\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./Models_Trained does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 80\u001b[0m\n\u001b[1;32m     76\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     77\u001b[0m train_model(max_iter,reps,n_batches)\n\u001b[0;32m---> 80\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_PINN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Models_Trained/AFSD_Exp_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOmega\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrpm_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmms.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# train_loss_full.append(train_loss)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# test_mse_full.append(test_mse_loss)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# test_re_full.append(test_re_loss)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (elapsed_time[reps]))\n",
      "File \u001b[0;32m~/anaconda3/envs/raghav/lib/python3.9/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/raghav/lib/python3.9/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/raghav/lib/python3.9/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ./Models_Trained does not exist."
     ]
    }
   ],
   "source": [
    "# from Seq_model import Sequentialmodel\n",
    "from Seq_Model_Parallel_calibration  import coupled_PINN\n",
    "from training_samples_calibration import trainingdata_uvw,trainingdata_T\n",
    "\n",
    "folder_main = '/home/smartlab/Documents/jupyterNB/raghav/Projects_git_summer2024/PINN_AFSD/Code_Final_Sept2024/AFSD_PINN/Experimental_Comparison_Only/ExperimentalData_and_Plots/'\n",
    "# filename = 'Models_Trained_AFSD_Exp_2mms_z1mm_300rpm_2mms_8.pt'\n",
    "filename = 'AFSD_Exp_300rpm_1mms.pt'\n",
    "\n",
    "sheet_name = '300rpm_1mms'\n",
    "label = 'meltpool'\n",
    "max_reps = 1\n",
    "max_iter = 10000\n",
    "p_iters = 10\n",
    "\n",
    "N_B = 1000\n",
    "N_f = 10000\n",
    "n_batches = 5\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*3)\n",
    "     #Total number of collocation points\n",
    "\n",
    "\n",
    "    layers1 = np.array([3,50,50,50,4]) #9 hidden layers\n",
    "    layers2 = np.array([3,50,50,50,1]) #9 hidden layers\n",
    "    # layers = np.array([3,50,50,50,5])\n",
    "    # layers = np.array([3,64,64,64,1])\n",
    "    model_PINN = coupled_PINN(layers1,layers2,device1,device2,lb_xyz,ub_xyz,sheet_name)\n",
    "    # PINN = nn.DataParallel(PINN)\n",
    "    #PINN.to(device)\n",
    "    # trained_PINN = torch.load(folder_main + filename)\n",
    "    # state_dict_model2 = model_PINN.state_dict()\n",
    "\n",
    "    # # filtered_state_dict = {k: v for k, v in trained_PINN.items() if k in state_dict_model2 and state_dict_model2[k].shape == v.shape}\n",
    "    # filtered_state_dict = {k: v for k, v in trained_PINN.items() if k in state_dict_model2}\n",
    "\n",
    "    # state_dict_model2.update(filtered_state_dict)\n",
    "    # model_PINN.load_state_dict(state_dict_model2)\n",
    "    model_PINN.load_state_dict(torch.load(folder_main + filename),strict = False)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    #print(PINN)\n",
    "\n",
    "    #params = list(PINN.parameters())\n",
    "\n",
    "    # optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25,\n",
    "    #                           max_iter = 30,\n",
    "    #                           max_eval = 50,\n",
    "    #                           tolerance_grad = 1e-5,\n",
    "    #                           tolerance_change = 1e-5,\n",
    "    #                           history_size = 100,\n",
    "    #                           line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model_PINN.parameters(),lr=0.008, betas=(0.9, 0.999))\n",
    "    # params = [model_PINN.C_bot]\n",
    "    # params = [model_PINN.PINN_T.linears[0], model_PINN.C_bot]\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model_PINN.PINN_T.parameters(),lr=0.008, betas=(0.9, 0.999))\n",
    "    optimizer = torch.optim.Adam(model_PINN.PINN_T.parameters(),lr=0.008, betas=(0.9, 0.999))\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps,n_batches)\n",
    "\n",
    "\n",
    "    torch.save(model_PINN.state_dict(),'./Models_Trained/AFSD_Exp_'+str(Omega)+'rpm_'+str(V)+ 'mms.pt')\n",
    "    # train_loss_full.append(train_loss)\n",
    "    # test_mse_full.append(test_mse_loss)\n",
    "    # test_re_full.append(test_re_loss)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e263c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_PINN.state_dict(),'AFSD_Exp_calib_'+str(Omega)+'rpm_'+str(V)+ 'mms_1data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fccb21-7030-4050-ae16-7f138f7ba249",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_min,y_min,z_min] = lb_xyz\n",
    "[x_max,y_max,z_max] = ub_xyz\n",
    "\n",
    "x_min = -20.0\n",
    "x_max = 20.0\n",
    "\n",
    "x = np.linspace(x_min,x_max,200).reshape(-1,1)\n",
    "# y = np.linspace(y_min,y_max,200).reshape(-1,1)\n",
    "y = 0.0\n",
    "z = np.linspace(z_min,z_max,50).reshape(-1,1)\n",
    "# z = 0.0\n",
    "X,Y,Z = np.meshgrid(x,y,z)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "Z = Z.flatten('F').reshape(-1,1)\n",
    "\n",
    "xyz = np.hstack((X,Y,Z))\n",
    "xyz_test_tensor = torch.from_numpy(xyz).float().to(device1)\n",
    "\n",
    "uvwp = model_PINN.PINN_uvw.forward(xyz_test_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac24ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fb7ac662f10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow(uvwp[:,1].reshape(50,200)/1000)\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d000d1-c6fb-4074-8861-823db543fa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$y$ (mm)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_vel = np.sqrt(np.square(uvwp[:,0])+np.square(uvwp[:,1])+np.square(uvwp[:,2]))\n",
    "# r_vel = np.sqrt(np.square(uvwp[:,2]))\n",
    "# r_vel = uvwp[:,2]\n",
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow((r_vel/1000).reshape(50,200),cmap = 'jet',extent = [-20,20,-20,20],vmax = 0.15,vmin = 0)\n",
    "fig.colorbar(im)\n",
    "ax.set_title('Resultant Velocity (m/s)',fontsize = 18)\n",
    "ax.set_xlabel('$x$ (mm)',math_fontfamily = 'cm',fontsize = 14)\n",
    "ax.set_ylabel('$y$ (mm)',math_fontfamily = 'cm',fontsize = 14)\n",
    "# plt.savefig('Res_vel_xy_Proposal.svg',format = 'svg',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1f440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$z$ (mm)')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cross Section\n",
    "r_vel = np.sqrt(np.square(uvwp[:,0])+np.square(uvwp[:,1])+np.square(uvwp[:,2]))\n",
    "# r_vel = np.sqrt(np.square(uvwp[:,2]))\n",
    "# r_vel = uvwp[:,2]\n",
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow((np.flip(r_vel)/1000).reshape(50,200),cmap = 'jet',extent = [-20,20,-3,0],vmax = 0.15,vmin = 0)\n",
    "# fig.colorbar(im)\n",
    "# ax.set_title('Resultant Velocity (m/s)',fontsize = 18)\n",
    "ax.set_xlabel('$x$ (mm)',math_fontfamily = 'cm',fontsize = 14)\n",
    "ax.set_ylabel('$z$ (mm)',math_fontfamily = 'cm',fontsize = 14)\n",
    "# plt.savefig('Res_vel_xz_Proposal.svg',format = 'svg',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef87f55-d125-4c5b-a16d-544611395158",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = xyz_test_tensor.clone()\n",
    "g.requires_grad = True\n",
    "\n",
    "out_full = model_PINN.PINN_uvw.forward(g.to(device1)).cpu() \n",
    "u = out_full[:,0:1]\n",
    "v = out_full[:,1:2]\n",
    "w = out_full[:,2:3]\n",
    "p = out_full[:,3:4]\n",
    "\n",
    "\n",
    "# print(T.shape)\n",
    "T = model_PINN.PINN_T.forward(g.to(device2)).cpu()\n",
    "\n",
    "# p_xyz = autograd.grad(p,g,torch.ones([xyz_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "u_xyz = autograd.grad(u,g,torch.ones([xyz_test_tensor.shape[0], 1]).to('cpu'),retain_graph=True,allow_unused = True)[0].cpu()\n",
    "v_xyz = autograd.grad(v,g,torch.ones([xyz_test_tensor.shape[0], 1]).to('cpu'),retain_graph=True,allow_unused = True)[0].cpu()\n",
    "w_xyz = autograd.grad(w,g,torch.ones([xyz_test_tensor.shape[0], 1]).to('cpu'),retain_graph=True,allow_unused = True)[0].cpu()\n",
    "\n",
    "eps2_11 = torch.square(1/2*(2*u_xyz[:,0]))\n",
    "eps2_12 = torch.square(1/2*(u_xyz[:,1] + v_xyz[:,0]))\n",
    "eps2_13 = torch.square(1/2*(u_xyz[:,2] + w_xyz[:,0]))\n",
    "\n",
    "eps2_21 = eps2_12\n",
    "eps2_22 = torch.square(1/2*(2*v_xyz[:,1])) \n",
    "eps2_23 = torch.square(1/2*(v_xyz[:,2] + w_xyz[:,1]))\n",
    "\n",
    "eps2_31 = eps2_13\n",
    "eps2_32 = eps2_23 \n",
    "eps2_33 = torch.square(1/2*(2*w_xyz[:,2]))\n",
    "\n",
    "eps_e = torch.sqrt((2/3)*(eps2_11 + eps2_12 + eps2_13 + eps2_21 + eps2_22 + eps2_23 + eps2_31 + eps2_32 + eps2_33)).reshape(-1,1)\n",
    "\n",
    "\n",
    "# Z = eps_e*torch.exp(E_a/(R*T))\n",
    "# log_Z = torch.log(eps_e) + E_a/(R*T)\n",
    "log_Z = torch.log(eps_e) + E_a/(R*T) #Simplification\n",
    "\n",
    "\n",
    "W = (log_Z - log_A)/n\n",
    "\n",
    "\n",
    "\n",
    "# sigma_e =  (1/alpha_sig)*torch.asinh(W) \n",
    "sigma_e = (1/alpha_sig)*(np.log(2)/n + W) #Approximation\n",
    "\n",
    "#____________________________#\n",
    "mu_vis = sigma_e/(3*eps_e)\n",
    "\n",
    "\n",
    "eps_e = eps_e.cpu().detach().numpy()\n",
    "sigma_e = sigma_e.cpu().detach().numpy()\n",
    "mu_vis = mu_vis.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592b72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(786.9044, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(T)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfcef78-9322-40dc-9938-90498e7cc584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$y$ (mm)')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow(T.cpu().detach().numpy().reshape(50,200),cmap = 'jet',extent = [-20,20,-3,0],vmax = 800,vmin = 300)\n",
    "fig.colorbar(im)\n",
    "ax.set_title('Temperature (K)',fontsize = 18)\n",
    "ax.set_xlabel('$x$ (mm)',math_fontfamily = 'cm',fontsize = 14)\n",
    "ax.set_ylabel('$y$ (mm)',math_fontfamily = 'cm',fontsize = 14)\n",
    "# plt.savefig('Temp_xy_Proposal.svg',format = 'svg',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ff22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$z$ (mm)')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow(np.flip(T.cpu().detach().numpy().reshape(50,200)),cmap = 'jet',extent = [-20,20,-3,0],vmax = 800,vmin = 300)\n",
    "# fig.colorbar(im)\n",
    "# ax.set_title('Temperature (K)',fontsize = 18)\n",
    "ax.set_xlabel('$x$ (mm)',math_fontfamily = 'cm',fontsize = 14)\n",
    "ax.set_ylabel('$z$ (mm)',math_fontfamily = 'cm',fontsize = 14)\n",
    "# plt.savefig('Temp_xz_Proposal.svg',format = 'svg',bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raghav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
