{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "# from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "#     y = np.exp(-4.0*x) + np.exp(3.0*x)\n",
    "#     return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "#MATLAB Van Der Pol Example https://www.mathworks.com/help/matlab/ref/ode89.html\n",
    "mu = 1\n",
    "fo_val = 0.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,10,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = np.array([2]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "# y_true = true_1D_1(x_test)\n",
    "# y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) #+ self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        # f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        f = dy2_d2x - mu*(1-torch.square(y))*dy_dx + y \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        # test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        # test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        test_mse = 0\n",
    "        test_re = 0\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1-3): 3 x Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 0.7811314 Test MSE 0 Test RE 0\n",
      "1 Train Loss 0.67949754 Test MSE 0 Test RE 0\n",
      "2 Train Loss 0.57560885 Test MSE 0 Test RE 0\n",
      "3 Train Loss 0.40096796 Test MSE 0 Test RE 0\n",
      "4 Train Loss 0.3000301 Test MSE 0 Test RE 0\n",
      "5 Train Loss 0.2814544 Test MSE 0 Test RE 0\n",
      "6 Train Loss 0.27823907 Test MSE 0 Test RE 0\n",
      "7 Train Loss 0.27139273 Test MSE 0 Test RE 0\n",
      "8 Train Loss 0.26810887 Test MSE 0 Test RE 0\n",
      "9 Train Loss 0.2676194 Test MSE 0 Test RE 0\n",
      "10 Train Loss 0.26693773 Test MSE 0 Test RE 0\n",
      "11 Train Loss 0.26676816 Test MSE 0 Test RE 0\n",
      "12 Train Loss 0.26676112 Test MSE 0 Test RE 0\n",
      "13 Train Loss 0.26675576 Test MSE 0 Test RE 0\n",
      "14 Train Loss 0.26675057 Test MSE 0 Test RE 0\n",
      "15 Train Loss 0.2667449 Test MSE 0 Test RE 0\n",
      "16 Train Loss 0.2667382 Test MSE 0 Test RE 0\n",
      "17 Train Loss 0.26672998 Test MSE 0 Test RE 0\n",
      "18 Train Loss 0.26663217 Test MSE 0 Test RE 0\n",
      "19 Train Loss 0.26662344 Test MSE 0 Test RE 0\n",
      "20 Train Loss 0.2666153 Test MSE 0 Test RE 0\n",
      "21 Train Loss 0.2666075 Test MSE 0 Test RE 0\n",
      "22 Train Loss 0.2665997 Test MSE 0 Test RE 0\n",
      "23 Train Loss 0.2665917 Test MSE 0 Test RE 0\n",
      "24 Train Loss 0.26658338 Test MSE 0 Test RE 0\n",
      "25 Train Loss 0.2665746 Test MSE 0 Test RE 0\n",
      "26 Train Loss 0.26656494 Test MSE 0 Test RE 0\n",
      "27 Train Loss 0.26639315 Test MSE 0 Test RE 0\n",
      "28 Train Loss 0.26636225 Test MSE 0 Test RE 0\n",
      "29 Train Loss 0.26635566 Test MSE 0 Test RE 0\n",
      "30 Train Loss 0.26634985 Test MSE 0 Test RE 0\n",
      "31 Train Loss 0.26634482 Test MSE 0 Test RE 0\n",
      "32 Train Loss 0.26633993 Test MSE 0 Test RE 0\n",
      "33 Train Loss 0.2663354 Test MSE 0 Test RE 0\n",
      "34 Train Loss 0.26633066 Test MSE 0 Test RE 0\n",
      "35 Train Loss 0.26632562 Test MSE 0 Test RE 0\n",
      "36 Train Loss 0.2663199 Test MSE 0 Test RE 0\n",
      "37 Train Loss 0.26631346 Test MSE 0 Test RE 0\n",
      "38 Train Loss 0.2663053 Test MSE 0 Test RE 0\n",
      "39 Train Loss 0.2661308 Test MSE 0 Test RE 0\n",
      "40 Train Loss 0.26612204 Test MSE 0 Test RE 0\n",
      "41 Train Loss 0.26611423 Test MSE 0 Test RE 0\n",
      "42 Train Loss 0.26610777 Test MSE 0 Test RE 0\n",
      "43 Train Loss 0.26610228 Test MSE 0 Test RE 0\n",
      "44 Train Loss 0.26609677 Test MSE 0 Test RE 0\n",
      "45 Train Loss 0.26609096 Test MSE 0 Test RE 0\n",
      "46 Train Loss 0.26608485 Test MSE 0 Test RE 0\n",
      "47 Train Loss 0.2660782 Test MSE 0 Test RE 0\n",
      "48 Train Loss 0.2660713 Test MSE 0 Test RE 0\n",
      "49 Train Loss 0.2660636 Test MSE 0 Test RE 0\n",
      "50 Train Loss 0.2660574 Test MSE 0 Test RE 0\n",
      "51 Train Loss 0.2660511 Test MSE 0 Test RE 0\n",
      "52 Train Loss 0.26604506 Test MSE 0 Test RE 0\n",
      "53 Train Loss 0.26603872 Test MSE 0 Test RE 0\n",
      "54 Train Loss 0.26603553 Test MSE 0 Test RE 0\n",
      "55 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "56 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "57 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "58 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "59 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "60 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "61 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "62 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "63 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "64 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "65 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "66 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "67 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "68 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "69 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "70 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "71 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "72 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "73 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "74 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "75 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "76 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "77 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "78 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "79 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "80 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "81 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "82 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "83 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "84 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "85 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "86 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "87 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "88 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "89 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "90 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "91 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "92 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "93 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "94 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "95 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "96 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "97 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "98 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "99 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "100 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "101 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "102 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "103 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "104 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "105 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "106 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "107 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "108 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "109 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "110 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "111 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "112 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "113 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "114 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "115 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "116 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "117 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "118 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "119 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "120 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "121 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "122 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "123 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "124 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "125 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "126 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "127 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "128 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "129 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "130 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "131 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "132 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "133 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "134 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "135 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "136 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "137 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "138 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "139 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "140 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "141 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "142 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "143 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "144 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "145 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "146 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "147 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "148 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "149 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "150 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "151 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "152 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "153 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "154 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "155 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "156 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "157 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "158 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "159 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "160 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "161 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "162 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "163 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "164 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "165 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "166 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "167 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "168 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "169 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "170 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "171 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "172 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "173 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "174 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "175 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "176 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "177 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "178 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "179 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "180 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "181 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "182 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "183 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "184 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "185 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "186 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "187 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "188 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "189 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "190 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "191 Train Loss 0.26603317 Test MSE 0 Test RE 0\n",
      "192 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "193 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "194 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "195 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "196 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "197 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "198 Train Loss 0.2660332 Test MSE 0 Test RE 0\n",
      "199 Train Loss 0.26603323 Test MSE 0 Test RE 0\n",
      "Training time: 11.97\n",
      "Training time: 11.97\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    # layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    layers = np.array([1,50,50,50,50,1])\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0e68084e80>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(x,u_pred,'r')\n",
    "# plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     a \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m+\u001b[39m \u001b[43mtest_re_full\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
