{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "#     y = np.exp(-4.0*x) + np.exp(3.0*x)\n",
    "#     return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "#MATLAB Van Der Pol Example https://www.mathworks.com/help/matlab/ref/ode89.html\n",
    "mu = 1\n",
    "fo_val = 0.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,10,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = np.array([2]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "# y_true = true_1D_1(x_test)\n",
    "# y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) #+ self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        # f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        f = dy2_d2x - mu*(1-torch.square(y))*dy_dx + y \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        # test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        # test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        test_mse = 0\n",
    "        test_re = 0\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 0.7811313 Test MSE 0 Test RE 0\n",
      "1 Train Loss 0.6794974 Test MSE 0 Test RE 0\n",
      "2 Train Loss 0.57544214 Test MSE 0 Test RE 0\n",
      "3 Train Loss 0.40310928 Test MSE 0 Test RE 0\n",
      "4 Train Loss 0.3042112 Test MSE 0 Test RE 0\n",
      "5 Train Loss 0.2912309 Test MSE 0 Test RE 0\n",
      "6 Train Loss 0.2804998 Test MSE 0 Test RE 0\n",
      "7 Train Loss 0.27341226 Test MSE 0 Test RE 0\n",
      "8 Train Loss 0.2709482 Test MSE 0 Test RE 0\n",
      "9 Train Loss 0.26796272 Test MSE 0 Test RE 0\n",
      "10 Train Loss 0.26749682 Test MSE 0 Test RE 0\n",
      "11 Train Loss 0.2666667 Test MSE 0 Test RE 0\n",
      "12 Train Loss 0.2664555 Test MSE 0 Test RE 0\n",
      "13 Train Loss 0.26644877 Test MSE 0 Test RE 0\n",
      "14 Train Loss 0.2664439 Test MSE 0 Test RE 0\n",
      "15 Train Loss 0.2664399 Test MSE 0 Test RE 0\n",
      "16 Train Loss 0.26643652 Test MSE 0 Test RE 0\n",
      "17 Train Loss 0.2664329 Test MSE 0 Test RE 0\n",
      "18 Train Loss 0.26642877 Test MSE 0 Test RE 0\n",
      "19 Train Loss 0.26642388 Test MSE 0 Test RE 0\n",
      "20 Train Loss 0.26641753 Test MSE 0 Test RE 0\n",
      "21 Train Loss 0.26641017 Test MSE 0 Test RE 0\n",
      "22 Train Loss 0.2664012 Test MSE 0 Test RE 0\n",
      "23 Train Loss 0.26639122 Test MSE 0 Test RE 0\n",
      "24 Train Loss 0.26632783 Test MSE 0 Test RE 0\n",
      "25 Train Loss 0.2663209 Test MSE 0 Test RE 0\n",
      "26 Train Loss 0.2663156 Test MSE 0 Test RE 0\n",
      "27 Train Loss 0.2663115 Test MSE 0 Test RE 0\n",
      "28 Train Loss 0.26630834 Test MSE 0 Test RE 0\n",
      "29 Train Loss 0.26630563 Test MSE 0 Test RE 0\n",
      "30 Train Loss 0.26630285 Test MSE 0 Test RE 0\n",
      "31 Train Loss 0.2662999 Test MSE 0 Test RE 0\n",
      "32 Train Loss 0.26629633 Test MSE 0 Test RE 0\n",
      "33 Train Loss 0.26629254 Test MSE 0 Test RE 0\n",
      "34 Train Loss 0.26628852 Test MSE 0 Test RE 0\n",
      "35 Train Loss 0.26628333 Test MSE 0 Test RE 0\n",
      "36 Train Loss 0.26627654 Test MSE 0 Test RE 0\n",
      "37 Train Loss 0.26626742 Test MSE 0 Test RE 0\n",
      "38 Train Loss 0.2661434 Test MSE 0 Test RE 0\n",
      "39 Train Loss 0.26611188 Test MSE 0 Test RE 0\n",
      "40 Train Loss 0.26610497 Test MSE 0 Test RE 0\n",
      "41 Train Loss 0.26609924 Test MSE 0 Test RE 0\n",
      "42 Train Loss 0.2660941 Test MSE 0 Test RE 0\n",
      "43 Train Loss 0.2660893 Test MSE 0 Test RE 0\n",
      "44 Train Loss 0.2660854 Test MSE 0 Test RE 0\n",
      "45 Train Loss 0.2660818 Test MSE 0 Test RE 0\n",
      "46 Train Loss 0.26607886 Test MSE 0 Test RE 0\n",
      "47 Train Loss 0.26607612 Test MSE 0 Test RE 0\n",
      "48 Train Loss 0.26607364 Test MSE 0 Test RE 0\n",
      "49 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "50 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "51 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "52 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "53 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "54 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "55 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "56 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "57 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "58 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "59 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "60 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "61 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "62 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "63 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "64 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "65 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "66 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "67 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "68 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "69 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "70 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "71 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "72 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "73 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "74 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "75 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "76 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "77 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "78 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "79 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "80 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "81 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "82 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "83 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "84 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "85 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "86 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "87 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "88 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "89 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "90 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "91 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "92 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "93 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "94 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "95 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "96 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "97 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "98 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "99 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "100 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "101 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "102 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "103 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "104 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "105 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "106 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "107 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "108 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "109 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "110 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "111 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "112 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "113 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "114 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "115 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "116 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "117 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "118 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "119 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "120 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "121 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "122 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "123 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "124 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "125 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "126 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "127 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "128 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "129 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "130 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "131 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "132 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "133 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "134 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "135 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "136 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "137 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "138 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "139 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "140 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "141 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "142 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "143 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "144 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "145 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "146 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "147 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "148 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "149 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "150 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "151 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "152 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "153 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "154 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "155 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "156 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "157 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "158 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "159 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "160 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "161 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "162 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "163 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "164 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "165 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "166 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "167 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "168 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "169 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "170 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "171 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "172 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "173 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "174 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "175 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "176 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "177 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "178 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "179 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "180 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "181 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "182 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "183 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "184 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "185 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "186 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "187 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "188 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "189 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "190 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "191 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "192 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "193 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "194 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "195 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "196 Train Loss 0.26607117 Test MSE 0 Test RE 0\n",
      "197 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "198 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "199 Train Loss 0.26607123 Test MSE 0 Test RE 0\n",
      "Training time: 11.61\n",
      "Training time: 11.61\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    # layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    layers = np.array([1,50,50,50,50,1])\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa9bc387350>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(x,u_pred,'r')\n",
    "# plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25567208616517456\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
