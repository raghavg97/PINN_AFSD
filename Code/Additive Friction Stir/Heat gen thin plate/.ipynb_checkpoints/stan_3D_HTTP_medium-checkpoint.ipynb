{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"3D_HTTP_stan_\" + level\n",
    "\n",
    "max_temp = 1000.0\n",
    "\n",
    "loss_thresh = 20000\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,3000,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300.0*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = max_temp*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('3D_HTTP_FEA_'+level+'.mat')\n",
    "xy = fea_data['xy']\n",
    "t = fea_data['t']\n",
    "xyt = np.zeros((497*101,3))\n",
    "u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    t_temp = t[0,i]*np.ones((497,1))\n",
    "    xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "    u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "    #print(i)\n",
    "#print(xyt)\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = 2.0*(xyt - lbxyt)/(ubxyt - lbxyt) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "\n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "\n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_HTTP_stan_medium\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 333875.56 Test MSE 109860.79097536897 Test RE 0.5875715009160609\n",
      "1 Train Loss 197341.53 Test MSE 56503.90009618775 Test RE 0.4213845176572936\n",
      "2 Train Loss 171692.56 Test MSE 43203.70390716843 Test RE 0.36846806418364225\n",
      "3 Train Loss 138280.17 Test MSE 32033.32751415837 Test RE 0.31727831413209956\n",
      "4 Train Loss 110940.58 Test MSE 22279.258306867316 Test RE 0.2645999309881011\n",
      "5 Train Loss 84690.38 Test MSE 16016.99514233588 Test RE 0.22435196833472795\n",
      "6 Train Loss 64696.32 Test MSE 11419.157673944408 Test RE 0.18943333146280306\n",
      "7 Train Loss 55984.617 Test MSE 8876.457744416763 Test RE 0.1670164341759306\n",
      "8 Train Loss 50821.152 Test MSE 7757.554389098142 Test RE 0.15613554257557793\n",
      "9 Train Loss 47914.008 Test MSE 7139.868971361902 Test RE 0.1497905733473875\n",
      "10 Train Loss 44224.996 Test MSE 6718.509981488784 Test RE 0.14530342306964086\n",
      "11 Train Loss 42352.305 Test MSE 6408.107687799655 Test RE 0.14190714508382274\n",
      "12 Train Loss 40517.28 Test MSE 5759.199263915986 Test RE 0.13453039418899043\n",
      "13 Train Loss 38895.426 Test MSE 4961.11379208654 Test RE 0.1248616174456566\n",
      "14 Train Loss 37010.16 Test MSE 4427.595147395114 Test RE 0.11795688999654308\n",
      "15 Train Loss 34610.637 Test MSE 3765.9653354299717 Test RE 0.1087871304610053\n",
      "16 Train Loss 32863.98 Test MSE 3521.192069155058 Test RE 0.10519236510010263\n",
      "17 Train Loss 31431.05 Test MSE 3414.1559811859665 Test RE 0.10358122467963474\n",
      "18 Train Loss 30110.639 Test MSE 3401.246481234131 Test RE 0.10338521016750798\n",
      "19 Train Loss 28761.771 Test MSE 3078.4084339250203 Test RE 0.09835636779764746\n",
      "20 Train Loss 27756.545 Test MSE 2813.844260488416 Test RE 0.09403497003592899\n",
      "21 Train Loss 26678.895 Test MSE 2457.841042073052 Test RE 0.08788530377350015\n",
      "22 Train Loss 26154.465 Test MSE 2298.85246497475 Test RE 0.08499530022659915\n",
      "23 Train Loss 25626.322 Test MSE 2317.734216128201 Test RE 0.08534364319231949\n",
      "24 Train Loss 25128.605 Test MSE 2333.676819081796 Test RE 0.08563665955494651\n",
      "25 Train Loss 24839.361 Test MSE 2312.6855789825777 Test RE 0.08525064202403966\n",
      "26 Train Loss 24422.395 Test MSE 2382.54461075611 Test RE 0.08652864098533485\n",
      "27 Train Loss 23728.209 Test MSE 2227.60287037048 Test RE 0.08366778016095897\n",
      "28 Train Loss 23350.713 Test MSE 2370.8391285737575 Test RE 0.08631582092414573\n",
      "29 Train Loss 22965.984 Test MSE 2309.559414834962 Test RE 0.08519300391995989\n",
      "30 Train Loss 22729.3 Test MSE 2401.3408773448373 Test RE 0.08686928943030393\n",
      "31 Train Loss 22482.434 Test MSE 2537.390913011475 Test RE 0.0892962153048872\n",
      "32 Train Loss 22149.945 Test MSE 2508.964729048793 Test RE 0.08879461739038613\n",
      "33 Train Loss 21977.408 Test MSE 2487.41840405716 Test RE 0.08841252295462866\n",
      "34 Train Loss 21803.457 Test MSE 2383.7882786960936 Test RE 0.08655122164481314\n",
      "35 Train Loss 21657.96 Test MSE 2278.821986363892 Test RE 0.08462419738626344\n",
      "36 Train Loss 21587.68 Test MSE 2317.1477575989297 Test RE 0.08533284521695123\n",
      "37 Train Loss 21483.812 Test MSE 2340.234178385423 Test RE 0.08575688966183054\n",
      "38 Train Loss 21378.246 Test MSE 2371.322960963519 Test RE 0.08632462798721176\n",
      "39 Train Loss 21313.137 Test MSE 2354.9593484458146 Test RE 0.08602626539441202\n",
      "40 Train Loss 21233.895 Test MSE 2395.305221111479 Test RE 0.08676004982860068\n",
      "41 Train Loss 21171.78 Test MSE 2378.1882840959975 Test RE 0.086449498817897\n",
      "42 Train Loss 21064.61 Test MSE 2193.6880799307205 Test RE 0.08302842503862448\n",
      "43 Train Loss 20969.217 Test MSE 2137.239437829183 Test RE 0.08195320678470167\n",
      "44 Train Loss 20935.936 Test MSE 2137.4229765110545 Test RE 0.08195672563719111\n",
      "45 Train Loss 20863.227 Test MSE 2167.7240621421706 Test RE 0.08253560923975313\n",
      "46 Train Loss 20809.248 Test MSE 2163.5259994421003 Test RE 0.08245565035887749\n",
      "47 Train Loss 20759.812 Test MSE 2119.473499330774 Test RE 0.08161187535276236\n",
      "48 Train Loss 20721.611 Test MSE 2130.1979606710506 Test RE 0.08181809142942127\n",
      "49 Train Loss 20638.613 Test MSE 2090.7746386058916 Test RE 0.08105745687359311\n",
      "50 Train Loss 20559.053 Test MSE 2084.3948589491474 Test RE 0.08093369322186456\n",
      "51 Train Loss 20463.508 Test MSE 2021.3932069173989 Test RE 0.07970118217256926\n",
      "52 Train Loss 20379.34 Test MSE 2034.100620975678 Test RE 0.07995130896058165\n",
      "53 Train Loss 20304.543 Test MSE 2066.4418758738793 Test RE 0.08058439676881421\n",
      "54 Train Loss 20185.057 Test MSE 1986.739707022721 Test RE 0.0790150552375796\n",
      "55 Train Loss 20103.896 Test MSE 1980.1487715921583 Test RE 0.07888388159593827\n",
      "56 Train Loss 20046.174 Test MSE 1963.6808120140233 Test RE 0.07855517680578429\n",
      "57 Train Loss 19964.957 Test MSE 1916.6250917870582 Test RE 0.07760826007295714\n",
      "58 Train Loss 19889.129 Test MSE 1897.66638942657 Test RE 0.07722346688501544\n",
      "59 Train Loss 19806.383 Test MSE 1950.9427452548948 Test RE 0.07829997516798931\n",
      "60 Train Loss 19749.979 Test MSE 1951.9975868030492 Test RE 0.07832114003967583\n",
      "61 Train Loss 19659.854 Test MSE 1982.5348390846334 Test RE 0.07893139459110185\n",
      "62 Train Loss 19541.475 Test MSE 1946.5981786872744 Test RE 0.07821274322504972\n",
      "63 Train Loss 19426.734 Test MSE 1895.6375364240123 Test RE 0.07718217486381157\n",
      "64 Train Loss 19360.229 Test MSE 1916.3580462652085 Test RE 0.07760285326169361\n",
      "65 Train Loss 19290.443 Test MSE 1890.3341579424637 Test RE 0.0770741339094742\n",
      "66 Train Loss 19201.604 Test MSE 1891.9647620755293 Test RE 0.07710736885634782\n",
      "67 Train Loss 19112.58 Test MSE 1869.943839270216 Test RE 0.07665732211739666\n",
      "68 Train Loss 19017.008 Test MSE 1898.4499233644058 Test RE 0.07723940776982302\n",
      "69 Train Loss 18869.074 Test MSE 1776.4285968122997 Test RE 0.07471593577257854\n",
      "70 Train Loss 18716.459 Test MSE 1680.5206522840435 Test RE 0.07267102556558319\n",
      "71 Train Loss 18621.16 Test MSE 1626.2518538628055 Test RE 0.07148801932946092\n",
      "72 Train Loss 18477.31 Test MSE 1610.0660443822794 Test RE 0.07113137562775157\n",
      "73 Train Loss 18314.057 Test MSE 1620.2742528385158 Test RE 0.07135651440284548\n",
      "74 Train Loss 18241.387 Test MSE 1646.1917161663946 Test RE 0.07192494991996826\n",
      "75 Train Loss 18134.053 Test MSE 1592.9515689519176 Test RE 0.07075231398260462\n",
      "76 Train Loss 18032.848 Test MSE 1594.3085351127277 Test RE 0.07078244297713362\n",
      "77 Train Loss 17964.918 Test MSE 1603.132095569125 Test RE 0.07097804232333033\n",
      "78 Train Loss 17911.459 Test MSE 1607.5966992427109 Test RE 0.07107680789233828\n",
      "79 Train Loss 17836.902 Test MSE 1525.704049043599 Test RE 0.06924278250936385\n",
      "80 Train Loss 17744.008 Test MSE 1520.8786725880623 Test RE 0.0691331979882662\n",
      "81 Train Loss 17685.195 Test MSE 1553.9881945716015 Test RE 0.06988166111789453\n",
      "82 Train Loss 17647.867 Test MSE 1576.5798289646468 Test RE 0.07038779250478785\n",
      "83 Train Loss 17613.525 Test MSE 1522.3556725118867 Test RE 0.06916675916283285\n",
      "84 Train Loss 17574.123 Test MSE 1499.3604959866097 Test RE 0.06864238966410545\n",
      "85 Train Loss 17455.363 Test MSE 1553.8248719702488 Test RE 0.06987798877513905\n",
      "86 Train Loss 17262.9 Test MSE 1656.6032025336565 Test RE 0.07215203930527236\n",
      "87 Train Loss 17115.432 Test MSE 1638.224511578295 Test RE 0.07175068837557351\n",
      "88 Train Loss 16974.963 Test MSE 1648.2163417242682 Test RE 0.07196916601693353\n",
      "89 Train Loss 16862.812 Test MSE 1627.3957925860987 Test RE 0.07151315797491764\n",
      "90 Train Loss 16709.967 Test MSE 1396.4860679888725 Test RE 0.06624569578146004\n",
      "91 Train Loss 16490.936 Test MSE 1314.635851043284 Test RE 0.06427500187941874\n",
      "92 Train Loss 16143.477 Test MSE 1306.6361475701783 Test RE 0.06407914324820038\n",
      "93 Train Loss 15869.332 Test MSE 1381.1318318171282 Test RE 0.06588050652831139\n",
      "94 Train Loss 15724.994 Test MSE 1466.5757734211584 Test RE 0.06788778131990292\n",
      "95 Train Loss 15496.61 Test MSE 1347.3580004262712 Test RE 0.06507000872196553\n",
      "96 Train Loss 15337.236 Test MSE 1365.2966517203374 Test RE 0.06550174572509439\n",
      "97 Train Loss 15230.222 Test MSE 1356.6160946497457 Test RE 0.06529318361430285\n",
      "98 Train Loss 15151.422 Test MSE 1251.8298454022056 Test RE 0.06272086121397982\n",
      "99 Train Loss 15052.7705 Test MSE 1229.7133948360638 Test RE 0.062164338131438915\n",
      "100 Train Loss 14931.23 Test MSE 1298.102320027504 Test RE 0.06386954543498224\n",
      "101 Train Loss 14805.446 Test MSE 1252.688371948193 Test RE 0.06274236505306423\n",
      "102 Train Loss 14510.613 Test MSE 1168.3728304514113 Test RE 0.06059406477328094\n",
      "103 Train Loss 14288.889 Test MSE 1132.898548662728 Test RE 0.05966709204176484\n",
      "104 Train Loss 14115.874 Test MSE 1122.1730021063565 Test RE 0.05938397574972174\n",
      "105 Train Loss 13925.952 Test MSE 1231.2217436916408 Test RE 0.062202451391583355\n",
      "106 Train Loss 13797.087 Test MSE 1279.7195119976475 Test RE 0.06341569523801921\n",
      "107 Train Loss 13608.541 Test MSE 1113.3740754256585 Test RE 0.059150703544887814\n",
      "108 Train Loss 13525.784 Test MSE 1158.8567648922105 Test RE 0.06034679952569147\n",
      "109 Train Loss 13420.594 Test MSE 1192.3529670319706 Test RE 0.06121273447180697\n",
      "110 Train Loss 13313.607 Test MSE 1235.3505185553208 Test RE 0.06230665884834868\n",
      "111 Train Loss 13151.586 Test MSE 1216.5818114947579 Test RE 0.06183153409054661\n",
      "112 Train Loss 13023.062 Test MSE 1204.9005299794321 Test RE 0.061533973463679684\n",
      "113 Train Loss 12922.416 Test MSE 1234.8887865636802 Test RE 0.062295013705545424\n",
      "114 Train Loss 12803.221 Test MSE 1178.191273799438 Test RE 0.06084813381055237\n",
      "115 Train Loss 12595.961 Test MSE 1232.5187998015467 Test RE 0.062235206999053425\n",
      "116 Train Loss 12467.6455 Test MSE 1206.7079302390566 Test RE 0.06158010791244272\n",
      "117 Train Loss 12328.453 Test MSE 1171.4120462242042 Test RE 0.06067282338017817\n",
      "118 Train Loss 12186.95 Test MSE 1201.1015410770985 Test RE 0.06143689016457751\n",
      "119 Train Loss 12072.891 Test MSE 1231.963792329878 Test RE 0.062221193056933376\n",
      "120 Train Loss 11974.806 Test MSE 1274.2053850748887 Test RE 0.06327892320109069\n",
      "121 Train Loss 11782.764 Test MSE 1236.6725998260895 Test RE 0.06233999045659067\n",
      "122 Train Loss 11576.155 Test MSE 1319.2080172998997 Test RE 0.06438667573966074\n",
      "123 Train Loss 11419.421 Test MSE 1236.3195355185978 Test RE 0.06233109093187931\n",
      "124 Train Loss 11250.926 Test MSE 1166.1039981415668 Test RE 0.06053520317775662\n",
      "125 Train Loss 11156.3955 Test MSE 1198.8250116475822 Test RE 0.06137863979209719\n",
      "126 Train Loss 11100.121 Test MSE 1220.536039155271 Test RE 0.061931937373090586\n",
      "127 Train Loss 10998.31 Test MSE 1276.8443091035351 Test RE 0.06334441574360812\n",
      "128 Train Loss 10933.266 Test MSE 1201.9536179015631 Test RE 0.06145867835963186\n",
      "129 Train Loss 10827.548 Test MSE 1147.3991157944015 Test RE 0.06004773322672423\n",
      "130 Train Loss 10759.221 Test MSE 1142.612410912346 Test RE 0.05992234914032747\n",
      "131 Train Loss 10627.132 Test MSE 1147.3685285420358 Test RE 0.06004693284811764\n",
      "132 Train Loss 10546.438 Test MSE 1162.4236490754151 Test RE 0.060439599893401426\n",
      "133 Train Loss 10450.92 Test MSE 1192.2102817794598 Test RE 0.061209071791363534\n",
      "134 Train Loss 10326.623 Test MSE 1175.0149946451231 Test RE 0.06076605838761427\n",
      "135 Train Loss 10255.596 Test MSE 1166.9336659466132 Test RE 0.06055673435291566\n",
      "136 Train Loss 10218.656 Test MSE 1129.1389318638962 Test RE 0.05956800469547969\n",
      "137 Train Loss 10138.764 Test MSE 1111.0299611529638 Test RE 0.059088402351373824\n",
      "138 Train Loss 10096.43 Test MSE 1115.8490362493924 Test RE 0.05921641120343522\n",
      "139 Train Loss 10049.754 Test MSE 1082.645009987745 Test RE 0.0583287140031213\n",
      "140 Train Loss 10002.878 Test MSE 1098.429878792246 Test RE 0.058752389059188434\n",
      "141 Train Loss 9956.695 Test MSE 1078.0948039924842 Test RE 0.05820601121637358\n",
      "142 Train Loss 9874.369 Test MSE 1133.5030128240317 Test RE 0.05968300776925386\n",
      "143 Train Loss 9830.451 Test MSE 1118.235606670224 Test RE 0.05927970321057124\n",
      "144 Train Loss 9739.74 Test MSE 1086.4894960237639 Test RE 0.058432185225839775\n",
      "145 Train Loss 9676.024 Test MSE 1100.9824140816775 Test RE 0.05882061395301541\n",
      "146 Train Loss 9617.388 Test MSE 1042.0880022049248 Test RE 0.05722575880972053\n",
      "147 Train Loss 9552.432 Test MSE 1051.7640508895126 Test RE 0.05749082270288334\n",
      "148 Train Loss 9487.828 Test MSE 1061.7473581918152 Test RE 0.05776302871742774\n",
      "149 Train Loss 9427.561 Test MSE 1047.2262927264073 Test RE 0.05736666869480284\n",
      "150 Train Loss 9329.349 Test MSE 1003.0043423793993 Test RE 0.05614237346289816\n",
      "151 Train Loss 9274.573 Test MSE 994.6951649770729 Test RE 0.05590934001891059\n",
      "152 Train Loss 9226.307 Test MSE 1006.4633965596768 Test RE 0.05623909904916028\n",
      "153 Train Loss 9171.321 Test MSE 1016.440512581746 Test RE 0.05651716197019379\n",
      "154 Train Loss 9105.324 Test MSE 1027.9495479405114 Test RE 0.05683622988107751\n",
      "155 Train Loss 9021.947 Test MSE 1032.5100720474854 Test RE 0.056962168037696256\n",
      "156 Train Loss 8968.209 Test MSE 1026.3535914371441 Test RE 0.05679209182694048\n",
      "157 Train Loss 8939.433 Test MSE 1028.5990887603152 Test RE 0.05685418388544327\n",
      "158 Train Loss 8874.468 Test MSE 1040.088233188269 Test RE 0.057170824267841834\n",
      "159 Train Loss 8839.006 Test MSE 1041.5805981074918 Test RE 0.0572118251880609\n",
      "160 Train Loss 8781.577 Test MSE 1063.8316674680532 Test RE 0.05781969803071541\n",
      "161 Train Loss 8716.029 Test MSE 1079.014636164433 Test RE 0.05823083665210361\n",
      "162 Train Loss 8671.087 Test MSE 1056.706591959164 Test RE 0.057625747312492336\n",
      "163 Train Loss 8601.04 Test MSE 1068.4171606752636 Test RE 0.05794417578015784\n",
      "164 Train Loss 8505.975 Test MSE 1040.4185336409912 Test RE 0.057179901406474484\n",
      "165 Train Loss 8391.1 Test MSE 955.5874767762132 Test RE 0.054799246511992576\n",
      "166 Train Loss 8324.86 Test MSE 921.7707025717499 Test RE 0.053820882230523316\n",
      "167 Train Loss 8286.543 Test MSE 916.2398803312431 Test RE 0.05365917086191977\n",
      "168 Train Loss 8253.021 Test MSE 904.938534224589 Test RE 0.05332721496853794\n",
      "169 Train Loss 8203.581 Test MSE 893.4008663947186 Test RE 0.052986172235011855\n",
      "170 Train Loss 8127.9727 Test MSE 876.5987787874747 Test RE 0.05248555486817927\n",
      "171 Train Loss 8083.175 Test MSE 856.3185571666905 Test RE 0.051874872229804685\n",
      "172 Train Loss 8045.3076 Test MSE 869.5306795222466 Test RE 0.0522735285975979\n",
      "173 Train Loss 7990.9365 Test MSE 874.90674780697 Test RE 0.05243487599271236\n",
      "174 Train Loss 7941.1343 Test MSE 879.2009738149827 Test RE 0.05256339917119511\n",
      "175 Train Loss 7908.0767 Test MSE 887.6159105989028 Test RE 0.05281434540553775\n",
      "176 Train Loss 7858.873 Test MSE 910.1331788840646 Test RE 0.053480053817962905\n",
      "177 Train Loss 7797.5522 Test MSE 910.1380522948081 Test RE 0.053480197000260826\n",
      "178 Train Loss 7749.881 Test MSE 890.6978181964591 Test RE 0.052905954786872086\n",
      "179 Train Loss 7698.87 Test MSE 887.8330966731803 Test RE 0.05282080644269722\n",
      "180 Train Loss 7662.4053 Test MSE 867.0417289866939 Test RE 0.05219866093174558\n",
      "181 Train Loss 7623.808 Test MSE 861.2641579939846 Test RE 0.05202445619602307\n",
      "182 Train Loss 7588.0205 Test MSE 860.0620310561508 Test RE 0.051988136422495584\n",
      "183 Train Loss 7542.1885 Test MSE 848.469366008543 Test RE 0.051636577059065124\n",
      "184 Train Loss 7500.92 Test MSE 839.9274915398198 Test RE 0.051375996704291285\n",
      "185 Train Loss 7468.509 Test MSE 848.8607783140163 Test RE 0.05164848606962292\n",
      "186 Train Loss 7439.3057 Test MSE 853.7570513435351 Test RE 0.05179722746523929\n",
      "187 Train Loss 7414.7305 Test MSE 843.1017915169979 Test RE 0.051472986643538554\n",
      "188 Train Loss 7389.5405 Test MSE 836.6629647754964 Test RE 0.0512760585535744\n",
      "189 Train Loss 7357.88 Test MSE 846.533978810607 Test RE 0.05157765105120525\n",
      "190 Train Loss 7334.715 Test MSE 848.1245576791688 Test RE 0.05162608373316518\n",
      "191 Train Loss 7288.553 Test MSE 861.1419197288767 Test RE 0.05202076417838223\n",
      "192 Train Loss 7208.284 Test MSE 850.3730036164882 Test RE 0.051694470873752583\n",
      "193 Train Loss 7173.788 Test MSE 834.2546228019427 Test RE 0.05120220606794652\n",
      "194 Train Loss 7144.768 Test MSE 821.7897947355702 Test RE 0.05081825335382572\n",
      "195 Train Loss 7109.3813 Test MSE 831.9089888031519 Test RE 0.05113017399765064\n",
      "196 Train Loss 7083.245 Test MSE 840.0272127348491 Test RE 0.05137904644595433\n",
      "197 Train Loss 7042.0347 Test MSE 840.5485808311339 Test RE 0.05139498833454853\n",
      "198 Train Loss 7008.5845 Test MSE 829.6091818979561 Test RE 0.05105945057104788\n",
      "199 Train Loss 6980.831 Test MSE 809.5523323909717 Test RE 0.05043846096065692\n",
      "200 Train Loss 6946.324 Test MSE 804.344807748114 Test RE 0.05027597406654558\n",
      "201 Train Loss 6909.5244 Test MSE 782.9124289474713 Test RE 0.04960163086812279\n",
      "202 Train Loss 6889.8955 Test MSE 783.6545439710134 Test RE 0.04962513374995444\n",
      "203 Train Loss 6877.8184 Test MSE 784.1124551254626 Test RE 0.049639630305788496\n",
      "204 Train Loss 6863.7344 Test MSE 787.0079729778489 Test RE 0.04973119879692604\n",
      "205 Train Loss 6845.3477 Test MSE 785.7699547270726 Test RE 0.049692068085637794\n",
      "206 Train Loss 6824.246 Test MSE 774.0365059098622 Test RE 0.04931966114869519\n",
      "207 Train Loss 6803.1597 Test MSE 770.9443896093492 Test RE 0.049221051628586705\n",
      "208 Train Loss 6779.6157 Test MSE 749.7142634202942 Test RE 0.048538600347815694\n",
      "209 Train Loss 6756.557 Test MSE 728.177289893007 Test RE 0.047836338160774775\n",
      "210 Train Loss 6734.8574 Test MSE 717.9862012372114 Test RE 0.04750041577716529\n",
      "211 Train Loss 6717.811 Test MSE 715.3550635544522 Test RE 0.047413300701171286\n",
      "212 Train Loss 6705.5884 Test MSE 708.8484766092402 Test RE 0.047197181823044526\n",
      "213 Train Loss 6682.911 Test MSE 705.8559890422865 Test RE 0.047097452217854885\n",
      "214 Train Loss 6667.161 Test MSE 702.6888509173871 Test RE 0.046991671550026044\n",
      "215 Train Loss 6649.144 Test MSE 701.6759180682023 Test RE 0.04695778985842337\n",
      "216 Train Loss 6635.286 Test MSE 703.2422640027086 Test RE 0.04701017240399803\n",
      "217 Train Loss 6621.2646 Test MSE 704.948719741513 Test RE 0.04706717422009696\n",
      "218 Train Loss 6596.6245 Test MSE 722.8302805131036 Test RE 0.047660383324476814\n",
      "219 Train Loss 6577.8843 Test MSE 727.4017387892538 Test RE 0.047810857135119164\n",
      "220 Train Loss 6562.3267 Test MSE 719.5916138917186 Test RE 0.0475534914407851\n",
      "221 Train Loss 6532.824 Test MSE 732.2967259298229 Test RE 0.047971456918239064\n",
      "222 Train Loss 6512.628 Test MSE 740.3685497850573 Test RE 0.048235117768378806\n",
      "223 Train Loss 6494.184 Test MSE 744.5581165857604 Test RE 0.04837140068290381\n",
      "224 Train Loss 6475.5215 Test MSE 748.5193996804829 Test RE 0.04849990551225371\n",
      "225 Train Loss 6461.585 Test MSE 743.916979692932 Test RE 0.0483505699593139\n",
      "226 Train Loss 6446.6 Test MSE 734.4191764361905 Test RE 0.0480409255986574\n",
      "227 Train Loss 6433.9805 Test MSE 741.3816734526368 Test RE 0.048268109062388216\n",
      "228 Train Loss 6415.596 Test MSE 739.0568851434903 Test RE 0.04819237125689571\n",
      "229 Train Loss 6391.235 Test MSE 732.8968965663478 Test RE 0.04799111094790805\n",
      "230 Train Loss 6375.2256 Test MSE 733.9310455742451 Test RE 0.048024957771961196\n",
      "231 Train Loss 6355.9995 Test MSE 739.8185222454365 Test RE 0.04821719725266981\n",
      "232 Train Loss 6346.3843 Test MSE 729.5292776218113 Test RE 0.04788072580594072\n",
      "233 Train Loss 6323.01 Test MSE 729.8272431025492 Test RE 0.047890502896080524\n",
      "234 Train Loss 6306.907 Test MSE 720.9468627485817 Test RE 0.04759825050835962\n",
      "235 Train Loss 6286.798 Test MSE 715.59831731803 Test RE 0.0474213613720995\n",
      "236 Train Loss 6275.3223 Test MSE 712.5942205303562 Test RE 0.0473217187504208\n",
      "237 Train Loss 6261.4673 Test MSE 705.94984667684 Test RE 0.04710058338666984\n",
      "238 Train Loss 6244.2983 Test MSE 720.2509505862947 Test RE 0.04757527225538597\n",
      "239 Train Loss 6232.6367 Test MSE 727.6614951591731 Test RE 0.04781939304141473\n",
      "240 Train Loss 6213.8433 Test MSE 733.9173194342908 Test RE 0.0480245086831961\n",
      "241 Train Loss 6205.323 Test MSE 734.8769386130477 Test RE 0.04805589517847351\n",
      "242 Train Loss 6187.5396 Test MSE 747.7918278100201 Test RE 0.04847632847065181\n",
      "243 Train Loss 6170.9966 Test MSE 733.9047108842447 Test RE 0.04802409615578612\n",
      "244 Train Loss 6156.809 Test MSE 728.0321289785937 Test RE 0.0478315698764658\n",
      "245 Train Loss 6144.627 Test MSE 722.5773110911045 Test RE 0.04765204272572391\n",
      "246 Train Loss 6135.0054 Test MSE 718.1022182405875 Test RE 0.04750425333928407\n",
      "247 Train Loss 6111.442 Test MSE 735.7666793042391 Test RE 0.0480849778377917\n",
      "248 Train Loss 6093.198 Test MSE 729.006919450091 Test RE 0.047863580937432426\n",
      "249 Train Loss 6077.609 Test MSE 729.6932329907605 Test RE 0.047886105892308455\n",
      "250 Train Loss 6063.3853 Test MSE 737.8647845131816 Test RE 0.0481534883624788\n",
      "251 Train Loss 6052.5195 Test MSE 734.0816324874208 Test RE 0.04802988436504419\n",
      "252 Train Loss 6035.9062 Test MSE 729.1635890338758 Test RE 0.04786872379977051\n",
      "253 Train Loss 6023.6255 Test MSE 723.5945402777385 Test RE 0.047685572702434974\n",
      "254 Train Loss 6017.7104 Test MSE 718.6318243617608 Test RE 0.04752176749379048\n",
      "255 Train Loss 6005.301 Test MSE 713.295779594531 Test RE 0.04734500746970106\n",
      "256 Train Loss 5995.3774 Test MSE 708.2778613633724 Test RE 0.04717818139174053\n",
      "257 Train Loss 5984.833 Test MSE 701.6366586591678 Test RE 0.04695647617441375\n",
      "258 Train Loss 5975.396 Test MSE 700.4961291373434 Test RE 0.04691829613618303\n",
      "259 Train Loss 5963.7295 Test MSE 699.2229628513043 Test RE 0.04687563925532799\n",
      "260 Train Loss 5954.8833 Test MSE 693.7032625892981 Test RE 0.046690253374838425\n",
      "261 Train Loss 5938.371 Test MSE 710.5251589552316 Test RE 0.047252968033989866\n",
      "262 Train Loss 5916.4507 Test MSE 709.8710856529292 Test RE 0.04723121368753033\n",
      "263 Train Loss 5906.295 Test MSE 711.5076533446369 Test RE 0.04728562679167803\n",
      "264 Train Loss 5900.429 Test MSE 713.4789217392349 Test RE 0.047351085109550314\n",
      "265 Train Loss 5892.5474 Test MSE 705.9640409546663 Test RE 0.04710105690147328\n",
      "266 Train Loss 5876.56 Test MSE 700.7405902168753 Test RE 0.04692648226051626\n",
      "267 Train Loss 5856.2188 Test MSE 685.0722705092332 Test RE 0.04639888633230906\n",
      "268 Train Loss 5836.071 Test MSE 678.3603871125046 Test RE 0.04617103412671459\n",
      "269 Train Loss 5828.0693 Test MSE 678.5060909953612 Test RE 0.04617599235974092\n",
      "270 Train Loss 5821.6816 Test MSE 673.5156312693813 Test RE 0.0460058651322676\n",
      "271 Train Loss 5806.629 Test MSE 672.2858433519373 Test RE 0.045963844350140316\n",
      "272 Train Loss 5789.8438 Test MSE 677.7127501830903 Test RE 0.046148988907321116\n",
      "273 Train Loss 5774.134 Test MSE 673.0563634984339 Test RE 0.045990176844330724\n",
      "274 Train Loss 5755.226 Test MSE 662.7350089457765 Test RE 0.04563618359068632\n",
      "275 Train Loss 5740.2046 Test MSE 657.3832512654491 Test RE 0.04545154806026621\n",
      "276 Train Loss 5720.2993 Test MSE 646.1368199462559 Test RE 0.045061081110780915\n",
      "277 Train Loss 5702.0386 Test MSE 648.0632997665779 Test RE 0.04512820671856472\n",
      "278 Train Loss 5690.99 Test MSE 651.6051097448998 Test RE 0.045251356527747755\n",
      "279 Train Loss 5681.512 Test MSE 656.0402220210884 Test RE 0.04540509573014065\n",
      "280 Train Loss 5660.156 Test MSE 661.0678175444244 Test RE 0.04557874572943759\n",
      "281 Train Loss 5652.8755 Test MSE 660.5965199060273 Test RE 0.04556249551676698\n",
      "282 Train Loss 5635.0474 Test MSE 662.2399376930191 Test RE 0.0456191350092805\n",
      "283 Train Loss 5619.1216 Test MSE 663.0983812816577 Test RE 0.0456486928545395\n",
      "284 Train Loss 5604.2163 Test MSE 660.754637791937 Test RE 0.04556794802386257\n",
      "285 Train Loss 5587.009 Test MSE 647.9973069950881 Test RE 0.045125908940511945\n",
      "286 Train Loss 5572.0947 Test MSE 647.6624284640233 Test RE 0.04511424712430214\n",
      "287 Train Loss 5557.719 Test MSE 635.007318963817 Test RE 0.044671314029948096\n",
      "288 Train Loss 5539.6885 Test MSE 622.4347884218545 Test RE 0.04422687878612393\n",
      "289 Train Loss 5526.042 Test MSE 621.5935705755031 Test RE 0.04419698246660109\n",
      "290 Train Loss 5511.0874 Test MSE 617.3168232189396 Test RE 0.04404467588926922\n",
      "291 Train Loss 5498.9316 Test MSE 612.9643092707602 Test RE 0.04388912838994954\n",
      "292 Train Loss 5486.2476 Test MSE 614.6184949783411 Test RE 0.04394830953025788\n",
      "293 Train Loss 5470.581 Test MSE 619.775537654094 Test RE 0.04413230161083398\n",
      "294 Train Loss 5448.498 Test MSE 620.302186406549 Test RE 0.04415104814470264\n",
      "295 Train Loss 5435.897 Test MSE 617.979146440877 Test RE 0.04406829746419275\n",
      "296 Train Loss 5424.079 Test MSE 613.4663083089042 Test RE 0.04390709663932773\n",
      "297 Train Loss 5403.332 Test MSE 612.406443817274 Test RE 0.043869151858266696\n",
      "298 Train Loss 5393.472 Test MSE 619.1158373247699 Test RE 0.04410880774599448\n",
      "299 Train Loss 5381.8066 Test MSE 609.638378319619 Test RE 0.04376989570555391\n",
      "300 Train Loss 5374.2217 Test MSE 606.553980695808 Test RE 0.04365903083911172\n",
      "301 Train Loss 5361.5986 Test MSE 602.8167500041201 Test RE 0.043524322319702685\n",
      "302 Train Loss 5346.7744 Test MSE 603.1884313355729 Test RE 0.04353773824189674\n",
      "303 Train Loss 5335.115 Test MSE 600.7823197200252 Test RE 0.043450815707076255\n",
      "304 Train Loss 5326.096 Test MSE 598.1834719924386 Test RE 0.04335673467840772\n",
      "305 Train Loss 5319.7446 Test MSE 595.0845424978243 Test RE 0.04324428261566805\n",
      "306 Train Loss 5316.249 Test MSE 596.2868230006642 Test RE 0.043287944919267456\n",
      "307 Train Loss 5310.739 Test MSE 594.8676432156378 Test RE 0.04323640095513979\n",
      "308 Train Loss 5303.451 Test MSE 592.4443474305426 Test RE 0.043148245622084115\n",
      "309 Train Loss 5291.3193 Test MSE 591.8731998997433 Test RE 0.04312744201711377\n",
      "310 Train Loss 5282.7793 Test MSE 592.3247357464168 Test RE 0.04314388968985887\n",
      "311 Train Loss 5273.504 Test MSE 596.0456024813893 Test RE 0.04327918823003474\n",
      "312 Train Loss 5265.239 Test MSE 594.9307985973049 Test RE 0.0432386960362086\n",
      "313 Train Loss 5258.682 Test MSE 592.6383045943836 Test RE 0.04315530807995949\n",
      "314 Train Loss 5253.0513 Test MSE 591.8816622572697 Test RE 0.04312775032515095\n",
      "315 Train Loss 5247.932 Test MSE 591.2729368719595 Test RE 0.043105567081778755\n",
      "316 Train Loss 5243.407 Test MSE 592.9420768987549 Test RE 0.04316636685545546\n",
      "317 Train Loss 5238.7993 Test MSE 590.8286635100222 Test RE 0.04308936960956244\n",
      "318 Train Loss 5231.286 Test MSE 590.1600288550434 Test RE 0.04306498081170284\n",
      "319 Train Loss 5220.7803 Test MSE 582.2514650793238 Test RE 0.04277545691050871\n",
      "320 Train Loss 5213.3057 Test MSE 580.8401669050354 Test RE 0.04272358452050442\n",
      "321 Train Loss 5207.3877 Test MSE 576.9366658559253 Test RE 0.04257978188207376\n",
      "322 Train Loss 5202.344 Test MSE 575.3173470114868 Test RE 0.04251998442346751\n",
      "323 Train Loss 5197.8135 Test MSE 577.7411548799005 Test RE 0.04260945847926723\n",
      "324 Train Loss 5191.835 Test MSE 574.8206990638017 Test RE 0.04250162757872948\n",
      "325 Train Loss 5189.662 Test MSE 573.1548940576365 Test RE 0.04243999897700612\n",
      "326 Train Loss 5187.2974 Test MSE 573.6264608594801 Test RE 0.042457454275714894\n",
      "327 Train Loss 5184.328 Test MSE 570.0355382597884 Test RE 0.04232435303666369\n",
      "328 Train Loss 5180.565 Test MSE 565.9561742724909 Test RE 0.04217263754092423\n",
      "329 Train Loss 5175.12 Test MSE 562.2916789273991 Test RE 0.042035884543722604\n",
      "330 Train Loss 5170.7026 Test MSE 560.045109214198 Test RE 0.04195182580246865\n",
      "331 Train Loss 5166.253 Test MSE 556.2706185224671 Test RE 0.04181021713962353\n",
      "332 Train Loss 5162.0386 Test MSE 554.2256879258522 Test RE 0.04173329620327265\n",
      "333 Train Loss 5158.363 Test MSE 550.1159244757487 Test RE 0.04157827531425509\n",
      "334 Train Loss 5151.8096 Test MSE 550.238852791429 Test RE 0.04158292057321316\n",
      "335 Train Loss 5147.6577 Test MSE 545.3688203276542 Test RE 0.04139849133998838\n",
      "336 Train Loss 5142.66 Test MSE 543.6280717171121 Test RE 0.04133236914832247\n",
      "337 Train Loss 5138.2935 Test MSE 541.2482410715071 Test RE 0.04124179994138755\n",
      "338 Train Loss 5133.8174 Test MSE 538.1569904719053 Test RE 0.04112385840961387\n",
      "339 Train Loss 5130.1113 Test MSE 538.5590051119918 Test RE 0.041139215737560096\n",
      "340 Train Loss 5126.024 Test MSE 539.4846485526776 Test RE 0.0411745543878917\n",
      "341 Train Loss 5119.1606 Test MSE 536.5765146088279 Test RE 0.04106342710705173\n",
      "342 Train Loss 5110.124 Test MSE 535.2043354135512 Test RE 0.04101088804840452\n",
      "343 Train Loss 5103.6904 Test MSE 538.1981189945541 Test RE 0.04112542982024095\n",
      "344 Train Loss 5100.4004 Test MSE 538.2340835439985 Test RE 0.04112680388008404\n",
      "345 Train Loss 5097.718 Test MSE 540.7510429840548 Test RE 0.04122285294647411\n",
      "346 Train Loss 5093.6777 Test MSE 545.8698560516738 Test RE 0.041417503575834715\n",
      "347 Train Loss 5091.0645 Test MSE 548.5337724534679 Test RE 0.041518441998925966\n",
      "348 Train Loss 5088.061 Test MSE 547.8853338819754 Test RE 0.04149389463132872\n",
      "349 Train Loss 5083.503 Test MSE 545.5477536349385 Test RE 0.04140528212193011\n",
      "350 Train Loss 5076.1626 Test MSE 545.7511081713315 Test RE 0.04141299837353205\n",
      "351 Train Loss 5066.1484 Test MSE 541.6844874023755 Test RE 0.04125841704855937\n",
      "352 Train Loss 5062.174 Test MSE 541.8568997865437 Test RE 0.04126498258280586\n",
      "353 Train Loss 5058.048 Test MSE 539.0490854620895 Test RE 0.041157929505668885\n",
      "354 Train Loss 5051.2793 Test MSE 534.6393949215775 Test RE 0.04098923759925893\n",
      "355 Train Loss 5042.9507 Test MSE 531.2022995127891 Test RE 0.04085726913464062\n",
      "356 Train Loss 5034.205 Test MSE 532.8500032977586 Test RE 0.04092058639945519\n",
      "357 Train Loss 5023.5127 Test MSE 539.0213420157921 Test RE 0.04115687034655862\n",
      "358 Train Loss 5016.125 Test MSE 540.2288620586143 Test RE 0.041202944536743245\n",
      "359 Train Loss 5011.2197 Test MSE 542.2690803679808 Test RE 0.04128067435605201\n",
      "360 Train Loss 5007.2515 Test MSE 542.0066321662111 Test RE 0.04127068360576701\n",
      "361 Train Loss 5002.1226 Test MSE 543.3267395788064 Test RE 0.0413209123286529\n",
      "362 Train Loss 4996.6616 Test MSE 545.0351310411654 Test RE 0.041385824364666056\n",
      "363 Train Loss 4991.7773 Test MSE 547.4633427030473 Test RE 0.04147791187967393\n",
      "364 Train Loss 4987.5337 Test MSE 547.6501880778666 Test RE 0.0414849893351389\n",
      "365 Train Loss 4983.4053 Test MSE 547.5435770228916 Test RE 0.041480951197436694\n",
      "366 Train Loss 4979.35 Test MSE 548.330479421571 Test RE 0.04151074767584951\n",
      "367 Train Loss 4975.6855 Test MSE 551.8480643402028 Test RE 0.04164368224260606\n",
      "368 Train Loss 4971.7095 Test MSE 555.7495834739002 Test RE 0.041790631625525376\n",
      "369 Train Loss 4966.2593 Test MSE 557.2832971476482 Test RE 0.041848257132584304\n",
      "370 Train Loss 4958.491 Test MSE 553.2646470834591 Test RE 0.04169709722792094\n",
      "371 Train Loss 4953.9385 Test MSE 550.0349525004892 Test RE 0.04157521523289291\n",
      "372 Train Loss 4946.8027 Test MSE 544.6158100318961 Test RE 0.04136990127664979\n",
      "373 Train Loss 4936.3896 Test MSE 537.3829412915072 Test RE 0.04109427285751936\n",
      "374 Train Loss 4929.838 Test MSE 534.0626883920332 Test RE 0.040967124433531614\n",
      "375 Train Loss 4926.2363 Test MSE 534.3018529691834 Test RE 0.04097629637956491\n",
      "376 Train Loss 4923.8125 Test MSE 531.4215199167813 Test RE 0.040865698901568194\n",
      "377 Train Loss 4918.1074 Test MSE 534.7189622170423 Test RE 0.04099228758161621\n",
      "378 Train Loss 4912.1494 Test MSE 536.8450615377797 Test RE 0.04107370157632649\n",
      "379 Train Loss 4903.863 Test MSE 538.6615398631028 Test RE 0.04114313174161242\n",
      "380 Train Loss 4894.7207 Test MSE 535.0821389662443 Test RE 0.04100620603205301\n",
      "381 Train Loss 4889.65 Test MSE 532.722215672457 Test RE 0.040915679335479016\n",
      "382 Train Loss 4884.559 Test MSE 531.7407692928728 Test RE 0.04087797201207622\n",
      "383 Train Loss 4879.9375 Test MSE 527.7829808128174 Test RE 0.04072555889195511\n",
      "384 Train Loss 4869.191 Test MSE 518.5070182283645 Test RE 0.04036608977692556\n",
      "385 Train Loss 4864.115 Test MSE 515.7607140217461 Test RE 0.04025904711476245\n",
      "386 Train Loss 4861.065 Test MSE 514.2226187858566 Test RE 0.04019897227599357\n",
      "387 Train Loss 4858.437 Test MSE 511.7000319066742 Test RE 0.04010025037356895\n",
      "388 Train Loss 4854.1074 Test MSE 507.93521888581876 Test RE 0.03995246001745316\n",
      "389 Train Loss 4850.3286 Test MSE 505.98122515749753 Test RE 0.03987553871165505\n",
      "390 Train Loss 4843.583 Test MSE 504.8796426012362 Test RE 0.0398321081159052\n",
      "391 Train Loss 4837.776 Test MSE 506.75788078935346 Test RE 0.0399061304461979\n",
      "392 Train Loss 4832.4814 Test MSE 507.4084675637453 Test RE 0.039931738408343624\n",
      "393 Train Loss 4828.3506 Test MSE 504.31759706409184 Test RE 0.0398099308574439\n",
      "394 Train Loss 4825.6875 Test MSE 505.8443039184399 Test RE 0.03987014307906005\n",
      "395 Train Loss 4821.3965 Test MSE 504.4459525532523 Test RE 0.039814996611732864\n",
      "396 Train Loss 4816.6367 Test MSE 505.6312421965231 Test RE 0.03986174553858846\n",
      "397 Train Loss 4809.4805 Test MSE 509.98765752626616 Test RE 0.04003309756855064\n",
      "398 Train Loss 4805.0454 Test MSE 513.605840343985 Test RE 0.040174856943723976\n",
      "399 Train Loss 4799.502 Test MSE 516.2134045889946 Test RE 0.040276711210797685\n",
      "400 Train Loss 4794.0005 Test MSE 520.459111059035 Test RE 0.040442004205555285\n",
      "401 Train Loss 4790.6 Test MSE 521.6516486372307 Test RE 0.04048831045090474\n",
      "402 Train Loss 4786.654 Test MSE 527.1922552913084 Test RE 0.040702761299634925\n",
      "403 Train Loss 4783.558 Test MSE 529.0514487775 Test RE 0.04077446920803849\n",
      "404 Train Loss 4777.905 Test MSE 528.8294582845324 Test RE 0.04076591380737818\n",
      "405 Train Loss 4770.8784 Test MSE 526.1434443472032 Test RE 0.04066225353975771\n",
      "406 Train Loss 4763.1055 Test MSE 529.877076681967 Test RE 0.040806272746068406\n",
      "407 Train Loss 4758.83 Test MSE 530.5424674969947 Test RE 0.040831885856716585\n",
      "408 Train Loss 4755.555 Test MSE 531.9139096638073 Test RE 0.04088462661860845\n",
      "409 Train Loss 4751.829 Test MSE 530.443311088048 Test RE 0.040828070014808\n",
      "410 Train Loss 4750.1597 Test MSE 532.934604738869 Test RE 0.04092383478376174\n",
      "411 Train Loss 4748.2666 Test MSE 532.2732546544665 Test RE 0.040898434497126365\n",
      "412 Train Loss 4744.2104 Test MSE 530.3304159830848 Test RE 0.04082372503163059\n",
      "413 Train Loss 4740.6846 Test MSE 528.4359999413773 Test RE 0.040750745709974556\n",
      "414 Train Loss 4736.2217 Test MSE 531.0452946394794 Test RE 0.040851230696518935\n",
      "415 Train Loss 4733.387 Test MSE 531.8468517270029 Test RE 0.04088204939222585\n",
      "416 Train Loss 4730.0625 Test MSE 534.7414059087575 Test RE 0.04099314785465349\n",
      "417 Train Loss 4725.6484 Test MSE 531.2502001877044 Test RE 0.04085911122629835\n",
      "418 Train Loss 4720.2993 Test MSE 529.5685158558038 Test RE 0.040794389753422255\n",
      "419 Train Loss 4716.723 Test MSE 527.9769347662848 Test RE 0.04073304128328674\n",
      "420 Train Loss 4710.612 Test MSE 523.7606975948594 Test RE 0.04057007544959464\n",
      "421 Train Loss 4704.9863 Test MSE 521.6139553894436 Test RE 0.04048684763228623\n",
      "422 Train Loss 4700.3315 Test MSE 519.7367146302298 Test RE 0.040413927740486945\n",
      "423 Train Loss 4697.5757 Test MSE 517.5696614823618 Test RE 0.040329586372606405\n",
      "424 Train Loss 4693.4644 Test MSE 518.0762008946396 Test RE 0.04034931659507974\n",
      "425 Train Loss 4689.636 Test MSE 516.1309877679523 Test RE 0.04027349586325423\n",
      "426 Train Loss 4686.837 Test MSE 516.8821686691697 Test RE 0.04030279238113712\n",
      "427 Train Loss 4679.1685 Test MSE 508.5107031996036 Test RE 0.03997508643166147\n",
      "428 Train Loss 4675.1567 Test MSE 505.86907494831246 Test RE 0.03987111928103263\n",
      "429 Train Loss 4673.482 Test MSE 504.21409201510215 Test RE 0.039805845395909874\n",
      "430 Train Loss 4668.514 Test MSE 498.49285952790405 Test RE 0.039579365991167466\n",
      "431 Train Loss 4664.3623 Test MSE 496.7677204158475 Test RE 0.039510820286172296\n",
      "432 Train Loss 4660.68 Test MSE 495.6774537572654 Test RE 0.039467438852662226\n",
      "433 Train Loss 4657.16 Test MSE 494.3984446718084 Test RE 0.03941648654711869\n",
      "434 Train Loss 4653.8984 Test MSE 492.9213076687658 Test RE 0.03935755927315398\n",
      "435 Train Loss 4650.8535 Test MSE 488.6088093363822 Test RE 0.03918501421011166\n",
      "436 Train Loss 4646.1416 Test MSE 494.397133665839 Test RE 0.03941643428635212\n",
      "437 Train Loss 4638.497 Test MSE 496.12412692703134 Test RE 0.039485217628077975\n",
      "438 Train Loss 4633.24 Test MSE 494.4302493878566 Test RE 0.039417754360574465\n",
      "439 Train Loss 4624.8096 Test MSE 496.2363091066828 Test RE 0.039489681518430006\n",
      "440 Train Loss 4620.037 Test MSE 500.4240955461517 Test RE 0.03965596007530546\n",
      "441 Train Loss 4615.5615 Test MSE 498.79327892286625 Test RE 0.039591290553417174\n",
      "442 Train Loss 4612.1445 Test MSE 497.77769534784505 Test RE 0.039550964476679155\n",
      "443 Train Loss 4601.6494 Test MSE 499.84360820498085 Test RE 0.039632953127209505\n",
      "444 Train Loss 4595.473 Test MSE 494.2455549066611 Test RE 0.039410391419379694\n",
      "445 Train Loss 4585.376 Test MSE 493.0788918437221 Test RE 0.0393638499657987\n",
      "446 Train Loss 4579.6772 Test MSE 494.8233572136441 Test RE 0.03943342123063403\n",
      "447 Train Loss 4574.3384 Test MSE 496.0055049118255 Test RE 0.039480496938398474\n",
      "448 Train Loss 4569.2324 Test MSE 501.9917696155879 Test RE 0.039718026439603545\n",
      "449 Train Loss 4560.549 Test MSE 506.64783868228864 Test RE 0.03990179741728376\n",
      "450 Train Loss 4557.9155 Test MSE 506.1139812141039 Test RE 0.039880769510569986\n",
      "451 Train Loss 4555.806 Test MSE 507.6681929440909 Test RE 0.039941956959836204\n",
      "452 Train Loss 4552.08 Test MSE 507.53979986976384 Test RE 0.03993690583096356\n",
      "453 Train Loss 4547.003 Test MSE 506.09040803182387 Test RE 0.03987984073994596\n",
      "454 Train Loss 4541.8276 Test MSE 508.73669232673734 Test RE 0.0399839681831711\n",
      "455 Train Loss 4539.2886 Test MSE 509.6089612273328 Test RE 0.04001823132514387\n",
      "456 Train Loss 4534.8955 Test MSE 510.0844052756143 Test RE 0.04003689464908901\n",
      "457 Train Loss 4527.9224 Test MSE 512.0485918929332 Test RE 0.04011390579899936\n",
      "458 Train Loss 4524.224 Test MSE 508.49252231241957 Test RE 0.03997437180655296\n",
      "459 Train Loss 4518.7324 Test MSE 504.901065893147 Test RE 0.03983295319437094\n",
      "460 Train Loss 4514.972 Test MSE 503.84571045181906 Test RE 0.039791301555190346\n",
      "461 Train Loss 4512.2876 Test MSE 503.2269325052077 Test RE 0.039766860001328924\n",
      "462 Train Loss 4509.544 Test MSE 502.9096311944744 Test RE 0.03975432086079058\n",
      "463 Train Loss 4505.4937 Test MSE 503.05216912631016 Test RE 0.039759954176274565\n",
      "464 Train Loss 4499.4966 Test MSE 505.43452980454947 Test RE 0.039853990813941226\n",
      "465 Train Loss 4495.742 Test MSE 508.33878482762555 Test RE 0.03996832842962017\n",
      "466 Train Loss 4492.2603 Test MSE 512.0252089184424 Test RE 0.04011298987699558\n",
      "467 Train Loss 4490.431 Test MSE 513.811365625792 Test RE 0.04018289435518172\n",
      "468 Train Loss 4486.8945 Test MSE 513.4775214782287 Test RE 0.04016983800341756\n",
      "469 Train Loss 4484.4956 Test MSE 513.9658712076202 Test RE 0.04018893549715251\n",
      "470 Train Loss 4480.5586 Test MSE 513.960813472638 Test RE 0.040188737754951694\n",
      "471 Train Loss 4478.2437 Test MSE 511.0473412475541 Test RE 0.04007466760173311\n",
      "472 Train Loss 4476.4624 Test MSE 510.67218142819206 Test RE 0.040059955495782305\n",
      "473 Train Loss 4473.934 Test MSE 512.4145768926712 Test RE 0.04012823887759615\n",
      "474 Train Loss 4471.3145 Test MSE 511.97882610649145 Test RE 0.040111172978753835\n",
      "475 Train Loss 4469.375 Test MSE 512.4301942846535 Test RE 0.04012885038797203\n",
      "476 Train Loss 4466.8105 Test MSE 513.0064818546458 Test RE 0.040151408835624555\n",
      "477 Train Loss 4464.785 Test MSE 513.058160360237 Test RE 0.04015343114199477\n",
      "478 Train Loss 4461.657 Test MSE 513.7024336289771 Test RE 0.04017863458667756\n",
      "479 Train Loss 4458.219 Test MSE 511.25412552004707 Test RE 0.0400827744562419\n",
      "480 Train Loss 4453.8354 Test MSE 507.8637464925196 Test RE 0.039949649030651124\n",
      "481 Train Loss 4446.171 Test MSE 501.22398651163684 Test RE 0.03968764098245761\n",
      "482 Train Loss 4442.8384 Test MSE 497.3944073510504 Test RE 0.03953573445593757\n",
      "483 Train Loss 4438.0176 Test MSE 496.30702152566346 Test RE 0.03949249500807489\n",
      "484 Train Loss 4434.777 Test MSE 494.85613110660444 Test RE 0.03943472711616722\n",
      "485 Train Loss 4430.6216 Test MSE 496.71569502596077 Test RE 0.03950875129138138\n",
      "486 Train Loss 4427.4653 Test MSE 497.4881392677595 Test RE 0.03953945945319021\n",
      "487 Train Loss 4424.989 Test MSE 500.27272259206165 Test RE 0.039649961869089215\n",
      "488 Train Loss 4422.9644 Test MSE 500.0624471934163 Test RE 0.0396416281268679\n",
      "489 Train Loss 4419.717 Test MSE 498.80792635735224 Test RE 0.03959187186295139\n",
      "490 Train Loss 4417.9395 Test MSE 499.22999568486125 Test RE 0.03960861877134871\n",
      "491 Train Loss 4415.767 Test MSE 496.1113192684981 Test RE 0.03948470796081843\n",
      "492 Train Loss 4413.279 Test MSE 493.81684013276384 Test RE 0.03939329517806861\n",
      "493 Train Loss 4411.2993 Test MSE 493.9509882018891 Test RE 0.039398645517719544\n",
      "494 Train Loss 4409.3613 Test MSE 494.46731588824815 Test RE 0.03941923187011873\n",
      "495 Train Loss 4405.7656 Test MSE 494.30489123531277 Test RE 0.03941275704283622\n",
      "496 Train Loss 4399.992 Test MSE 495.4911557693081 Test RE 0.03946002133206304\n",
      "497 Train Loss 4392.674 Test MSE 497.4495332976954 Test RE 0.0395379252570128\n",
      "498 Train Loss 4389.2554 Test MSE 492.92711604414905 Test RE 0.03935779115885401\n",
      "499 Train Loss 4387.3066 Test MSE 492.8505356734877 Test RE 0.03935473375826656\n",
      "Training time: 301.56\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 500\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 0.5\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "    print(reps)\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "\n",
    "    # layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    layers =  np.array([3,50,50,50,1])\n",
    "    \n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
